{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 53 - Random Forest\n",
    "\n",
    "As mentioned in [Entry 51](), a Random Forest is an ensemble of Decision Trees. On page 85 [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413) lists this as one of the two ensemble models that have proven effective on a wide range of datasets for both classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Style\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <th>Supervision</th>\n",
    "        <th>Prediction types</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Supervised</td>\n",
    "        <td>Regression</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Classification</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Random Forest is a collection of Decision Trees where each tree is trained on a different subset of observations and/or features to introduce model diversity (see the Subsets of Data section in [Entry 51]() for the different options to accomplish this). The sampling method is generally the bagging method (with replacement).\n",
    "\n",
    "Regardless of the sampling method used, it includes an element of stochasticity (i.e. randomness - I know, I know, \"randomness\" is easier, but \"stochasticity\" sure sounds smart, doesn't it?), thus the \"Random\" part of the name. And I'm sure you can figure out the \"Forest\" part (lots of trees ... forest. Get it?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "Individual Decision Trees tend to overfit data. Random Forest uses many Decision Trees to counter this tendency.\n",
    "*Introduction to Machine Learning with Python* puts it this way on page 85:\n",
    "\n",
    "> The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data. If we build many trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior\n",
    "\n",
    "The ensemble of Decision Trees can be accomplished two ways using Scikit-Learn as discussed on page 197 of [Hands-On Machine Learning](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291):\n",
    "\n",
    "- Using `BaggingClassifier` just like any other ensemble collection and passing it to `DecisionTreeClassifier` of `DecisionTreeRegressor` for regression tasks\n",
    "- Using `RandomForestClassifier` (or `RandomForestRegressor` for regression tasks) which combines the functionality of `DecisionTreeClassifier`/`DecisionTreeRegressor` with `BaggingClassifier`, giving you access to almost all of the parameters available in the two underlying classes while being optimized for Decision Trees\n",
    "\n",
    "*Side note*, the `RandomForest-` classes have extra randomness built into the feature selection. It uses a random subset of features for each tree, then chooses the best features of that subgroup - as learned in [Entry 51]() this would be called *Random Subspaces* if you use all the observations, or if you also use a random subset of the observations it's called *Random Patches*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "For the sake of brevity, I'll just cover some of the most important parameters from the `RandomForestClassifier`/`RandomForestRegressor` classes.\n",
    "\n",
    "- `n_estimators`: the number of trees to build \n",
    "  - Generally, the more trees the better. However, at larger numbers there are diminishing returns\n",
    "  - Page 90 of *Introduction to Machine Learning with Python* gives the rule of thumb to \"build as many as you have time/memory for.\"\n",
    "- `max_features`: the maximum number of features to include in each training dataset; i.e., sets limits on the random subspaces / random patches (depending on whether the observations are also sampled - see [Entry 51]())\n",
    "  - Larger values (such as `max_features` = `n_features`) return more similar trees that are individually better able to fit the data\n",
    "  - Smaller values (such as 1) allow for more diversity between models, but can hinder the ability individual model predictions, limiting them to only searching over different thresholds for the small feature set that was randomly selected\n",
    "  - *Introduction to Machine Learning with Python* recommends `max_features` = `sqrt(n_features)` on page 90 as a good rule of thumb for classification and `max_features` = `n_features` for regression\n",
    "- `n_jobs`: controls the number of cores to use when training the Decision Trees; setting `n_jobs` = -1 uses all cores available in the computer\n",
    "- `random_state`: sets the random state which allows for reproducible results\n",
    "- `max_depth`: sets the maximum depth of each Decision Tree\n",
    "- `max_leaf_nodes`: the maximum number of leaf nodes allowed in each individual Decision Tree\n",
    "- `bootstrap`: allows setting whether sampling is with or without replacement (with replacement is the default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "- Has feature importance available under the `feature_importances_` variable. This is determined by aggregating feature importances over the trees in the forest\n",
    "- The feature importance of a Random Forest captures a much broader picture of the data than any single decision tree due to the randomness built into Random Forest, which forces the algorithm to consider different feature subgroups\n",
    "- Allows you to get a quick understanding of which features are actually important\n",
    "- Requires little to no parameter tuning\n",
    "- Scale easily as models can be trained in parallel (see Parallel Procession section of [Entry 51]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- More difficult to create a simple, easy to understand visualization of the decisions to reach the conclusion\n",
    "- Building the many Decision Trees that comprise the Random Forest can be computationally expensive\n",
    "- Changing the `random_state`, or not specifying a random state, can drastically change the results (the more trees there are in the forest the more robust the Random Forest will be from the choice of random state)\n",
    "- Doesn't tend to perform well on high dimensional, sparse data (like text data)\n",
    "- Requires more memory and is slower to train / predict than linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations\n",
    "\n",
    "### Extra-Trees\n",
    "\n",
    "Extra-trees, also know as *Extremely Randomized Trees*, is a Random Forest model with an extra element of randomness. Instead of using a random subset of features with the best threshold for each feature like Random Forest does, Extra-Trees uses a random threshold. Since finding the best threshold for the features is one of the most time-consuming aspects of training models, this makes training Extra-Trees substantially faster to train.\n",
    "\n",
    "The extra randomness trades more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n",
    "- [Hands-On Machine Learning with Scikit-Learn & TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291)\n",
    "- [Machine Learning with Python Cookbook](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
