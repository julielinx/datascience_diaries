{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 55 - AdaBoost\n",
    "\n",
    "Ensemble learning that combines multiple Decision Trees where each new tree attempts weight the training data to correct the mistakes of the previous tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Style\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <th>Supervision</th>\n",
    "        <th>Prediction types</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Supervised</td>\n",
    "        <td>Regression</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Classification</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "AdaBoost uses weights to give more importance to observations with incorrect predictions. It does this by first training a base model and using that model to make predictions. It then adjusts the weights for the observations to give more importance to observations that had incorrect predictions. The next model is trained on the same training dataset, but with weights also in consideration. Additional trees are trained using the same process:\n",
    "\n",
    "1. Train\n",
    "2. Predict\n",
    "3. Update weights\n",
    "\n",
    "[Hands-On Machine Learning](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291) has a nice figure (Figure 7-7) on page 200 that illustrates this process, but I wasn't able to find a digital version of it to include here. However, the [Hands-On Machine Learning Chapter 7 github page](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb) does have the illustration of how this process reflects in the accuracy of the trees and how the learning rate effects the amount of change allowed between models:\n",
    "\n",
    "<img src='images/adaboost_learning_rate.png'>\n",
    "\n",
    "To me, this looks pretty similar to the way Gradient Descent works conceptually (see [Entry 37b](https://julielinx.github.io/blog/37b_regression_gradient_descent/) for more on Gradient Descent). The ensemble moves toward a best solution with the step size controled by the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "To allow the model to predict more reliabily on harder cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior\n",
    "\n",
    "- Can be computationally expensive and time consuming to train\n",
    "- Models are trained sequentially, so unable to use parallel processing for training\n",
    "- The learning rate must be specified and I imagine it presents the same challenges as the learning rate as discussed in [Entry 37b](https://julielinx.github.io/blog/37b_regression_gradient_descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "- `learning_rate`: controls how strongly the subsequent model attempts to correct the mistakes of the previous tree\n",
    "- `n_estimators`: the number of trees to build\n",
    "  - a high numbers of trees/`n_estimators` can lead to overfit models\n",
    "  - more trees are needed when the `learning_rate` is low to build models of similar complexity\n",
    "- `max_depth`: sets the maximum depth of each Decision Tree\n",
    "- `max_leaf_nodes`: the maximum number of leaf nodes allowed in each individual Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Hands-On Machine Learning with Scikit-Learn & TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
