{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 54 - Gradient Boost\n",
    "\n",
    "Ensemble learning that combines multiple Decision Trees where each tree attempts to improve the mistakes of the previous tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Style\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <th>Supervision</th>\n",
    "        <th>Prediction types</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Supervised</td>\n",
    "        <td>Regression</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Classification</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Gradient Boost trains Decision Trees sequentially. It uses the errors from the previous tree as the input for the following tree. By default, there is no randomness to gradient boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "*Introduction to Machine Learning with Python* page 93: \n",
    "\n",
    "> As both gradient boosting and random forests perform well on similar kinds of data, a common approach is to first try random forests, which work quite robustly. If random forests work well but prediction time is at a premium, or it is important to squeeze out the last percentage of accuracy from the machine learning model, moving to gradient boosting often helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior\n",
    "\n",
    "Often the maximum depth or maximum number of nodes is set very low; *Introduction to Machine Learning with Python* states on page 94 that it is often not deeper than five splits. The implication on page 90 is that strong pre-pruning is used in lieu of randomness. The small size of the trees has the added benefit of using less memory and making predictions faster.\n",
    "\n",
    "The small size of the trees also means that the trees that comprise Gradient Boost are *weak learners* (see the \"Weak and Strong Learners\" section of [Entry 51]() for more details). Each tree only preforms well on a specific subset of the data, but when many trees are added together, the overall prediction generalizes well across the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "- `learning_rate`: controls how strongly the subsequent model attempts to correct the mistakes of the previous tree. Per page 205 of [Hands-On Machine Learning](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291):\n",
    "  - Lower values (such as 0.1) require more trees in the ensemble to fit the data\n",
    "  - Low values combine with more trees usually produce ensembles that generalize better\n",
    "  \n",
    "- `n_estimators`: the number of trees to build\n",
    "  - a high numbers of trees/`n_estimators` can lead to overfit models\n",
    "  - more trees are needed when the `learning_rate` is low to build models of similar complexity\n",
    "- `max_depth`: sets the maximum depth of each Decision Tree\n",
    "- `max_leaf_nodes`: the maximum number of leaf nodes allowed in each individual Decision Tree\n",
    "\n",
    "The ensemble is sensitive to changes in the parameters as can be seen in this illustration from the [Hands-On Machine Learning Chapter 7 gitthub page](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb) with two very different sets of parameters for the learning rate and number of estimators:\n",
    "\n",
    "<img src='images/under_overfit_ensembles.png'>\n",
    "\n",
    "*Hands-On Machine Learning* suggests two different ways to find the best number of trees.\n",
    "\n",
    "The first technique uses the `staged_predict` method, which will save your information for each tree. You can then make a chart that shows the errors after each tree, allowing you to find the minimum (chart from the [Hands-On Machine Learning Chapter 7 gitthub page](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb) - code available on Aurelien's github page or the Jupyter Notebook accompanying this entry):\n",
    "\n",
    "<img src='images/ensemble_trees_chart.png'>\n",
    "\n",
    "This technique is a good one to remember because it translates to finding optimal hyperparameters for other algorithms.\n",
    "\n",
    "The second technique uses the `warm_start`=True parameter, which keeps each tree as it's fit. From there you can use a for loop and a variable to keep track of reductions in error, then stop training when the validation metric doesn't improve after so many iterations (*Hands-On Machine Learning* uses 5). The code for this technique is available on Aurelien's [Chapter 7 github page](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb) or the Jupyter Notebook accompanying this entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "- Can provide better accuracy than Random Forests with the right parameter settings\n",
    "- Minimal preprocessing required (like centering and scaling - see [Entry 8](https://julielinx.github.io/blog/08_center_scale_and_latex/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- Can be computationally expensive and time consuming to train\n",
    "- Models are trained sequentially, so unable to use parallel processing for training\n",
    "- More sensitive to paramenter setting than Random Forests - careful tuning required\n",
    "- I imagine the learning rate presents the same challenges as the learning rate as discussed in [Entry 37b](https://julielinx.github.io/blog/37b_regression_gradient_descent/) while discussing gradient descent\n",
    "- Doesn't usually work well on high deminsonial sparce data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations\n",
    "\n",
    "### Stochastic Gradient Boosting\n",
    "\n",
    "A random subsample of the training data can be used for training each tree using the `subsample` hyperparameter. Here's what *Hands-On Machine Learning* has to say on page 207:\n",
    "\n",
    "> For example, if `subsample` = 0.25, then each tree is trained on 25% of the training instances, selected randomly. As you can probably guess by now, this technique trades a higher bias for a lower variance.\n",
    "\n",
    "Since a much smaller amount of data is used, training time is reduced considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Hands-On Machine Learning with Scikit-Learn & TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291)\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
