{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### Equation\n",
    "\n",
    "It took me a bit to work my way through the math of gradient descent in Andrew Ng's [Machine Learning course](https://www.coursera.org/learn/machine-learning).\n",
    "\n",
    "<img src='../img/gd_working_board.jpg' width=\"656\" height=\"400\">\n",
    "\n",
    "It's been a couple months since I went through it, so I guess it's time to re-explain it to myself.\n",
    "\n",
    "First things first: the symbol, name, dimensions, and an example of the arrays/matrices used in the calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td><b>Symbol</b></td>\n",
    "        <td><b>$\\alpha$</b></td>\n",
    "        <td><b>y</b></td>\n",
    "        <td><b>X</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Name</b></td>\n",
    "        <td>theta array / alpha / learning rate</td>\n",
    "        <td>target array</td>\n",
    "        <td>Feature matrix</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Dimensions</b></td>\n",
    "        <td>(n x 1)</td>\n",
    "        <td>(m x 1)</td>\n",
    "        <td>(m x n)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Example</b></td>\n",
    "        <td><img src='../img/gd_theta_array.jpg' width=\"209\" height=\"250\"/></td>\n",
    "        <td><img src='../img/gd_target_array.jpg' width=\"201\" height=\"250\"/></td>\n",
    "        <td><img src='../img/gd_matrix.jpg' width=\"343\" height=\"250\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m (h_{\\theta}(x_{(i)}) - y_{i}x_{j}^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- $\\alpha$ (alpha) = learning rate\n",
    "- m = the length of the training array, ie the number of training examples\n",
    "- n = the length of the theta array, ie the number of features. Also the number of rows of X\n",
    "- $\\theta$ (theta) = weights -> (n x 1)\n",
    "- y = target array -> (m x 1)\n",
    "- X = training examples -> (m x n)\n",
    "\n",
    "Equations:\n",
    "\n",
    "- $\\hat{y} = h_{0}(x) = \\theta x$\n",
    "- $h_{\\theta}(x) = g(\\theta^{T}x) = \\frac{1}{1+e^{}}$\n",
    "- $Z = X \\theta = \\theta^{T}x$ -> (m x n) x (n x 1 ) = (m x 1)\n",
    "- $J = \\frac{1}{m} ()$\n",
    "- Mean square error = $MSE = \\frac{1}{n} \\sum (y_{i} - \\hat{y_{i}})^2$\n",
    "- Cost function = $J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^m(\\hat{y}_{i} - y_{i})^2 = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^m(h_{\\theta}(x_{i}) - y_{i})^2$\n",
    "- Gradient descent algorithm = $\\theta_{j}:=\\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta_{0}, \\theta_{1})$\n",
    "- Regularized cost function = $\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$\n",
    "\n",
    "Notice how closly the equation for the cost function resembles the equation for the mean square error. This is because the cost function is just a squared error function. The *Machine Learning course* has this to say about the cost function:\n",
    "\n",
    "> [It] takes an average difference (actually a fancier version of an average) of all results of the hypothesis with inputs from x's and the actual output y's.\n",
    "\n",
    "Notice in the gradient descent algorithm that at each step it has to be run over the full training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
