{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 30 - Modeling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "There are a lot of machine learning algorithms. And there are even more programmatic or mathematic implementations of them. All of my machine learning books group these algorithms by either learning style (Supervised, Unsupervised, and Semisupervised) or by functional similarity (regression, classification, clustering, SVM, neural networks, trees, etc).\n",
    "\n",
    "The challenge in this series of entries is to become familiar with the major categories of algorithms, how to implement them, determine the common strengths and weaknesses of each category, and figure out what parameters are available for tuning the model.\n",
    "\n",
    "One thing to keep in mind when exploring the strenghts and weaknesses of algorithms is to understand that different algorithms are designed to find different kinds of patterns. Some will find linear patterns, others exponential / logarithmic / polynomial (curve) patterns. Yet others can find patterns such as a cluster, or a circle.\n",
    "\n",
    "<img src='../img/charts.png'>\n",
    "\n",
    "Of course, all of this becomes much more theoretical when there are over 400 features and some of them are numerical, some are categorical, some are representations of categorical, etc. [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413) points out on page 34 that\n",
    "\n",
    "> Any intuition derived from datasets with few features (also called *low-dimensional* datasets) might not hold in datasets with many features (*high-dimensional* datasets). As long as you keep that in mind, inspecting algorithms on low-dimensional datasets can be very instructive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Options\n",
    "\n",
    "In keeping with the spirit of prior entries, I want to devote one or more entries to each major category of algorithm. The number of entries dedicated to a particular category will depend on the number of subcategories and how different those subcategories are from each other.\n",
    "\n",
    "As for organization of the categories, a combination of learning style and functional similarity seems to fit the bill. While most of my books break the algorithms into categories by functional similarity, each of these categories generally falls under either Supervised or Unsupervised learning.\n",
    "\n",
    "- *Supervised learning* is when there is a correct answer that is known.\n",
    "  - This is the type of algorithm I've been running in my previous entries.\n",
    "  - For example, horse colic, either the horse died of it or didn't die of it - yes or no.\n",
    "- *Unsupervised learning* is when the answer (known or not) isn't used.\n",
    "  - The MNIST digits dataset is often used this way. By clustering numbers that the model thinks look the same, groups can be formed that include similar looking numbers - like all 9s. Such groupings can also include other similar looking numbers like 4s. \n",
    "  - During the creation of the groupings, the algorithm doesn't take into account the labels (ie either 9 or 4).\n",
    "  - A real world use case for this type of learning style would be when the true answer isn't known, or is partially known, such as in the case of fraud. By creating groups of similar claims, fruad that had previously gone unrecognized (and thus unlabelled), could then be identified and acted upon.\n",
    "  \n",
    "I reviewed all the algorithms discussed in my books and broke them into categories by functional similarity and then organized them by learning style. This process gave me the following list:\n",
    "\n",
    "### Supervised learning\n",
    "- Regression\n",
    "  - Linear regression\n",
    "    - Ordinary least squares regression (OLSR)\n",
    "      - Normal Equation\n",
    "      - Gradient Descent\n",
    "  - Regularization algorithms\n",
    "    - Least absolute shrinkage and selection operator (LASSO)\n",
    "    - Ridge regression\n",
    "    - Elastic net\n",
    "    - Least-angle regression (LARS)\n",
    "  - Stepwise regression\n",
    "  - Multivariate adaptive regression splines (MARS)\n",
    "  - Locally estimated scatterplot smoothing (LOESS)\n",
    "  - Polynomial regression\n",
    "  - Polynomials and splines\n",
    "  - Splines\n",
    "- Classification\n",
    "  - Logistic regression\n",
    "  - Decision trees\n",
    "    - Classification and regression tree (CART)\n",
    "    - Interative dichotomiser 3 (ID3)\n",
    "    - C4.5 and c5.0\n",
    "    - Chi-squared automatic interaction detection (CHAID)\n",
    "    - Decision stump\n",
    "    - M5\n",
    "    - Conditional decision trees\n",
    "  - Ensemble models\n",
    "    - Boosting\n",
    "    - Bagging (Bootstrapped aggregation)\n",
    "    - AdaBoost\n",
    "    - Weighted average (Blending)\n",
    "    - Stacked generalization (Stacking)\n",
    "    - Gradient boosting machines\n",
    "    - Gradient boosted regression trees \n",
    "    - Random forests\n",
    "- Support vector machines\n",
    "- Bayesian\n",
    "  - Naïve bayes\n",
    "  - Gaussian naïve bays\n",
    "  - Multinomial naïve bays\n",
    "  - Average one-dependence estimators (AODE)\n",
    "  - Bayesian belief network (BBN)\n",
    "  - Bayesian network (BN)\n",
    "- Discriminant analysis\n",
    "  - Linear discriminant analysis\n",
    "  - Nonlinear discriminant analysis\n",
    "  - Flexible discriminant analysis\n",
    "  - Mixture discriminant analysis\n",
    "  - Quadratic discriminant analysis\n",
    "\n",
    "### Unsupervised learning\n",
    "- Clustering\n",
    "  - K-Means clustering\n",
    "  - K-Medians\n",
    "  - DBSCAN\n",
    "  - K-Nearest neighbors\n",
    "  - Nearest shrunken centroids\n",
    "  - Expectation maximization (EM)\n",
    "  - Hierarchical clustering\n",
    "- Associated rule learning algorithms\n",
    "  - Apriori algorithm\n",
    "  - Market basket analysis\n",
    "  - Eclat algorithm\n",
    "- Gaussian mixture models\n",
    "- Dimensionality reduction\n",
    "  - Principal component analysis (PCA)\n",
    "  - Principal component regression (PCR)\n",
    "  - Partial least squares regression (PLSR)\n",
    "  - Multidimensional scaling (MDS)\n",
    "  - Projection pursuit\n",
    "\n",
    "### Other\n",
    "- Kernel methods\n",
    "- Manifold learning\n",
    "- Markov graphs\n",
    "- Markov models\n",
    "- Network graphs\n",
    "- Undirected graphical models\n",
    "- Outlier detection\n",
    "- Partial least squares\n",
    "- Penalized models\n",
    "- Neural networks\n",
    "- Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Proposed Solution\n",
    "\n",
    "In order to make these entries easy to reference in the future, I'm going to need to change my typical Problem - Options - Solution - Fail layout. As a start, I used some of the advice in Jason Brownlee's \"How to Study Machine Learning Algorithms\" series of posts, mostly the post [How to Learn a Machine Learning Algorithm](https://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/) (see \"Machine Learning Mastery resources\" in the Resources section below for the list of the full series). In reading those posts, I also added other considerations as they occurred to me.\n",
    "\n",
    "- Learning style\n",
    "  - Supervised vs unsupervised\n",
    "  - If supervised, what kind of target does it accept (numerical, categorical)\n",
    "  - Information processing strategy\n",
    "- Description\n",
    "  - The general description of the algorithm\n",
    "  - Name and standard abbreviation(s)\n",
    "  - Methaphors or analogies commonly used to describe the algorithm's behavior\n",
    "- Purpose\n",
    "  - Why the algorithm was created and what it is designed to find\n",
    "  - General class of problem the algorithm is suited to address\n",
    "- Behavior\n",
    "  - Heuristics or rules of thumb for using the algorithm\n",
    "- Parameters\n",
    "  - What parameters can be used to fine tune the model\n",
    "- Strengths\n",
    "  - What does it do well\n",
    "  - In what ways is it better or easier to use than other algorithms\n",
    "- Limitations\n",
    "  - Can it accept null values\n",
    "  - Do all features need to be numerical\n",
    "  - Do features need to be normalized (standardized 0-1 value ranges give very different results than unstandardized ranging between 0 and 1 million)\n",
    "- Evaluation\n",
    "  - What metrics are standard for evaluating the algorithm\n",
    "  - What are common benchmarks for the algorithm\n",
    "- Datasets\n",
    "  - Example datasets commonly used to demonstrate the algorithm\n",
    "- Resources\n",
    "  - Where to go to learn more about the algorithm\n",
    "  - Google Scholar and GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fail\n",
    "\n",
    "Alright, this is an anticipated fail, but in order to develop an intuitive understanding and have examples of how each algorithm works (and what it works on), I'll need multiple, well understood datasets for each algorithm. This sounds like a huge time suck to find the datasets and layout (or discover) what kinds of patterns they characterize. Maybe I'll get lucky and find a paper that does all that for me. Or maybe there's something in one of my many books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning Mastery resources:\n",
    "  - [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)\n",
    "  - [Take Control By Creating Targeted Lists of Machine Learning Algorithms](https://machinelearningmastery.com/create-lists-of-machine-learning-algorithms/)\n",
    "  - [How to Learn a Machine Learning Algorithm](https://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/)\n",
    "  - [How to Research a Machine Learning Algorithm](https://machinelearningmastery.com/how-to-research-a-machine-learning-algorithm/)\n",
    "  - [How To Investigate Machine Learning Algorithm Behavior](https://machinelearningmastery.com/how-to-investigate-machine-learning-algorithm-behavior/)\n",
    "  - [How to Study Machine Learning Algorithms](https://machinelearningmastery.com/how-to-study-machine-learning-algorithms/)\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
