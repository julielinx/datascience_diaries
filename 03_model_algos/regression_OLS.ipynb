{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry ? - Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Style\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <th>Supervision</th>\n",
    "        <th>Prediction types</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Supervised</td>\n",
    "        <td>Regression</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "On page 49, *Introduction to Machine Learning with Python* calls this \"the simplest and most classic linear method for regression.\" It is usually the default method of Linear Regression and is the method used in the `sklearn.linear_model.LinearRegression` function.\n",
    "\n",
    "This method uses the mean squared error (MSE) to find the best fit line.\n",
    "\n",
    "I covered mean squared error in [Entry 21](https://julielinx.github.io/blog/21_reg_score_theory/), but here's a reminder along with how the equation would be applied over a dataset/matrix:\n",
    "\n",
    "- **error**: the difference between predictions and the true value\n",
    "- **squared error**: literally square the error term. This makes all values positive. Squaring is used instead of absolute value in order to make outlier terms more important\n",
    "- **mean squared error**: sum the squared error of all data points and divide by the number of data points\n",
    "\n",
    "$MSE(X, h_{\\theta}) = \\frac{1}{m} \\sum (\\theta^{T}x^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Where:\n",
    "\n",
    "- X: matrix of features\n",
    "- $h_{\\theta}$: prediction function, also called a *hypothesis*\n",
    "- $\\theta$: array of weights\n",
    "- $x^{(i)}$: array of features for a specific observation\n",
    "- $y^{(i)}$: observed output for a specific observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "OLS is basically the starting point for linear regression. It calculates the theta array (ie the weights) used to calculate the output from an array of inputs.\n",
    "\n",
    "There are two options when calculating the theta array:\n",
    "\n",
    "- Normal equation\n",
    "- Gradient descent\n",
    "\n",
    "When the matrix has an inverse, it is calculated using the normal equation. When there is no inverse, then the iterative process of gradient descent is used.\n",
    "\n",
    "### Normal Equation\n",
    "\n",
    "$\\hat{\\theta} = (X^{T} X)^{-1} X^{T} y$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\hat{\\theta}$: theta array, the hypothesized weights\n",
    "- X: input feature matrix\n",
    "- $X^{T}$: the transpose of X\n",
    "- y: array of target values\n",
    "\n",
    "### Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior\n",
    "\n",
    "Normal equation vs gradient descent\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td><b>Comparison category</b></td>\n",
    "        <td><b>Normal Equation</b></td>\n",
    "        <td><b>Gradient Descent</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Alpha</td>\n",
    "        <td>No need to choose alpha</td>\n",
    "        <td>Need to choose alpha</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Interation</td>\n",
    "        <td>No need to iterate</td>\n",
    "        <td>Needs many iterations</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Computational complexity</td>\n",
    "        <td>$O(n^{3})$ (need to calculate inverse of $X^{T}X$) *</td>\n",
    "        <td>$O(kn^{2})$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Speed with large feature set</td>\n",
    "        <td>Slow if *n* is very large</td>\n",
    "        <td>Works well when *n* is large</td>\n",
    "    </tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Scikit-learn implementation of OLS uses psudoinverse instead of inverse, resolving this limitation.\n",
    "\n",
    "Where:\n",
    "\n",
    "- *n*: number of features\n",
    "- **X**: feature matrix\n",
    "\n",
    "#### Computational complexity\n",
    "\n",
    "A few notes on computational complexity. The formulas in the table above are from **Reading: Normal Equation** in week 2 of Andrew Ng's [Machine Learning](https://www.coursera.org/learn/machine-learning) course. On that slide, he also notes that:\n",
    "\n",
    "> [...] if we have a very large number of features, the normal equation will be slow. In practice, when *n* exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\n",
    "\n",
    "*Hands-On Machine Learning with Scikit-Learn* adds the following in relation to feature size and computational complexity:\n",
    "\n",
    "> Both the Normal Equation and the SVD approach get very slow when the number of features grows large (e.g., 100,000). On the positive side, both are linear with regard to the number of instances in the training set (they are both *O(m)*), so they handle large training sets efficiently, provided they can fit in memory.\n",
    "\n",
    "It also adds that the computational complexity for the SVD implementation of the normal equation in `sklearn.linear_model.LinearRegression` is $O(n^{2})$. This puts it at roughly the same computational complexity as gradient descent. However, everything still has to fit into memory and *Hands-On Machine Learning with Scikit-Learn* purports on page 122 that gradient descent is much faster than normal equation or SVD when there are hundreds of thousands of features. Gradent descent is still a better choice for large datasets due to these two properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "- Fast to train\n",
    "- Fast to predict\n",
    "  - Computational complexity is linear\n",
    "  - Ex: it takes twice as long to predict on twice as many instances (or twice as many features)\n",
    "- Easily scale to very large datasets\n",
    "- Work well with sparse data\n",
    "- Easy to intrepret / easy to see feature importance\n",
    "- Performs well when the number of features is large compared to the number of observations (ex, 104 features but only 5 observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- In low dimensions, linear models appear to have very limited usefulness. However, as more dimensions are added, the model becomes more powerful and can become overfit\n",
    "- Often unclear why coefficients are the what they are, particularly if there are highly correlated features\n",
    "- Features should be scaled to improve the algorithms ability/speed to converge on the correct solution (if you've forgotten what centering and scaling are, see [Entry 8](https://julielinx.github.io/blog/08_center_scale_and_latex/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n",
    "- [Hands-On Machine Learning with Scikit-Learn](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n",
    "- [Machine Learning course](https://www.coursera.org/learn/machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
