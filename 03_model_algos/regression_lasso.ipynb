{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry ? - Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Style\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <th>Supervision</th>\n",
    "        <th>Prediction types</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Supervised</td>\n",
    "        <td>Regression</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This is a form of regression regularization. When I think of \"regularization,\" I generally thinking of [centering, scaling, and normalization](https://julielinx.github.io/blog/08_center_scale_and_latex/). However, in this case, *Introduction to Machine Learning using Python* defines it this way on page 51:\n",
    "\n",
    "- **Regularization**: explicitly restricting a model to avoid overfitting.\n",
    "\n",
    "This method similar to Ridge Regression in that it uses the same formula as OLS and minimizes the magnitude of coefficients (*w*), but instead of using L2 regularization, Lasso Regression uses L1 regularization.\n",
    "\n",
    "*Introduction to Machine Learning with Pythong* puts it succinctly on page 55:\n",
    "\n",
    "> The consequence of L1 regularization is that when using the lasso, some coefficients are *exactly zero*. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "The outcome of keeping the coefficients as small as possible helps reduce the complexity of the model (ie makes it more generalized and reduces overfitting). The complexity of the model is controlled through the parameter `alpha`. Larger `alpha` values force the coefficients closer to 0 (reducing complexity), smaller values are less restricting (allowing for more complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "- `alpha`\n",
    "- `max_iter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strengths\n",
    "\n",
    "Having some of the coefficients be exactly zero can make a model easier to interpret and make more important features stand out. Coefficients that are 0 can also function as an automated form of feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "If `alpha` is set too high, very few features will be used and the model will be underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
