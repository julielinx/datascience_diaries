{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 8 - Centering and Scaling\n",
    "\n",
    "By the end of <font color='red'>Entry 7</font> I'd finalized the feature set to train a model to make predictions. One last step is needed before a prediction can be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "According to [*Hands-On Machine Learning with Scikit-Learn & TensorFlow*](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291) by Aurelien Geron: \"With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales.\"\n",
    "\n",
    "Algorithms like linear regression prefer normalized and standardized values. So when the range (the max value minus the min value: max - min) of values in one column is less than 23 (escape_vel_km_s) and the range in another column is more than 140,000 (diameter_km), the algorithm takes longer to reach an answer and will probably be biased toward features with larger ranges.\n",
    "\n",
    "There are various ways to do this and the terms quickly become confusing: centering, scaling, normalization, and standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Options\n",
    "\n",
    "According to Andrew Ng in the [Machine Learning](https://www.coursera.org/learn/machine-learning/) course by Stanford on Coursera, feature scaling and normalization can be defined as follows:\n",
    "\n",
    "**Feature scaling**: dividing the input values by the range of the input variable, resulting in a new range of 1. Basically, you bring all the features within the same range of values.\n",
    "\n",
    "Example: The minimum diameter_km is 2370 and the max is 142,984. To scale Mercury's diameter of 4879:\n",
    "- Mercury's value = 4,879\n",
    "- diameter_km range = 142,984 - 2,370 = 140,614\n",
    "- standardized value = $\\frac{4879}{140614}$ = 0.0347\n",
    "\n",
    "**Mean normalization**: subtracting the average for an input vairable from the value for the input variable, resulting in a new average value for the input variable of just zero. IE: this resets the mean value of the feature to 0.\n",
    "\n",
    "Example: To normalize the mass_1024_kg of Mercury:\n",
    "- Mercury's value = 0.33\n",
    "- mass_1024kg mean = 242.438\n",
    "- normalized value = 0.330 - 242.438 = -242.108\n",
    "\n",
    "### More definitions\n",
    "\n",
    "Cross referencing this with *Hands-On Machine Learning* and [*Applied Predictive Modeling*](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485) by Max Kuhn and Kjell Johnson produced some more helpful definitions and equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math symbols\n",
    "\n",
    "First, some helpful references:\n",
    "\n",
    "- $\\mu$ = mean\n",
    "- $\\sigma$ = standard deviation\n",
    "- *z* = z-score\n",
    "- X = a list (or column) of values\n",
    "- x = a specific value within X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Centering**\n",
    "\n",
    "*Applied Predictive Modeling*'s definition: the average predictor value is subtracted from all the values\n",
    "\n",
    "This transformation results in a new mean of 0. Mathematically it can be represented with the following equation:\n",
    "\n",
    "$x_{cen} = x - \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "Scaling transformations put all the features on the same scale, usually 0 to 1 or -1 to 1.\n",
    "\n",
    "This can be done via normalization (dividing by the range) or standardization (dividing by the standard deviation). In addition to making the features easier for the machine learning algorithms to use, scaling also generally allows dissimilar features to be compared. For example, where as comparing a diameter of 4,879 and a gravity of 3.7 isn't particularly enlightening, comparing mean normalized scaled features of diameter 0.0347 and gravity 0.165 shows that Mercury is on the low end for both features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-max scaling (normalization)**\n",
    "\n",
    "Geron's definition: values are shifted and rescaled so that they end up ranging from 0 to 1.\n",
    "\n",
    "Based on his explanation, the important part of the equation is to divide by the range. The numerator can be the raw value, the centered value, or the value minus the min. As such, the equations for min-max scaling include:\n",
    "\n",
    "$\\displaystyle x_{scale} = \\frac{x}{X_{max}-X_{min}}$\n",
    "\n",
    "$\\displaystyle x_{minscale} = \\frac{x-x_{min}}{X_{max}-X_{min}}$\n",
    "\n",
    "$\\displaystyle x_{norm} = \\frac{x-\\mu}{X_{max}-X_{min}}$\n",
    "\n",
    "Based on my reading, normalization is usually defined as the last equation - the centered value divided by the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization**\n",
    "\n",
    "Interestingly, *Hands-On Machine Learning* and *Applied Predictive Modeling* differ in their definitions of standardization. *Hands-On Machine Learning* says to divide by the variance. *Applied Predictive Modeling* says to divide by the standard deviation. Variance is just the square of standard deviation ($\\sigma^2$).\n",
    "\n",
    "I'm going to use Andrew Ng as the tie breaker. He uses standard deviation for standardization. As such, the equation for standardization is:\n",
    "\n",
    "$\\displaystyle x_{stand} = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "The benefit of standardization is that the value is centered with a mean of 0 and a standard deviation of 1. The benefit of this is that the standardized value is less affected by outliers.\n",
    "\n",
    "Side note: standardization doesn't bound values to a specific range the way normalization does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downside**\n",
    "\n",
    "Per *Applied Predictive Modeling* 'the only real downside to these transformations is a loss of interpretability of the individual values since the data are no longer in the original units.' However, as discussed in The Fail section the transformation parameters can be saved, which would allow the transformation to be reversed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Proposed Solution\n",
    "\n",
    "The features in the planets dataset certainly have a wide variety ranges. As such, normalization or standardization are recommended. Based on the EDA completed in <font color='red'>Entry 5</font>, Jupiter tends to be an outlier in several of the features (See? The visualization step was useful. I wouldn't have known this if I'd only done the automated correlation/collinear step. Which is a good argument for mathematically determing skewness and outliers if the visualization step is eliminated). Based on this, I applied standardization to all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fail\n",
    "\n",
    "### Time\n",
    "\n",
    "Most of the time I work on these entries over a couple sessions as I have time. This time, I sat down and did it all in one sitting. I assumed it wouldn't take long. I already had the basic code to run the standardization and the dataset had already been defined, so the code itself only took about 20 minutes. The write up, however, took over 3.5 hours.\n",
    "\n",
    "While this is a fail in that it took longer than the goal of 1-2 hours to complete, it also highlights the importance of breaking the problem out into the smallest level task as possible. There are always unexpected things that pop up or tasks that take longer than anticipated. Just getting the equations and mathematical symbols into the entry took about a half hour of research on LaTeX. And while I thought I understood normalization and standardization, once I tried to write down the definitions and confirm, I realized that I was actually wrong and that there are four terms for what I thought were two concepts.\n",
    "\n",
    "This also emphasize the importance of these write up. If I hadn't done the write up with the equations and definitions, I would have continued thinking I understood when there was actuallky a hole in my knowledge base.\n",
    "\n",
    "### Reproducibility - edit\n",
    "\n",
    "I didn't realize this until I was trying to transform the 'test' data, but the same parameters used to transform the training data need to be used on the test data. Academically, I knew I'd need to transform the test data, but the practical task of actually retaining that information completely slipped my mind.\n",
    "\n",
    "The transformation parameters as applied to the training data can be saved to be applied to the test set. Since I didn't notice the problem until I needed to transform the test data and this entry was already complete, I'm going to leave this entry the way it is and change the code to the functions that retain the transformation information in the notebook where I noticed this was a problem. See <font color='red'>Entry X</font> for the corrected functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "For more on scaling and centering see:\n",
    "- [About Feature Scaling and Normalization â€“ and the effect of standardization for machine learning algorithms](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "- [How, When and Why Should You Normalize / Standardize / Rescale Your Data?](https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff)\n",
    "\n",
    "LaTeX/Mathematics is available in Jupyter Notebook thanks to MathJax, which is included as part of the notebook functionality (no library import needed). Some references on how to use LaTex in Jupyter are:\n",
    "\n",
    "- [Wikibooks](https://en.wikibooks.org/wiki/LaTeX/Mathematics#List_of_mathematical_symbols): lists of symbols, operators, and examples\n",
    "- [Motivating Examples](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html): Examples from Jupyter's read the docs\n",
    "- [Spacing in math mode](https://www.overleaf.com/learn/latex/Spacing_in_math_mode): how to add spaces in equations\n",
    "- [Advanced Jupyter Notebooks](https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/): includes a lot of Magics functions, including %%latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
