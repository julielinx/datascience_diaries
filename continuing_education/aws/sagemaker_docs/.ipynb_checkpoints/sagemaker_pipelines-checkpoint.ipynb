{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda73921-b421-471f-806a-bf56080fc9ce",
   "metadata": {},
   "source": [
    "# SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0e8e5-50ec-4f7a-aa5b-be5e080eea02",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46625aab-0b0d-4b9f-b3bb-ff5357ab2635",
   "metadata": {},
   "source": [
    "## Wrangler\n",
    "\n",
    "- **Import**\n",
    "    - Connect to and import data.\n",
    "    - Sources: \n",
    "        - S3 (Amazon Simple Storage Service)\n",
    "        - Athena\n",
    "        - Redshift\n",
    "        - Databricks / JDBC (java database connectivity)\n",
    "        - Snowflake\n",
    "    - Formats:\n",
    "        - CSV (comma separated values)\n",
    "        - Parquet\n",
    "        - JSON (javascript object notation)\n",
    "        - ORC (optimized row columnar)\n",
    "- **Data Flow**\n",
    "    - Create a data flow to define a series of ML data prep steps. You can use a flow to combine datasets from different data sources, identify the number and types of transformations you want to apply to datasets, and define a data prep workflow that can be integrated into an ML pipeline.\n",
    "- **Transform**\n",
    "    - Clean and transform your dataset using standard transforms like string, vector, and numeric data formatting tools. Featurize your data using transforms like text and date/time embedding and categorical encoding.\n",
    "- **Analyze**\n",
    "    - Analyze features in your dataset at any point in your flow. Data Wrangler includes built-in data visualization tools like scatter plots and histograms, as well as data analysis tools like target leakage analysis and quick modeling to understand feature correlation.\n",
    "    - Our data is too big and varied to use the Data Wrangler analysis. We'll need to do this step separately using technology designed to handle big data.\n",
    "- **Export**\n",
    "    - Data Wrangler offers export options to other SageMaker services, including Data Wrangler jobs, Amazon SageMaker Feature Store, and pipelines, giving you the ability to integrate your data prep flow into your ML workflow. You can also export your Data Wrangler flow to Python code.\n",
    "    \n",
    "AWS recommends using Pyspark for datasets over 2GB. Parquet file format is designed to work well with Spark. Pickle is not natively supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09118786-cf27-4231-a6f4-2d03acaa8acd",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Prepare and Analyze Datasets](https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html)\n",
    "- [Prepare ML Data with Amazon SageMaker Data Wrangler](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html)\n",
    "- [Data Wrangler - Import](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa3ac3-c24b-45b9-9f78-31b4a5b14124",
   "metadata": {},
   "source": [
    "## Clarify\n",
    "\n",
    "- Detect data biases\n",
    "- Can be done:\n",
    "    - before training\n",
    "    - after training\n",
    "    - after deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe3a40-7423-49c2-92d2-cd8e463c516f",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Detect Pretraining Data Bias](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-detect-data-bias.html)\n",
    "- [Sample Notebook](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.html)\n",
    "- [Measure Pretraining Bias](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-data-bias.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1d922-c9dc-4731-9fdd-4917757b6620",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87f0f7-db52-4667-8889-495dcb6be84c",
   "metadata": {},
   "source": [
    "## Permissions\n",
    "\n",
    "The role for the SageMaker instance that is creating the pipeline must have the `iam:PassRole` permission for the pipeline execution role in order to pass it.\n",
    "\n",
    "Your pipeline execution role requires the following permissions:\n",
    "\n",
    "- To pass any role to a SageMaker job within a pipeline, the `iam:PassRole` permission for the role that is being passed. \n",
    "- `Create` and `Describe` permissions for each of the job types in the pipeline.\n",
    "- Amazon S3 permissions to use the `JsonGet` function. You control access to your Amazon S3 resources using resource-based policies and identity-based policies. A resource-based policy is applied to your Amazon S3 bucket and grants SageMaker Pipelines access to the bucket. An identity-based policy gives your pipeline the ability to make Amazon S3 calls from your account. For more information on resource-based policies and identity-based policies, see [Identity-based policies and resource-based policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c769f0a-e997-4d54-9d8e-16ca4ccfe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "{\n",
    "    \"Action\": [\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:HeadObject\"\n",
    "    ],\n",
    "    \"Resource\": \"arn:aws:s3:::<your-bucket-arn>/*\",\n",
    "    \"Effect\": \"Allow\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468bdd2-6209-4d09-9400-422ef56abdd3",
   "metadata": {},
   "source": [
    "### Resource\n",
    "\n",
    "- [Access Management - Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-access.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad86545-0b28-4254-8c7b-978cc263a728",
   "metadata": {},
   "source": [
    "## Step Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9158b2-3c12-415a-9ad8-bfa0e181e340",
   "metadata": {},
   "source": [
    "Amazon SageMaker Model Building Pipelines support the following step types:\n",
    "\n",
    "- Processing\n",
    "- Training\n",
    "- Tuning\n",
    "- CreateModel\n",
    "- RegisterModel\n",
    "- Transform\n",
    "- Condition\n",
    "- Callback\n",
    "- Lambda\n",
    "- ClarifyCheck\n",
    "- QualityCheck\n",
    "- EMR\n",
    "- Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbd6e7-8343-4aff-8ab2-0e4429ba2b4a",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Pipeline Steps - Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html)\n",
    "- [Pipelines - Read the Docs](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html)\n",
    "- [Define a Pipeline - Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html)\n",
    "- [Orchestrating Jobs with Amazon SageMaker Model Building Pipelines](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b4c87-1f87-4d41-aef8-d9123fba5595",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "779487c5-873e-4c98-8738-b032fc4fc026",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a99b83-3f3b-4bae-bc7a-07ef5b1494c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321df0d7-06c5-49cf-b780-8503c0255d1c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
