{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda73921-b421-471f-806a-bf56080fc9ce",
   "metadata": {},
   "source": [
    "# SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0e8e5-50ec-4f7a-aa5b-be5e080eea02",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46625aab-0b0d-4b9f-b3bb-ff5357ab2635",
   "metadata": {},
   "source": [
    "## Wrangler\n",
    "\n",
    "- **Import**\n",
    "    - Connect to and import data.\n",
    "    - Sources: \n",
    "        - S3 (Amazon Simple Storage Service)\n",
    "        - Athena\n",
    "        - Redshift\n",
    "        - Databricks / JDBC (java database connectivity)\n",
    "        - Snowflake\n",
    "    - Formats:\n",
    "        - CSV (comma separated values)\n",
    "        - Parquet\n",
    "        - JSON (javascript object notation)\n",
    "        - ORC (optimized row columnar)\n",
    "- **Data Flow**\n",
    "    - Create a data flow to define a series of ML data prep steps. You can use a flow to combine datasets from different data sources, identify the number and types of transformations you want to apply to datasets, and define a data prep workflow that can be integrated into an ML pipeline.\n",
    "- **Transform**\n",
    "    - Clean and transform your dataset using standard transforms like string, vector, and numeric data formatting tools. Featurize your data using transforms like text and date/time embedding and categorical encoding.\n",
    "- **Analyze**\n",
    "    - Analyze features in your dataset at any point in your flow. Data Wrangler includes built-in data visualization tools like scatter plots and histograms, as well as data analysis tools like target leakage analysis and quick modeling to understand feature correlation.\n",
    "    - Our data is too big and varied to use the Data Wrangler analysis. We'll need to do this step separately using technology designed to handle big data.\n",
    "- **Export**\n",
    "    - Data Wrangler offers export options to other SageMaker services, including Data Wrangler jobs, Amazon SageMaker Feature Store, and pipelines, giving you the ability to integrate your data prep flow into your ML workflow. You can also export your Data Wrangler flow to Python code.\n",
    "    \n",
    "AWS recommends using Pyspark for datasets over 2GB. Parquet file format is designed to work well with Spark. Pickle is not natively supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09118786-cf27-4231-a6f4-2d03acaa8acd",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Prepare and Analyze Datasets](https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html)\n",
    "- [Prepare ML Data with Amazon SageMaker Data Wrangler](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html)\n",
    "- [Data Wrangler - Import](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd7c0d-725d-41eb-ad2c-048c16cfa50e",
   "metadata": {},
   "source": [
    "## Feature Store\n",
    "\n",
    "The processing logic for the data is authored only once, and features generated are used for both training and inference, reducing the training-serving skew. Feature Store is a centralized store for features and associated metadata so features can be easily discovered and reused. You can create an online or an offline store. The online store is used for low latency real-time inference use cases, and the offline store is used for training and batch inference.\n",
    "\n",
    "Online store is primarily designed for supporting real-time predictions that need low millisecond latency reads and high throughput writes. Offline store is primarily intended for batch predictions and model training. Offline store is an append only store and can be used to store and access historical feature data. The offline store can help you store and serve features for exploration and model training. The online store retains only the latest feature data. Feature group definitions are immutable after they are created.\n",
    "\n",
    "- **Online** – In online mode, features are read with low latency (milliseconds) reads and used for high throughput predictions. This mode requires a feature group to be stored in an online store. \n",
    "- **Offline** – In offline mode, large streams of data are fed to an offline store, which can be used for training and batch inference. This mode requires a feature group to be stored in an offline store. The offline store uses your S3 bucket for storage and can also fetch data using Athena queries. \n",
    "- **Online and Offline** – This includes both online and offline modes.\n",
    "\n",
    "To ingest features into Feature Store, you must first define the feature group and the feature definitions (feature name and data type) for all features that belong to the feature group. After they are created, feature groups are immutable. Feature group names are unique within an AWS Region and AWS account. When creating a feature group, you can also create the metadata for the feature group, such as a short description, storage configuration, features for identifying each record, and the event time, as well as tags to store information such as the author, data source, version, and more.\n",
    "\n",
    "The offline store is an append-only store, enabling Feature Store to maintain a historical record of all feature values. Data is stored in the offline store in Parquet format for optimized storage and query access.\n",
    "\n",
    "Feature Store supports combining data to produce, train, validate, and test data sets, and allows you to extract data at different points in time.\n",
    "\n",
    "Supported datatypes are: String, Integral and Fractional. \n",
    "\n",
    "Feature Store automatically builds an AWS Glue data catalog when you create feature groups and you can turn this off if you want. The following describes how to create a single training dataset with feature values from both identity and transaction feature groups created earlier in this topic. Also, the following describes how to run an Amazon Athena query to join data stored in the offline store from both identity and transaction feature groups. \n",
    "\n",
    "Feature Store offers a single API call for data ingestion called PutRecord that enables you to ingest data in batches or from streaming sources. You can use Amazon SageMaker Data Wrangler to engineer features and then ingest your features into your Feature Store. You can also use Amazon EMR for batch data ingestion through a Spark connector.\n",
    "\n",
    "After the  feature group has been created, you can also select and join data across multiple feature groups to create new engineered features in Data Wrangler and then export your data set to an S3 bucket. \n",
    "\n",
    "Amazon SageMaker Feature Store supports batch data ingestion with Spark, using your existing ETL pipeline, or a pipeline on Amazon EMR. You can also use this functionality from an Amazon SageMaker Notebook Instance. Python developers can use the Amazon SageMaker-feature-store-pyspark Python library for local development, installation on Amazon EMR, or run it from Jupyter notebooks.\n",
    "\n",
    "After your FeatureStore has been created and populated with your data in the offline store, you have the capability to write SQL queries to join data stored in the offline store from different FeatureGroups. To do this, you can use Amazon Athena to write and execute SQL queries. You can set up a AWS Glue crawler to run on a schedule to ensure your catalog is always up to date as well.\n",
    "\n",
    "#### Limits\n",
    "\n",
    "- Maximum number of feature groups per AWS account: Soft limit of 100.\n",
    "- Maximum number of feature definitions per feature group: 2500.\n",
    "- Maximum Transactions per second (TPS) per API per AWS account: Soft limit of 10000 TPS per API excluding the BatchGetRecord API call, which has a soft limit of 500 TPS.\n",
    "- Maximum size of a record: 350KB.\n",
    "- Maximum size of a record identifier: 2KB.\n",
    "- Maximum size of a feature value: 350KB.\n",
    "- Maximum number of concurrent feature group creation workflows: 4.\n",
    "- BatchGetRecord API: Can contain as many as 100 records and can query up to 10 feature groups.\n",
    "\n",
    "AutoPilot\n",
    "\n",
    "- Autopilot is capable of handling datasets up to 5 GB.\n",
    "- Autopilot supports only tabular datasets in CSV format. Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order, is expected to have a header row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84983d26-8313-435a-9608-04a4cdfeb70a",
   "metadata": {},
   "source": [
    "### Run / Read\n",
    "\n",
    "- [SageMaker Immersion Day](https://catalog.us-east-1.prod.workshops.aws/workshops/63069e26-921c-4ce1-9cc7-dd882ff62575/en-US/lab2)\n",
    "- [aws/amazon-sagemaker-examples repo](https://github.com/aws/amazon-sagemaker-examples)\n",
    "- [Amazon SageMaker Example Notebooks, readthedocs](https://sagemaker-examples.readthedocs.io/en/latest/)\n",
    "- [Amazon SageMaker Python SDK, readthedocs](https://sagemaker.readthedocs.io/en/stable/)\n",
    "- [Data Processing with Spark, readthedocs](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#data-processing-with-spark)\n",
    "- [Batch Ingestion Spark Connector Setup](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-ingestion-spark-connector-setup.html)\n",
    "- [Data Processing with Apache Spark, AWS docs](https://docs.aws.amazon.com/sagemaker/latest/dg/use-spark-processing-container.html)\n",
    "- [AWS Modernization Week](https://onlinexperiences.com/scripts/Server.nxp)\n",
    "- [Prepare Data at Scale with Studio Notebooks, AWS docs / EMR and Spark](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-cluster.html)\n",
    "- [Perform interactive data engineering and data science workflows from Amazon SageMaker Studio notebooks, EMR and Spark](https://aws.amazon.com/blogs/machine-learning/perform-interactive-data-engineering-and-data-science-workflows-from-amazon-sagemaker-studio-notebooks/)\n",
    "- [sagemaker-feature-store-pyspark Python library](https://pypi.org/project/sagemaker-feature-store-pyspark/)\n",
    "- [aws/sagemaker-spark, github repo](https://github.com/aws/sagemaker-spark#getting-sagemaker-spark)\n",
    "- [Amazon SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa3ac3-c24b-45b9-9f78-31b4a5b14124",
   "metadata": {},
   "source": [
    "## Clarify\n",
    "\n",
    "- Detect data biases\n",
    "- Can be done:\n",
    "    - before training\n",
    "    - after training\n",
    "    - after deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe3a40-7423-49c2-92d2-cd8e463c516f",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Detect Pretraining Data Bias](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-detect-data-bias.html)\n",
    "- [Sample Notebook](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.html)\n",
    "- [Measure Pretraining Bias](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-data-bias.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1d922-c9dc-4731-9fdd-4917757b6620",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87f0f7-db52-4667-8889-495dcb6be84c",
   "metadata": {},
   "source": [
    "## Permissions\n",
    "\n",
    "The role for the SageMaker instance that is creating the pipeline must have the `iam:PassRole` permission for the pipeline execution role in order to pass it.\n",
    "\n",
    "Your pipeline execution role requires the following permissions:\n",
    "\n",
    "- To pass any role to a SageMaker job within a pipeline, the `iam:PassRole` permission for the role that is being passed. \n",
    "- `Create` and `Describe` permissions for each of the job types in the pipeline.\n",
    "- Amazon S3 permissions to use the `JsonGet` function. You control access to your Amazon S3 resources using resource-based policies and identity-based policies. A resource-based policy is applied to your Amazon S3 bucket and grants SageMaker Pipelines access to the bucket. An identity-based policy gives your pipeline the ability to make Amazon S3 calls from your account. For more information on resource-based policies and identity-based policies, see [Identity-based policies and resource-based policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c769f0a-e997-4d54-9d8e-16ca4ccfe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "{\n",
    "    \"Action\": [\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:HeadObject\"\n",
    "    ],\n",
    "    \"Resource\": \"arn:aws:s3:::<your-bucket-arn>/*\",\n",
    "    \"Effect\": \"Allow\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468bdd2-6209-4d09-9400-422ef56abdd3",
   "metadata": {},
   "source": [
    "### Resource\n",
    "\n",
    "- [Access Management - Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-access.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad86545-0b28-4254-8c7b-978cc263a728",
   "metadata": {},
   "source": [
    "## Step Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9158b2-3c12-415a-9ad8-bfa0e181e340",
   "metadata": {},
   "source": [
    "Amazon SageMaker Model Building Pipelines support the following step types:\n",
    "\n",
    "- Processing\n",
    "- Training\n",
    "- Tuning\n",
    "- CreateModel\n",
    "- RegisterModel\n",
    "- Transform\n",
    "- Condition\n",
    "- Callback\n",
    "- Lambda\n",
    "- ClarifyCheck\n",
    "- QualityCheck\n",
    "- EMR\n",
    "- Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbd6e7-8343-4aff-8ab2-0e4429ba2b4a",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Pipeline Steps - Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html)\n",
    "- [Pipelines - Read the Docs](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html)\n",
    "- [Define a Pipeline - Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html)\n",
    "- [Orchestrating Jobs with Amazon SageMaker Model Building Pipelines](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b4c87-1f87-4d41-aef8-d9123fba5595",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "779487c5-873e-4c98-8738-b032fc4fc026",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a99b83-3f3b-4bae-bc7a-07ef5b1494c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321df0d7-06c5-49cf-b780-8503c0255d1c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
