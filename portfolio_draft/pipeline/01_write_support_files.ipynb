{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Files for ML Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_case = 'sample'\n",
    "folder_date = datetime.today().strftime('%Y-%m-%d')\n",
    "target_rate = 0.5\n",
    "\n",
    "params = {\n",
    "    'use_case': use_case,\n",
    "    'folder_date': folder_date,\n",
    "    'project_dir': os.path.join(use_case, folder_date),\n",
    "    'resource_dir': 'support_files',\n",
    "    'data_dir': 'data',\n",
    "    'metrics_dir': 'metrics',\n",
    "    'model_dir': 'model_artifacts',\n",
    "    'sample_size': 10000,\n",
    "    'target_name': 'target',\n",
    "    'predictability': 0.8,\n",
    "    'target_rate': target_rate,\n",
    "    'target_wts': [1-float(target_rate), float(target_rate)],\n",
    "    'train_size': 0.8,\n",
    "    # 'unique_id' = 'unique_identifer',\n",
    "    'train_size': 0.8\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(params['project_dir'], params['resource_dir']), exist_ok=True)\n",
    "os.makedirs(os.path.join(params['project_dir'], params['data_dir']), exist_ok=True)\n",
    "os.makedirs(os.path.join(params['project_dir'], params['metrics_dir']), exist_ok=True)\n",
    "os.makedirs(os.path.join(params['project_dir'], params['model_dir']), exist_ok=True)\n",
    "\n",
    "with open(os.path.join(params['project_dir'], 'params.yaml'), \"w\") as config_file:\n",
    "    yaml.dump(params, config_file)\n",
    "with open(os.path.join('params.yaml'), \"w\") as config_file:\n",
    "    yaml.dump(params, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_dict = {\n",
    "            # 'LogisticRegression': {'module': 'sklearn.linear_model',\n",
    "            #                         'class': \"LogisticRegression\",\n",
    "            #                         'defaults': {'max_iter': 10000},\n",
    "            #                         'params': {\"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\",\n",
    "            #                                                \"sag\",\"saga\"],\n",
    "            #                                    \"penalty\": [\"l1\", \"l2\", None],\n",
    "            #                                    'max_iter': [10000]}},\n",
    "             # 'SGDClassifier': {'module': 'sklearn.linear_model',\n",
    "             #                   'class': \"SGDClassifier\",\n",
    "             #                   'defaults': {'loss': 'modified_huber'},\n",
    "             #                   'params': {\"loss\": [\"log_loss\", \"modified_huber\"],\n",
    "             #                              \"penalty\": [\"l1\", \"l2\", None],\n",
    "             #                              'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "             #                              'learning_rate':['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "             #                              'eta0':[10e-5]}},\n",
    "             'KNeighborsClassifier': {'module': 'sklearn.neighbors',\n",
    "                     'class': \"KNeighborsClassifier\",\n",
    "                     'params': {'n_neighbors': [5, 15, 25, 50],\n",
    "                                'weights': [\"uniform\", \"distance\"],\n",
    "                                # 'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "                                'metric': [\"cityblock\", \"cosine\", \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"nan_euclidean\"]}},\n",
    "             'DecisionTreeClassifier': {'module': 'sklearn.tree',\n",
    "                              'class': \"DecisionTreeClassifier\",\n",
    "                              'params': {\"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "                                         \"splitter\":[\"best\", \"random\"],\n",
    "                                         \"max_depth\": [None, 10, 100],\n",
    "                                         \"min_samples_split\": [2, 100, 1000],\n",
    "                                         \"min_samples_leaf\":[1, 10, 100],\n",
    "                                         \"max_features\": [None, \"sqrt\", \"log2\", 0.25, 0.5, 0.75],\n",
    "                                         \"max_leaf_nodes\":[None, 10, 100],\n",
    "                                         \"random_state\": [12]}},\n",
    "             'ExtraTreeClassifier': {'module': 'sklearn.tree',\n",
    "                           'class': \"ExtraTreeClassifier\",\n",
    "                           'defaults': {'splitter': 'best', \"min_samples_leaf\": 10},\n",
    "                           'params': {\"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "                                      \"splitter\":[\"best\", \"random\"],\n",
    "                                      \"max_depth\": [None, 10, 100],\n",
    "                                      \"min_samples_split\": [2, 100, 1000],\n",
    "                                      \"min_samples_leaf\":[1, 10, 100],\n",
    "                                      \"max_features\": [None, \"sqrt\", \"log2\", 0.25, 0.5, 0.75],\n",
    "                                      \"max_leaf_nodes\":[None, 10, 100],\n",
    "                                      \"random_state\": [12]}},\n",
    "             'AdaBoostClassifier': {'module': 'sklearn.ensemble',\n",
    "                          'class': \"AdaBoostClassifier\",\n",
    "                          'params': {\"n_estimators\":[5, 25, 50],\n",
    "                                     \"learning_rate\":[0.5, 1, 10],\n",
    "                                     \"algorithm\": ['SAMME', 'SAMME.R'],\n",
    "                                     \"random_state\": [12]}},\n",
    "             # 'BaggingClassifier': {'module': 'sklearn.ensemble',\n",
    "             #             'class': \"BaggingClassifier\",\n",
    "             #             'defaults': {\"n_estimators\": 50,\n",
    "             #                          \"max_samples\": 0.5,\n",
    "             #                          \"max_features\": 0.6,\n",
    "             #                          \"bootstrap_features\": True},\n",
    "             #             'params': {\"n_estimators\":[10, 25, 50, 100],\n",
    "             #                        \"max_samples\":[0.25, 0.5, 0.75, 1],\n",
    "             #                        \"max_features\": [0.25, 0.5, 0.75, 1],\n",
    "             #                        \"bootstrap\":[True, False],\n",
    "             #                        \"bootstrap_features\":[True, False],\n",
    "             #                        \"warm_start\":[True, False],\n",
    "             #                        \"random_state\": [12]}},\n",
    "            'ExtraTreesClassifier': {'module': 'sklearn.ensemble',\n",
    "                           'class': \"ExtraTreesClassifier\",\n",
    "                           'params': {\"n_estimators\":[50, 100, 150],\n",
    "                                      \"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "                                      \"max_depth\": [None, 10, 100],\n",
    "                                      \"min_samples_split\": [2, 100, 1000],\n",
    "                                      \"min_samples_leaf\":[1, 10, 100],\n",
    "                                      \"max_features\": [None, \"sqrt\", \"log2\", 0.25, 0.5, 0.75],\n",
    "                                      \"max_leaf_nodes\":[None, 10, 100],\n",
    "                                      \"bootstrap\":[True, False],\n",
    "                                      \"warm_start\":[True, False],\n",
    "                                      \"random_state\": [12]}},\n",
    "            # 'GradientBoostingClassifier': {'module': 'sklearn.ensemble',\n",
    "            #                   'class': \"GradientBoostingClassifier\",\n",
    "            #                   'params': {\"loss\":['log_loss', 'exponential'],\n",
    "            #                              \"learning_rate\":[0.5, 1, 10],\n",
    "            #                              \"n_estimators\":[50, 100, 150],\n",
    "            #                              \"subsample\": [0.5, 1],\n",
    "            #                              \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
    "            #                              \"min_samples_split\": [2, 100, 1000],\n",
    "            #                              \"min_samples_leaf\":[1, 10, 100],\n",
    "            #                              \"max_depth\": [3, 15, 25],\n",
    "            #                              \"max_features\": [None, \"sqrt\", \"log2\", 0.25, 0.5, 0.75],\n",
    "            #                              \"warm_start\":[True, False],\n",
    "            #                              \"validation_fraction\": [0.4, 0.2],\n",
    "            #                              # \"n_iter_no_change\": [5, 10],\n",
    "            #                              \"random_state\": [12]}},\n",
    "            'RandomForestClassifier': {'module': 'sklearn.ensemble',\n",
    "                             'class': \"RandomForestClassifier\",\n",
    "                             'params': {\"n_estimators\":[50, 100, 150],\n",
    "                                        \"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "                                        \"max_depth\": [None, 10, 100],\n",
    "                                        \"min_samples_split\": [2, 100, 1000],\n",
    "                                        \"min_samples_leaf\":[1, 10, 100],\n",
    "                                        \"max_features\": [None, \"sqrt\", \"log2\", 0.25, 0.5, 0.75],\n",
    "                                        \"max_leaf_nodes\":[None, 10, 100],\n",
    "                                        \"bootstrap\":[True, False],\n",
    "                                        \"warm_start\":[True, False],\n",
    "                                        \"random_state\": [12]}},\n",
    "            'HistGradientBoostingClassifier': {'module': 'sklearn.ensemble',\n",
    "                                  'class': \"HistGradientBoostingClassifier\",\n",
    "                                  'params': {\"learning_rate\":[0.25, 0.5, 0.75, 1],\n",
    "                                             \"max_iter\": [50, 100, 150],\n",
    "                                             \"max_leaf_nodes\":[10, 30, 100],\n",
    "                                             \"max_depth\": [None, 10, 25, 50],\n",
    "                                             \"min_samples_leaf\":[1, 10, 100],\n",
    "                                             \"warm_start\":[True, False],\n",
    "                                             \"validation_fraction\": [0.4, 0.2],\n",
    "                                             \"random_state\": [12]}},\n",
    "            'XGBClassifier': {'module': 'xgboost',\n",
    "                        'class': \"XGBClassifier\",\n",
    "                        'params': {\"learning_rate\":[0.25, 0.5, 0.75, 1],\n",
    "                                   \"gamma\": [0, 0.5, 1, 100],\n",
    "                                   \"max_depth\": [1, 10, 100],\n",
    "                                   \"min_child_weight\":[1, 10, 100],\n",
    "                                   \"max_delta_step\":[0, 2, 10],\n",
    "                                   \"subsample\": [0.25, 0.5, 0.75, 1],\n",
    "                                   \"lambda\": [0.25, 0.5, 0.75,],\n",
    "                                   \"alpha\": [0.25, 0.5, 0.75,],\n",
    "                                   \"tree_method\": [\"auto\", \"exact\", \"approx\"],\n",
    "                                   \"random_state\": [12]}}\n",
    "            }\n",
    "\n",
    "with open(os.path.join(params['project_dir'], params['resource_dir'], \"algo_dict.json\"), \"w\") as ad:\n",
    "    ad.write(json.dumps(algo_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict = {\n",
    "        \"true_false\": {\n",
    "            'transformer': \"TrueFalseTransformer\",\n",
    "            'columns': [\n",
    "                'true_false'\n",
    "                ]},\n",
    "        \"one_hot\": {\n",
    "            'transformer': \"OneHotTransformer\",\n",
    "            'columns':[\n",
    "                'one_hot'\n",
    "                ]},\n",
    "        \"date_cols\": {\n",
    "            'transformer': \"DateTransformer\",\n",
    "            'columns':[\n",
    "                'dates'\n",
    "                ]},\n",
    "        \"float_cols\": {\n",
    "            'transformer': \"FloatTransformer\",\n",
    "            'columns':[\n",
    "                'floats'\n",
    "                ]},\n",
    "        \"max_of_list\": {\n",
    "            'transformer': \"ListMaxTransformer\",\n",
    "            'columns':[\n",
    "                'max_of_list'\n",
    "                ]},\n",
    "        \"count_unique\": {\n",
    "            'transformer': \"ListNuniqueTransformer\",\n",
    "            'columns':[\n",
    "                'nunique_of_list'\n",
    "                ]},\n",
    "        \"desc_stat_cols\":{\n",
    "            'transformer': \"DescStatTransformer\",\n",
    "            'columns':[\n",
    "                'desc_stats'\n",
    "                ]},\n",
    "        \"list_to_labels\": {\n",
    "            'transformer': \"MultilabelTransformer\",\n",
    "            'columns':[\n",
    "                'multi_label'\n",
    "                ]},\n",
    "        \"drop_cols\": {\n",
    "            'transformer': 'drop',\n",
    "            'columns':[\n",
    "                'random_col',\n",
    "                'other']}}\n",
    "\n",
    "with open(os.path.join(params['project_dir'], params['resource_dir'], \"col_dict.json\"), \"w\") as cd:\n",
    "    cd.write(json.dumps(col_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transformers_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transformers_script.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "\n",
    "class FeatureFiller(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "        self._col_dict = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        \n",
    "        init_dict = X.dtypes.apply(lambda x: x.name).to_dict()\n",
    "        self._col_dict = {}\n",
    "        for col, dtype in init_dict.items():\n",
    "            self._col_dict.setdefault(dtype, []).append(col)      \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running FeatureFiller')\n",
    "        new_X = pd.DataFrame(columns=self._col_names)\n",
    "\n",
    "        for col_type in self._col_dict:\n",
    "            columns = self._col_dict[col_type]\n",
    "            new_X[columns] = new_X[columns].astype(col_type)\n",
    "        \n",
    "        new_df = pd.concat([new_X, X])\n",
    "        # print(\"Finished FeatureFiller\")\n",
    "        return new_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "\n",
    "class TrueFalseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running TrueFalseTransformer')\n",
    "        X = X.replace({'None':np.nan}).fillna('-1')\n",
    "        X = X.replace({'true':'1', 'false':'0'})\n",
    "        X = X.apply(pd.to_numeric, args=('coerce',))\n",
    "        # print(\"Finished TrueFalseTransformer\")\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running DateTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_datetime(X[col])\n",
    "            temp_df[f'{col}-month'] = X[col].dt.month.astype(float)\n",
    "            temp_df[f'{col}-day_of_week'] = X[col].dt.dayofweek.astype(float)\n",
    "            temp_df[f'{col}-hour'] = X[col].dt.hour.astype(float)\n",
    "            temp_df[f'{col}-day_of_month'] = X[col].dt.day.astype(float)\n",
    "            temp_df[f'{col}-is_month_start'] = X[col].dt.is_month_start.astype(int)\n",
    "            temp_df[f'{col}-is_month_end'] = X[col].dt.is_month_end.astype(int)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        temp_df = temp_df.fillna(-1)\n",
    "        # print(\"Finished DateTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "\n",
    "class FloatTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running FloatTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype != 'float':\n",
    "                X[col] = X[col].astype(float)\n",
    "        X = X.fillna(-1.0)\n",
    "        # print(\"Finished FloatTransformer\")\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "\n",
    "class ListMaxTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running ListMaxTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.replace({'true':'1', 'false':'0'}).fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).max()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        # print(\"Finished ListMaxTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "\n",
    "class ListNuniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running ListNuniqueTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col] = X[col].dropna().str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.groupby(temp_series.index).nunique()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        # print(\"Finished ListNuniqueTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "\n",
    "class DescStatTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running DescStatTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).agg(['min', 'max', 'mean', 'std', 'nunique'])\n",
    "            temp_series.columns = [f'{col}-{x}' for x in temp_series.columns]\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        # print(\"Finished DescStatTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "\n",
    "class OneHotTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._filler = 'ml_empty'\n",
    "        self._col_names = None\n",
    "        self._encoder = None\n",
    "        self._transformer = None\n",
    "        self._transformed_feats = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = X.dropna(axis=1, how='all').columns\n",
    "        X = X[self._col_names].fillna(self._filler)\n",
    "        self._encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self._transformer = self._encoder.fit(X)\n",
    "        self._transformed_feats = self._transformer.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running OneHotTransformer')\n",
    "        X = X.replace({'None':np.nan}).fillna(self._filler)\n",
    "        X = self._transformer.transform(X[self._col_names])\n",
    "        # print(\"Finished OneHotTransformer\")\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return list(self._transformed_feats)\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return list(self._transformed_feats)\n",
    "\n",
    "class MultilabelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._filler = 'ml_empty'\n",
    "        self._encoder = None\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        self._encoder = MultiLabelBinarizer()\n",
    "        self._encoder.fit(X)\n",
    "        self._col_names = [X.name + '_' + x for x in self._encoder.classes_]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running MultilabelTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        trans_array = self._encoder.transform(X)\n",
    "        df = pd.DataFrame(trans_array, columns=self._col_names, index=X.index)   \n",
    "        # print(\"Finished MultilabelTransformer\")     \n",
    "        return df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "    \n",
    "class DropSingleValueCols(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_index = []\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        for i in range(len(X.columns)):\n",
    "            if X.iloc[:,i].nunique() > 1:\n",
    "                self._col_index.append(i)\n",
    "        self._col_names = list(X.iloc[:,self._col_index].columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running DropSingleValueCols')\n",
    "        if type(X) == np.ndarray:\n",
    "            print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        X = X[self._col_names]\n",
    "        # X = X.iloc[:,self._col_index]\n",
    "        # print(\"Finished DropSingleValueCols\")\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "       \n",
    "class RemoveCollinearity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._corr_dict = {}\n",
    "        self._drop_cols = set()\n",
    "        self._col_index = []\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        drop_list = []\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        corr_df = X.corr()\n",
    "        for i, col in enumerate(corr_df.columns):\n",
    "            sliced_col = abs(corr_df.iloc[i+1:, i])\n",
    "            corr_feats = sliced_col[sliced_col > .97].index.tolist()\n",
    "            if len(corr_feats) > 0:\n",
    "                self._corr_dict[col] = corr_feats\n",
    "                drop_list += corr_feats\n",
    "        self._drop_cols = set(drop_list)\n",
    "        # print('Collinear feature drop list:', drop_list)\n",
    "        print('Number of collinear feature:', len(drop_list))\n",
    "        self._col_names = list(set(X.columns) - self._drop_cols)\n",
    "        for i, col in enumerate(X.columns):\n",
    "            if col in self._col_names:\n",
    "                self._col_index.append(i)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running RemoveCollinearity')\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        X =  X[self._col_names]\n",
    "        # X =  X.iloc[:,self._col_index]\n",
    "        # print(\"Finished RemoveCollinearity\")\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n",
    "    \n",
    "class SetColumnOrder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        self._col_names = list(set(X.columns))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # print('Running SetColumnOrder')\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        X =  X[self._col_names]\n",
    "        # X =  X.iloc[:,self._col_index]\n",
    "        # print(\"Finished SetColumnOrder\")\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self._col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "alibi==0.9.6\n",
    "matplotlib==3.9.2\n",
    "numpy==1.26.4\n",
    "pandas==2.2.3\n",
    "pyarrow==14.0.1\n",
    "scikit-explain==0.1.3\n",
    "scikit-learn==1.4.2\n",
    "shap==0.44.1\n",
    "xgboost==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate_data_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_data_module.py\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "def calc_wts(col, predictability):\n",
    "    low_wt = 1 - predictability\n",
    "    col_len = len(col)\n",
    "    if predictability > 0:\n",
    "        wts = [predictability] + [(low_wt/(col_len-1)) for x in range(col_len-1)]\n",
    "    elif predictability == 0:\n",
    "        wts = [(low_wt/(col_len)) for x in range(col_len)]\n",
    "    else:\n",
    "        print('''High weight must be between 0 or greater and less than 1.\n",
    "              A value of 0 gives equal weight to all values\n",
    "              A value greater than 0 and less than one gives that weight to the first value and equal weights to all other values''')\n",
    "    xwts = list(reversed(wts))\n",
    "    return wts, xwts\n",
    "\n",
    "def generate_dates(df, target_name, year=2022):\n",
    "    true_vals = []\n",
    "    false_vals = []\n",
    "    month_wts, month_xwts = calc_wts(range(1, 12), 0)\n",
    "    day_wts, day_xwts = calc_wts(range(1, 31), 0.8)    \n",
    "    \n",
    "    t_year = [year for x in range(len(df[df[target_name]==1]))]\n",
    "    t_month = random.choices(range(1, 12), month_wts, k=len(df[df[target_name]==1]))\n",
    "    t_day = random.choices(range(1, 31), day_wts, k=len(df[df[target_name]==1]))\n",
    "    true_collist = zip(t_year,\n",
    "                       t_month,\n",
    "                       t_day)\n",
    "    for yr, mt, dy in true_collist:\n",
    "        try:\n",
    "            date = datetime.date(yr, mt, dy)\n",
    "            true_vals.append(date)\n",
    "        except ValueError:\n",
    "            true_vals.append(np.nan)\n",
    "            \n",
    "    f_year = [year for x in range(len(df[df[target_name]==0]))]\n",
    "    f_month = random.choices(range(1, 12), month_xwts, k=len(df[df[target_name]==0]))\n",
    "    f_day = random.choices(range(1, 31), day_xwts, k=len(df[df[target_name]==0]))\n",
    "    false_collist = zip(f_year,\n",
    "                        f_month,\n",
    "                        f_day)\n",
    "    for yr, mt, dy in false_collist:\n",
    "        try:\n",
    "            date = datetime.date(yr, mt, dy)\n",
    "            false_vals.append(date)\n",
    "        except ValueError:\n",
    "            false_vals.append(np.nan)\n",
    "    \n",
    "    return true_vals, false_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions_module.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def pandas_reads(file_path, col_list = None):\n",
    "    \"\"\"\n",
    "    Reads all useful data file formats\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    file_path : str\n",
    "        The directory path and file name of the file to read.\n",
    "\n",
    "    col_list : list\n",
    "        Optional list of column names to load. This functionality\n",
    "        is only compatible for parquet, csv, and txt file formats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc : dataframe\n",
    "        A pandas dataframe of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'parquet':\n",
    "        doc = pd.read_parquet(file_path, columns=col_list)\n",
    "    elif file_type == 'csv':\n",
    "        doc = pd.read_csv(file_path, usecols=col_list)\n",
    "    elif file_type == 'json':\n",
    "        with open(file_path, 'r') as f:\n",
    "            doc = json.loads(f.read())\n",
    "    elif file_type == 'txt':\n",
    "        doc = pd.read_csv(file_path, sep='\\t', usecols=col_list)\n",
    "    elif file_type == 'pickle':\n",
    "        doc = pd.read_pickle(file_path)\n",
    "    else:\n",
    "        print(\"\"\"Error: Accepted file types include parquet, csv, json, txt, and pickle.\n",
    "        \n",
    "        Use one of these file types or add to the pandas_reads function in functions_module.py\"\"\")\n",
    "    return doc\n",
    "\n",
    "def pandas_writes(df, file_path):\n",
    "    \"\"\"\n",
    "    Writes a dataframe to the specified data file formats.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df : dataframe\n",
    "        The dataframe to be written.\n",
    "\n",
    "    file_path : str\n",
    "        The directory path and file name of the file to write.\n",
    "    \"\"\"\n",
    "\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'parquet':\n",
    "        df.to_parquet(file_path)\n",
    "    elif file_type == 'csv':\n",
    "        df.to_csv(file_path)\n",
    "    elif file_type == 'json':\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(json.dumps(df.to_json(date_format='iso')))\n",
    "    elif file_type == 'txt':\n",
    "        df.to_csv(file_path, sep='\\t')\n",
    "    elif file_type == 'pickle':\n",
    "        df.to_pickle(file_path)\n",
    "    else:\n",
    "        print(\"\"\"Error: Accepted file type is dataframe only.\n",
    "        If you have a dictionary, turn it into a dataframe first.\n",
    "        Accepted write types include parquet, csv, json, txt, and pickle.\n",
    "        Use one of these file types or add to the pandas_writes\n",
    "         function in functions_module.py\"\"\")\n",
    "\n",
    "def get_file_list(input_path):\n",
    "    \"\"\"\n",
    "    Make a list of file paths for all files in a directory.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    input_path : str\n",
    "        The file path to the directory where the files are stored.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    file_list : list\n",
    "        A list of the file paths where each file path is a strings.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    This is a help function that allows for a dynamic number of files\n",
    "    to be found in a directory. This allows for changes in\n",
    "    underlying data and the tables or collections where they're stored\n",
    "    without having to change this script.\n",
    "    \"\"\"\n",
    "\n",
    "    file_types = ['parquet', 'csv', 'json', 'txt', 'pickle']\n",
    "    try:\n",
    "        file_list = [os.path.join(input_path, file) for file in os.listdir(input_path) if file.split('.')[-1] in file_types]\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"\"\"Error: Accepted file types include parquet, csv, json, txt, and pickle.\n",
    "        \n",
    "        Use one or more of these file types in the specified directory or add to the get_file_list function in functions_module.py\"\"\")\n",
    "    return file_list\n",
    "\n",
    "def read_in_files(input_path):\n",
    "    \"\"\"\n",
    "    Reads the all files in the directory specified.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    input_path : str\n",
    "        The file path to the directory where the files are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_dict : dictionary\n",
    "        A dictionary of dataframes where each file is a key and\n",
    "        the values in that file are a dataframe of those values.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The dictionary structure is used to accomodate the dynamic\n",
    "    number of files.\n",
    "    \"\"\"\n",
    "\n",
    "    df_dict = {}\n",
    "    file_list = get_file_list(input_path)\n",
    "    for file in file_list:\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        print(f'read_in_files: reading file {file}')\n",
    "        df_dict[file_name] = pandas_reads(file)\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_functions_module.py\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functions_module import get_file_list, read_in_files, pandas_reads\n",
    "\n",
    "\n",
    "def filter_features(input_path, index_col, filter_col, event_param):\n",
    "    \"\"\"\n",
    "    Reads parquet files, combines data into a single dataframe,\n",
    "    and filters the data to a single claim event.\n",
    "\n",
    "    Parameters\n",
    "    ----------            \n",
    "    input_path : str, default: ``/opt/ml/processing/input``\n",
    "        Subdirectory where the parquet files holding the features\n",
    "        were placed by the corresponding Sagemaker Pipeline step.\n",
    "        If changed, the location must be changed in both the Python\n",
    "        script and the Sagemaker Pipeline step.\n",
    "\n",
    "    index_col : str\n",
    "        The column that holds the unique identifier. Passed as an argument\n",
    "        from the corresponding Sagemaker Pipeline step.\n",
    "\n",
    "    filter-col : str\n",
    "        The column that holds the event type. Passed as an argument\n",
    "        from the corresponding Sagemaker Pipeline step.\n",
    "\n",
    "    event_param : {fulfill, ship, pay}\n",
    "        The name of the event to be used. Passed as an argument from\n",
    "        the corresponding Sagemaker Pipeline step.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    features_df : pd.DataFrame\n",
    "        The combined and filtered feature data.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('combine_filter_save: reading files into a dictionary of dataframes')\n",
    "    df_dict = read_in_files(input_path)\n",
    "    print('combine_filter_save: create temp file using index column: {}'.format(index_col))\n",
    "    df = pd.DataFrame(columns=[index_col])\n",
    "    print('combine_filter_save: merging dictionary of dataframes into a single dataframe')\n",
    "    for key in df_dict:\n",
    "        print(key)\n",
    "        df = df.merge(df_dict[key], on=index_col, how='outer')\n",
    "\n",
    "    print('combine_filter_save: filter by event_name')\n",
    "    features_df = filter_by_event(df, filter_col, event_param)\n",
    "    features_df = features_df.reset_index(drop=True)\n",
    "    print('combine_filter_save: saving dataframes filtered by event_name')\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def filter_by_event(df, filter_col, event_param):\n",
    "    \"\"\"\n",
    "    Filters data to claims that occured at a specific event.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        A single dataframe that holds all claims.\n",
    "    filter_col : str\n",
    "        The column that holds the event type.\n",
    "    event_param : str\n",
    "        The name of the event to be used.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features_df : pandas dataframe\n",
    "        The filtered claims that will be used for the rest of the\n",
    "        training pipeline.    \n",
    "    \"\"\"\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    features_df = df[df[filter_col].str.lower().str.contains(event_param)]\n",
    "    features_df = features_df.drop_duplicates()\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def filter_gt(gt_filepath, sr_filepath, gt_index_col, fraud_target, merge_col): \n",
    "    \"\"\"\n",
    "    Reads ground truth and service request files, combines data into a \n",
    "    single dataframe, and filters the ground truth to the specified claims.\n",
    "\n",
    "    Parameters\n",
    "    ----------            \n",
    "    input_path : str, default: ``/opt/ml/processing/input``\n",
    "        Subdirectory where the parquet files holding the features\n",
    "        were placed by the corresponding Sagemaker Pipeline step.\n",
    "        If changed, the location must be changed in both the Python\n",
    "        script and the Sagemaker Pipeline step.\n",
    "\n",
    "    gt_index_col : str\n",
    "        The column that holds the unique identifier for ground truth.\n",
    "        Passed as an argument from the corresponding Sagemaker Pipeline step.\n",
    "\n",
    "    fraud_target : {Reviewed, Bulk, POSTVERIFIEDFRAUD, ORG_FRAUD_DENIAL, ORG_FRAUD, DISCLAIMS, BROADFRAUD_DSCL, VOIDS, BROADFRAUD_DSCL_VOID, DIV_GROUND_TRUTH, REG_ADJ_DISCLAIM_VOID, ASG_DISCLAIM_VOID, ASG_DISCLAIM_VOID_REASON}\n",
    "        The name of the fraud target to be used. `--target` name is case insensitive.\n",
    "        Passed as an argument from the corresponding Sagemaker Pipeline step.\n",
    "\n",
    "    merge-col : str\n",
    "        The column name to merge the features and ground truth.\n",
    "        Passed as an argument from the corresponding Sagemaker Pipeline step.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    temp_df : pd.DataFrame\n",
    "        The filtered ground truth data.\n",
    "    \"\"\"\n",
    "       \n",
    "    gt_cols = [gt_index_col.upper(), fraud_target.upper()]\n",
    "    \n",
    "    gt_df = pandas_reads(gt_filepath, col_list=gt_cols)\n",
    "    gt_df.columns = [merge_col, fraud_target]\n",
    "        \n",
    "    sr_df = pandas_reads(sr_filepath, col_list=[merge_col])\n",
    "    print('Merging gt_df with feats_df')\n",
    "    temp_df = gt_df.merge(sr_df, on=merge_col, how='inner')\n",
    "    print('Dropping any duplicate rows')\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    print(temp_df.sample(5))\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "def combine_feats_gt(feats_filepath, gt_filepath, merge_col, target_col):\n",
    "    \"\"\"\n",
    "    Combines the prepared feature and prepared ground truth\n",
    "    datasets into a single dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : '/opt/ml/processing/input'\n",
    "        Subdirectory where the parquet files holding the features\n",
    "        were placed by the corresponding Sagemaker Pipeline step.\n",
    "        If changed, the location must be changed in both the Python\n",
    "        script and the Sagemaker Pipeline step.\n",
    "    \n",
    "    merge_col : str\n",
    "        The column name to merge the features and ground truth.\n",
    "    \n",
    "    target_col : {Reviewed, Bulk, POSTVERIFIEDFRAUD, ORG_FRAUD_DENIAL, ORG_FRAUD, DISCLAIMS, BROADFRAUD_DSCL, VOIDS, BROADFRAUD_DSCL_VOID, DIV_GROUND_TRUTH, REG_ADJ_DISCLAIM_VOID, ASG_DISCLAIM_VOID, ASG_DISCLAIM_VOID_REASON}\n",
    "        The name of the fraud target to be used. `--target` name is case insensitive.\n",
    "        Passed as an argument from the corresponding Sagemaker Pipeline step.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    full_data : pd.DataFrame\n",
    "        The combined feature and ground truth data as a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Loading data')\n",
    "    feats_df = pandas_reads(feats_filepath)\n",
    "    print(f'Feature data size: {feats_df.shape}')\n",
    "    print('Loading ground truth')\n",
    "    gt_df = pandas_reads(gt_filepath)\n",
    "    print(f'Ground truth data size {gt_df.shape}')\n",
    "    print('Combining features and ground truth')    \n",
    "    full_data = feats_df.merge(gt_df, on=merge_col, how='outer')\n",
    "    print(f'There are {full_data[full_data[target_col].isna()].shape[0]} rows with no truth data. Dropping those rows')\n",
    "    full_data = full_data[full_data[target_col].notna()].drop_duplicates()\n",
    "    print(f'Merged dataframe size: {full_data.shape}')\n",
    "    print(f'Set target as {target_col}')    \n",
    "    return full_data\n",
    "\n",
    "\n",
    "def split_datasets(train_size, full_data, target_col):\n",
    "    \"\"\"\n",
    "    Splits the training data into train, validate, and test datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : str\n",
    "        File path to the input data. This is automatically populated\n",
    "        from the Sagemaker Pipeline step.\n",
    "    \n",
    "    merge_col : str\n",
    "        The column to merge on.\n",
    "    \n",
    "    \n",
    "    target_col : str\n",
    "        The fraud target.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_data : pd.Dataframe\n",
    "        The training split of the data.\n",
    "        \n",
    "    validate_data : pd.Dataframe\n",
    "        The validation split of the data.\n",
    "        \n",
    "    test_data : pd.Dataframe\n",
    "        The test split of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_size = float(train_size)\n",
    "    if train_size > 1:\n",
    "        train_size = train_size/100\n",
    "  \n",
    "    print('Splitting data into train, validate, test datasets')\n",
    "    train_dataF, test_data = train_test_split(full_data,\n",
    "                                              train_size=train_size,\n",
    "                                              random_state=12,\n",
    "                                              stratify=full_data[target_col])\n",
    "    train_data, validate_data = train_test_split(train_dataF,\n",
    "                                                 train_size=train_size,\n",
    "                                                 random_state=12,\n",
    "                                                 stratify=train_dataF[target_col])\n",
    "    \n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    validate_data = validate_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    \n",
    "    return train_data, validate_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_functions_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_functions_module.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "from transformers_script import FeatureFiller\n",
    "from transformers_script import DropSingleValueCols\n",
    "from transformers_script import RemoveCollinearity\n",
    "from transformers_script import SetColumnOrder\n",
    "from sklearn.metrics import(\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score)\n",
    "\n",
    "import skexplain\n",
    "import shap\n",
    "import alibi\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "\n",
    "def define_algo_pipe(algo, col_dict):\n",
    "    \"\"\"\n",
    "    Defines a sklearn pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    algo : algorithm\n",
    "        The algorithm to use for model training. Example: ``XGBClassifier``\n",
    "    col_dict : dict\n",
    "        The dictionary of mappings for feature to preprocessing method. This\n",
    "        tells the sklearn pipeline which custom Asurion transformer\n",
    "        (i.e. ``TrueFalseTransformer``) to apply to which column(s).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    algo_pipe : sklearn pipeline\n",
    "        The sklearn pipeline for the preprocessing and model training.\n",
    "        \n",
    "    Notes\n",
    "    -----    \n",
    "    This function is also in the ``tune_model.py`` file. As these functions\n",
    "    are maintained in different scripts, differences may occur.\n",
    "    \"\"\"\n",
    "    \n",
    "    transformer_list = []\n",
    "    tModule = importlib.import_module(\"transformers_script\")\n",
    "    key_list = [x for x in col_dict.keys() if not x == 'drop_cols']\n",
    "    for key in key_list:\n",
    "        trans_function = getattr(tModule, col_dict[key]['transformer'])\n",
    "        if key == 'list_to_labels':\n",
    "            for col in col_dict[key]['columns']:\n",
    "                step_name = key + '_' + col\n",
    "                transformer_list.append((step_name, trans_function(), col))\n",
    "        else:\n",
    "            transformer_list.append((key, trans_function(), col_dict[key]['columns']))\n",
    "    transformer_list.append((\"drop_cols\", 'drop', col_dict[\"drop_cols\"][\"columns\"]))\n",
    "    preprocessor = ColumnTransformer(transformer_list)\n",
    "    \n",
    "    all_preprocess = Pipeline([\n",
    "        ('featurefiller', FeatureFiller()),\n",
    "        ('preprocess', preprocessor),\n",
    "        ('dropsingle', DropSingleValueCols()),\n",
    "        ('removemulticollinear', RemoveCollinearity()),\n",
    "        ('setcolumorder', SetColumnOrder())])\n",
    "\n",
    "    algo_pipe = Pipeline([\n",
    "        ('all_preprocess', all_preprocess),\n",
    "        ('algorithm', algo)])\n",
    "    return algo_pipe\n",
    "\n",
    "def train_model(algo, algo_data, col_dict, Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Trains a model and returns the evaluation metrics.\n",
    "    \n",
    "    Parameters\n",
    "    -----------    \n",
    "    algo : algorithm\n",
    "        The algorithm to use for model training. Example: `XGBClassifier`\n",
    "    algo_data : dict\n",
    "        The dictionary holding algorithm information including the\n",
    "        package information, any changes to default settings, and\n",
    "        the parameters for hyperparameter tuning.\n",
    "    col_dict : dict\n",
    "        The dictionary of mappings for feature to preprocessing method. This\n",
    "        tells the sklearn pipeline which custom Asurion transformer\n",
    "        (i.e. ``TrueFalseTransformer``) to apply to which column(s).\n",
    "    Xtrain : pd.DataFrame\n",
    "        The training feature dataset.\n",
    "    ytrain : pd.DataFrame\n",
    "        The training ground truth data.\n",
    "    \n",
    "    Returns\n",
    "    --------    \n",
    "    metric_result_dict : dict\n",
    "        A dictionary of the evaluation metrics for the trained model.\n",
    "        \n",
    "    Notes\n",
    "    -----    \n",
    "    The trained model is not returned because this model isn't used going\n",
    "    forward. Training is done at this stage to collect information on\n",
    "    how well different algorithms perform on the training data. The best\n",
    "    performing algorithm is then trained and tuned in a different step\n",
    "    for further use.\n",
    "    \"\"\"\n",
    "    \n",
    "    metric_result_dict = {}\n",
    "    results = None\n",
    "    print(algo_data['class'])\n",
    "    try:\n",
    "        if 'defaults' in algo_data:\n",
    "            model = algo(**algo_data['defaults'])\n",
    "        else:\n",
    "            model = algo()\n",
    "        model_pipe = define_algo_pipe(model, col_dict)\n",
    "        results = cross_validate(model_pipe, Xtrain, ytrain, cv=5, scoring=['accuracy', 'precision', 'f1', 'recall', 'roc_auc'])\n",
    "        for result in results.keys():\n",
    "            results[result] = results[result].tolist()\n",
    "        metric_result_dict[algo_data['class']] = results\n",
    "    except Exception as error:\n",
    "        print(\"Error on\", algo_data['class'])\n",
    "        print(\"Error:\", error)\n",
    "    print('\\n')\n",
    "    print(algo_data['class'], \"Result Dictionary:\")\n",
    "    print(results)\n",
    "    return metric_result_dict\n",
    "\n",
    "def compile_algo_metrics(metric_dict):\n",
    "    \"\"\"\n",
    "    Compiles cross validation metrics from the baseline models\n",
    "    and calculates several aggregated metrics.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    metric_dict : dict\n",
    "        The dictionary of evaluation metrics. Each algorithm\n",
    "        (i.e. ``XGBoostClassifier``constitutes one row with the\n",
    "        metrics (i.e. ``f1``) making up the columns.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    metric_desc_stat_df : dataframe\n",
    "        A dataframe with the metrics and calculated aggreated/averaged\n",
    "        results for all trained algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    metric_result_df = pd.DataFrame(metric_dict).T\n",
    "    metric_desc_stat_df = pd.DataFrame()\n",
    "    for col in metric_result_df.columns:\n",
    "        if 'time' in col:\n",
    "            continue\n",
    "        else:\n",
    "            mean_col = col + '_mean'\n",
    "            stdv_col = col + '_stdev'\n",
    "            penalized = col + '_penalized'\n",
    "            metric_desc_stat_df[mean_col] = metric_result_df[col].apply(np.mean)\n",
    "            metric_desc_stat_df[stdv_col] = metric_result_df[col].apply(np.std)\n",
    "            metric_desc_stat_df[penalized] = metric_desc_stat_df[mean_col] * (1 - metric_desc_stat_df[stdv_col])\n",
    "    metric_desc_stat_df = metric_desc_stat_df.sort_values([\n",
    "        'test_f1_penalized',\n",
    "        'test_roc_auc_penalized',\n",
    "        'test_recall_penalized',\n",
    "        'test_precision_penalized',\n",
    "        'test_accuracy_penalized'], ascending=False)\n",
    "    return metric_desc_stat_df\n",
    "\n",
    "def tune_best_algo(algo, algo_data, col_dict, Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Trains and tunes a model.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    algo : algorithm\n",
    "        The algorithm to use for model training. Example: ``XGBClassifier``    \n",
    "    algo_data : dict\n",
    "        The dictionary holding algorithm information including the\n",
    "        package information, any changes to default settings, and\n",
    "        the parameters for hyperparameter tuning.\n",
    "    col_dict : dict\n",
    "        The dictionary of mappings for feature to preprocessing method. This\n",
    "        tells the sklearn pipeline which custom Asurion transformer\n",
    "        (i.e. ``TrueFalseTransformer``) to apply to which column(s).\n",
    "    Xtrain : dataframe\n",
    "        The training feature dataset.\n",
    "    ytrain : dataframe\n",
    "        The training ground truth data.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    model : trained model\n",
    "        The trained model that has been tuned using ``RandomizedSearchCV``.\n",
    "        \n",
    "    See Also\n",
    "    --------\n",
    "    sklearn.mode_selection.RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    \n",
    "    algo_pipe = define_algo_pipe(algo(), col_dict)\n",
    "    param_grid = dict(('algorithm__'+key, value) for (key, value) in algo_data['params'].items())\n",
    "    search_params = {\"estimator\": algo_pipe,\n",
    "                     \"cv\": 5,\n",
    "                     \"param_distributions\": param_grid,\n",
    "                     \"scoring\": {'f1':'f1',\n",
    "                                 'auc':'roc_auc'},\n",
    "                     \"verbose\": 5,\n",
    "                     \"refit\": \"f1\",\n",
    "                     \"random_state\": 12}\n",
    "    print(\"Tuning for hyperparameters\")\n",
    "    tuner = RandomizedSearchCV(**search_params)\n",
    "    print(\"Training model\")\n",
    "    tuner.fit(Xtrain, ytrain)\n",
    "    model = tuner.best_estimator_\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def metrics_row(y_test, model, testX):\n",
    "    \"\"\"\n",
    "    Evalute trained model using the metrics accuracy, precision, AUC,\n",
    "    f1, recall, and the confusion matrix.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    y_test : pd.DataFrame\n",
    "        The validation ground truth values.\n",
    "    model : trained model\n",
    "        The model trained on the training dataset.\n",
    "    testX : pd.DataFrame\n",
    "        The validation feature dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result_dict : dict\n",
    "        A dictionary of the evaluation metrics rounded to 3 decimal places\n",
    "        including accuracy, precision, f1, recall, ROC AUC, and an\n",
    "        annotated confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = model.predict(testX)\n",
    "    proba_predictions = model.predict_proba(testX)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    \n",
    "    conf_matrix_norm = np.round(conf_matrix / conf_matrix.sum(axis=1), 3)\n",
    "    \n",
    "    annotation_list = []\n",
    "    lbls = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "    ct = 0\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tmp = []\n",
    "        for x in range(conf_matrix[i].shape[0]):\n",
    "            val = f\"{lbls[ct]}\\n\\nCount: {conf_matrix[i][x]}\\nActl Rate: {conf_matrix_norm[i][x]}\"\n",
    "            tmp.append(val)\n",
    "            ct += 1\n",
    "        annotation_list.append(tmp)\n",
    "        \n",
    "    result_dict = {\"Accuracy\": round(accuracy, 3),\n",
    "                   \"Precision\": round(precision, 3),\n",
    "                   \"F1\": round(f1, 3),\n",
    "                   \"Recall\": round(recall, 3),\n",
    "                   \"ROC AUC\": round(roc_auc, 3),\n",
    "                   \"conf_matrix\": annotation_list}\n",
    "    return result_dict\n",
    "\n",
    "# def eval_report(y_test, model, testX):\n",
    "def eval_report(y_test, preprocessor, model, testX):\n",
    "    \"\"\"\n",
    "    Evaluates the trained and tuned model against the hold out/test\n",
    "    data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    y_test : dataframe\n",
    "        The test dataset ground truth values.\n",
    "    preprocessor : sklearn pipeline\n",
    "        The preprocessor steps from the sklearn pipeline with the\n",
    "        named step ``all_preprocess``.\n",
    "    model : sklearn pipeline\n",
    "        The algorithm step from the sklearn pipeline with the named\n",
    "        step ``algorithm``.\n",
    "    testX : dataframe\n",
    "        The test dataset features.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    report_dict : dict\n",
    "        A dictionary of the evaluation metrics. These are specifically\n",
    "        formated to be compatible with the Model Registry's Model\n",
    "        Quality page.\n",
    "    \"\"\"\n",
    "\n",
    "    Xprocessed = preprocessor.transform(testX)\n",
    "    predictions = model.predict(Xprocessed)\n",
    "    # predictions = model.predict(testX)\n",
    "    # proba_predictions = model.predict_proba(testX)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    pr_curve = precision_recall_curve(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    # avg_precision = average_precision_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "        \n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\"value\":accuracy, \"standard_deviation\":\"NaN\"},\n",
    "            \"precision\": {\"value\":precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"f1\": {\"value\":f1, \"standard_deviation\":\"NaN\"},\n",
    "            \"recall\": {\"value\":recall, \"standard_deviation\":\"NaN\"},\n",
    "            # \"ROC AUC\": {\"value\":auc_score, \"standard_deviation\":\"NaN\"},\n",
    "            # \"avg_percision\": {\"value\":avg_precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"roc_auc\": {\"value\":roc_auc, \"standard_deviation\":\"NaN\"},\n",
    "            # \"log_loss\": {\"value\":log_loss_score, \"standard_deviation\":\"NaN\"},\n",
    "            # \"informedness\": {\"value\":informedness, \"standard_deviation\":\"NaN\"},\n",
    "            # \"cohen_kappa\": {\"value\":cohen_kappa, \"standard_deviation\":\"NaN\"},\n",
    "            # \"mathews_coef\": {\"value\":matthews_coef, \"standard_deviation\":\"NaN\"},\n",
    "            # \"fbeta\": {\"value\":fbeta, \"standard_deviation\":\"NaN\"},,\n",
    "            # \"roc\": {\n",
    "            #     \"fpr\": roc[0].tolist(),\n",
    "            #     \"tpr\": roc[1].tolist(),\n",
    "            #     \"thresholds\": roc[2].tolist()}\n",
    "            # \"pr_curve\": {\"precision\": pr_curve[0].tolist(),\n",
    "            #              \"recall\": pr_curve[1].tolist(),\n",
    "            #              \"thresholds\": pr_curve[2].tolist()},\n",
    "            \"confusion_matrix\": {\"0\": {\"0\": int(conf_matrix[0][0]), \"1\": int(conf_matrix[0][1])},\n",
    "                                 \"1\": {\"0\": int(conf_matrix[1][0]), \"1\": int(conf_matrix[1][1])}\n",
    "                                },\n",
    "            # \"receiver_operating_charastic_curve\": {\n",
    "            #     \"false_positive_rates\": list(fpr),\n",
    "            #     \"true_positive_rates\": list(tpr)\n",
    "            # }\n",
    "        }\n",
    "    }\n",
    "    return report_dict\n",
    "\n",
    "def explainability_mods(model, Xtrain):\n",
    "    \"\"\"\n",
    "    Trains a shap and an anchor explainer on preprocessed training data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pipeline\n",
    "        Pipeline that has been trained on the training data and includes\n",
    "        two final named steps ``all_preprocess`` and ``algorithm``.\n",
    "    Xtrain : pd.DataFrame\n",
    "        The training dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    shap_explainer : trained model\n",
    "        A fitted shap explainer model.\n",
    "    anchor_explainer : trained model\n",
    "        A fitted anchor explainer model.\n",
    "        \n",
    "    See Also\n",
    "    ---------\n",
    "    shap documentation\n",
    "    alibi documentation\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess_pipe = model.named_steps['all_preprocess']\n",
    "    train_processed = preprocess_pipe.transform(Xtrain)\n",
    "    just_model = model.named_steps['algorithm']\n",
    "    \n",
    "    shap_mask = shap.maskers.Partition(train_processed,\n",
    "                                       max_samples=1000)\n",
    "    shap_explainer = shap.Explainer(just_model.predict,\n",
    "                                    shap_mask,\n",
    "                                    algorithm = \"permutation\")\n",
    "    \n",
    "    predict_fn = lambda x: just_model.predict(x)\n",
    "    anchor_explainer = alibi.explainers.AnchorTabular(predict_fn, train_processed.columns, seed=1)\n",
    "    anchor_explainer.fit(train_processed.to_numpy())\n",
    "    \n",
    "    return shap_explainer, anchor_explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
