{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import joblib\n",
    "import importlib\n",
    "\n",
    "from model_functions_module import train_model, tune_best_algo, explainability_mods\n",
    "from model_functions_module import compile_algo_metrics, metrics_row\n",
    "\n",
    "from sklearn import set_config\n",
    "import alibi\n",
    "\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('params.yaml', 'r') as config_file:\n",
    "    config_params = yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Support Files and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_params['project_dir'], config_params['resource_dir'], \"col_dict.json\"), \"r\") as cd:\n",
    "    col_dict = json.loads(cd.read())\n",
    "with open(os.path.join(config_params['project_dir'], config_params['resource_dir'], \"algo_dict.json\"), \"r\") as ad:\n",
    "    algo_dict = json.loads(ad.read())\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(config_params['project_dir'], config_params['data_dir'], 'train.parquet'))\n",
    "validate_df = pd.read_parquet(os.path.join(config_params['project_dir'], config_params['data_dir'], 'validate.parquet'))\n",
    "test_df = pd.read_parquet(os.path.join(config_params['project_dir'], config_params['data_dir'], 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "KNeighborsClassifier Result Dictionary:\n",
      "{'fit_time': [0.0994877815246582, 0.15291285514831543, 0.0958712100982666, 0.14492177963256836, 0.09598588943481445], 'score_time': [0.3770272731781006, 0.10036420822143555, 0.10024309158325195, 0.09916424751281738, 0.10172629356384277], 'test_accuracy': [0.971875, 0.9725, 0.975625, 0.9775, 0.97125], 'test_precision': [0.9724310776942355, 0.9712858926342073, 0.9822335025380711, 0.9823008849557522, 0.9688279301745636], 'test_f1': [0.9718221665623044, 0.9725, 0.9754253308128544, 0.9773584905660377, 0.97125], 'test_recall': [0.9712140175219024, 0.9737171464330413, 0.9687108886107635, 0.9724655819774718, 0.9736842105263158], 'test_roc_auc': [0.9876101368908389, 0.990012484394507, 0.9898202965942134, 0.9897374839648188, 0.9903155644722779]}\n",
      "DecisionTreeClassifier\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "DecisionTreeClassifier Result Dictionary:\n",
      "{'fit_time': [0.10132598876953125, 0.14481091499328613, 0.09144806861877441, 0.14168691635131836, 0.09182476997375488], 'score_time': [0.05782508850097656, 0.05396008491516113, 0.0536959171295166, 0.05677199363708496, 0.053865909576416016], 'test_accuracy': [0.985625, 0.98875, 0.989375, 0.985625, 0.989375], 'test_precision': [0.9862155388471178, 0.9850931677018634, 0.9924433249370277, 0.983790523690773, 0.9887359198998749], 'test_f1': [0.9855979962429555, 0.9887780548628429, 0.9893283113622097, 0.985633978763273, 0.989355040701315], 'test_recall': [0.9849812265331664, 0.9924906132665833, 0.986232790988736, 0.9874843554443054, 0.9899749373433584], 'test_roc_auc': [0.9856241962878066, 0.9887546699291718, 0.9893710771423081, 0.9856273212926895, 0.9893764961031005]}\n",
      "ExtraTreeClassifier\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "ExtraTreeClassifier Result Dictionary:\n",
      "{'fit_time': [0.08862781524658203, 0.13754487037658691, 0.0881199836730957, 0.13736891746520996, 0.0883479118347168], 'score_time': [0.053037166595458984, 0.05396604537963867, 0.053312063217163086, 0.05382513999938965, 0.05321502685546875], 'test_accuracy': [0.9825, 0.985, 0.985, 0.97875, 0.981875], 'test_precision': [0.9849056603773585, 0.9825653798256538, 0.9874213836477987, 0.9728059332509271, 0.9848675914249685], 'test_f1': [0.9824341279799247, 0.9850187265917603, 0.9849435382685069, 0.9788557213930348, 0.9817724701445631], 'test_recall': [0.9799749687108886, 0.9874843554443054, 0.9824780976220275, 0.9849812265331664, 0.9786967418546366], 'test_roc_auc': [0.9921156126806449, 0.9935281148876794, 0.996869526358635, 0.9933195208117513, 0.9945468409177557]}\n",
      "AdaBoostClassifier\n",
      "Number of collinear feature: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julie.fisher/Documents/.venv310/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collinear feature: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julie.fisher/Documents/.venv310/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collinear feature: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julie.fisher/Documents/.venv310/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collinear feature: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julie.fisher/Documents/.venv310/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "AdaBoostClassifier Result Dictionary:\n",
      "{'fit_time': [0.2107071876525879, 0.24090218544006348, 0.1934976577758789, 0.24126601219177246, 0.19296479225158691], 'score_time': [0.0622868537902832, 0.06271076202392578, 0.06188392639160156, 0.0628509521484375, 0.06223607063293457], 'test_accuracy': [0.99375, 0.994375, 0.99, 0.9925, 0.990625], 'test_precision': [0.9962264150943396, 0.9912935323383084, 0.9949431099873578, 0.9900373599003736, 0.9887640449438202], 'test_f1': [0.9937264742785445, 0.9943855271366189, 0.989937106918239, 0.9925093632958801, 0.9906191369606003], 'test_recall': [0.9912390488110138, 0.9974968710888611, 0.9849812265331664, 0.9949937421777222, 0.9924812030075187], 'test_roc_auc': [0.9996265619165031, 0.9998687497949216, 0.9995734368334951, 0.9996390619360344, 0.9994765592284952]}\n",
      "ExtraTreesClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julie.fisher/Documents/.venv310/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "ExtraTreesClassifier Result Dictionary:\n",
      "{'fit_time': [0.2543790340423584, 0.29968786239624023, 0.2526850700378418, 0.24766325950622559, 0.29374098777770996], 'score_time': [0.06991100311279297, 0.11041522026062012, 0.0699620246887207, 0.07016587257385254, 0.06974005699157715], 'test_accuracy': [0.991875, 0.988125, 0.986875, 0.986875, 0.985625], 'test_precision': [0.9937185929648241, 0.9875, 0.9886934673366834, 0.9874686716791979, 0.986198243412798], 'test_f1': [0.9918495297805643, 0.9881175734834271, 0.986833855799373, 0.986850344395742, 0.9855799373040752], 'test_recall': [0.9899874843554443, 0.9887359198998749, 0.9849812265331664, 0.986232790988736, 0.9849624060150376], 'test_roc_auc': [0.9994343741162096, 0.9993999990624985, 0.9994601554064928, 0.9987187479980437, 0.9990749942187138]}\n",
      "RandomForestClassifier\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "RandomForestClassifier Result Dictionary:\n",
      "{'fit_time': [0.24032998085021973, 0.2883131504058838, 0.23838591575622559, 0.2845950126647949, 0.24013900756835938], 'score_time': [0.06575894355773926, 0.06704592704772949, 0.0661311149597168, 0.0667269229888916, 0.06684279441833496], 'test_accuracy': [0.9925, 0.993125, 0.993125, 0.9925, 0.990625], 'test_precision': [0.9937264742785445, 0.9900497512437811, 0.9987341772151899, 0.9924906132665833, 0.9912170639899623], 'test_f1': [0.9924812030075187, 0.9931378665003119, 0.9930774071743235, 0.9924906132665833, 0.9905956112852664], 'test_recall': [0.9912390488110138, 0.9962453066332916, 0.9874843554443054, 0.9924906132665833, 0.9899749373433584], 'test_roc_auc': [0.9994890617016589, 0.999668749482421, 0.9996843745068351, 0.9991703112036112, 0.9988320239501497]}\n",
      "HistGradientBoostingClassifier\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "HistGradientBoostingClassifier Result Dictionary:\n",
      "{'fit_time': [0.5562450885772705, 0.6257479190826416, 0.5839669704437256, 0.5963180065155029, 0.5315303802490234], 'score_time': [0.06691598892211914, 0.06796908378601074, 0.06709814071655273, 0.06508326530456543, 0.06563091278076172], 'test_accuracy': [0.991875, 0.993125, 0.9925, 0.990625, 0.991875], 'test_precision': [0.99125, 0.9900497512437811, 0.9974715549936789, 0.9912280701754386, 0.9924717691342535], 'test_f1': [0.991869918699187, 0.9931378665003119, 0.9924528301886792, 0.9906073888541015, 0.9918495297805643], 'test_recall': [0.9924906132665833, 0.9962453066332916, 0.9874843554443054, 0.9899874843554443, 0.9912280701754386], 'test_roc_auc': [0.9992203112817363, 0.9997265620727532, 0.9997109370483391, 0.9996156243994132, 0.9994140588378678]}\n",
      "XGBClassifier\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "Number of collinear feature: 3\n",
      "\n",
      "\n",
      "XGBClassifier Result Dictionary:\n",
      "{'fit_time': [0.19951486587524414, 0.1964871883392334, 0.19955778121948242, 0.23488211631774902, 0.1881239414215088], 'score_time': [0.10943412780761719, 0.060933828353881836, 0.06083416938781738, 0.06198692321777344, 0.06838393211364746], 'test_accuracy': [0.993125, 0.994375, 0.9925, 0.99125, 0.99], 'test_precision': [0.993734335839599, 0.9912935323383084, 0.9974715549936789, 0.9937106918238994, 0.9912060301507538], 'test_f1': [0.9931120851596744, 0.9943855271366189, 0.9924528301886792, 0.9912170639899623, 0.9899623588456713], 'test_recall': [0.9924906132665833, 0.9974968710888611, 0.9874843554443054, 0.9887359198998749, 0.9887218045112782], 'test_roc_auc': [0.9994312491113267, 0.9996640619750968, 0.9996515619555655, 0.9994609366577135, 0.999159369746061]}\n"
     ]
    }
   ],
   "source": [
    "Xtrain = train_df.drop(config_params['target_name'], axis=1)\n",
    "ytrain = train_df[[config_params['target_name']]]\n",
    "\n",
    "info_dict = Xtrain.dtypes.apply(lambda x: x.name).to_dict()\n",
    "type_dict = {}\n",
    "for col, dtype in info_dict.items():\n",
    "    type_dict.setdefault(dtype, []).append(col)\n",
    "    \n",
    "col_list = list(Xtrain.columns)\n",
    "\n",
    "algo_list = list(algo_dict.keys())\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "for algo_name in algo_list:\n",
    "    algo_data = algo_dict[algo_name]\n",
    "    module = importlib.import_module(algo_data['module'])\n",
    "    algo = getattr(module, algo_data['class'])\n",
    "    algo_result_dict = train_model(algo, algo_data, col_dict, Xtrain, ytrain)\n",
    "    result_dict = {**result_dict, **algo_result_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is: RandomForestClassifier\n",
      "Model's performance was:\n",
      "test_f1_mean           0.992357\n",
      "test_roc_auc_mean      0.999369\n",
      "test_recall_mean       0.991487\n",
      "test_precision_mean    0.993244\n",
      "test_accuracy_mean     0.992375\n",
      "Name: RandomForestClassifier, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "agg_metrics_df = compile_algo_metrics(result_dict)\n",
    "\n",
    "agg_metrics_df.to_csv(os.path.join(config_params['project_dir'], config_params['metrics_dir'], 'baseline_metrics.csv'))\n",
    "\n",
    "best_algo = agg_metrics_df['test_f1_penalized'].idxmax()\n",
    "best_results = agg_metrics_df.loc[best_algo, ['test_f1_mean',\n",
    "                                          'test_roc_auc_mean',\n",
    "                                          'test_recall_mean',\n",
    "                                          'test_precision_mean',\n",
    "                                          'test_accuracy_mean']]\n",
    "print('Best model is:', best_algo)\n",
    "print(\"Model's performance was:\")\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Tune Best Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning best algorithm\n",
      "Tuning for hyperparameters\n",
      "Training model\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.25, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.989) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.25, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.989) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.25, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.992) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.25, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.994) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.25, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.987) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.983) total time=   0.5s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.986) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.987) total time=   0.5s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.987) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.979) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.25, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=150, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.990) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.25, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=150, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.988) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.25, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=150, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.992) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.25, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=150, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.994) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.25, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=150, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.986) total time=   0.4s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=100, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.984) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=100, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.988) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=100, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.990) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=100, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=1.000) f1: (test=0.988) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=True, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=100, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.984) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=False, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.990) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=False, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.988) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=False, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.991) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=False, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.994) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=False, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.986) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=log2, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=1000, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.989) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=log2, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=1000, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.988) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=log2, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=1000, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.990) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=log2, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=1000, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.992) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=False, algorithm__criterion=entropy, algorithm__max_depth=None, algorithm__max_features=log2, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=1000, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.986) total time=   0.2s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=True, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=sqrt, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.989) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=True, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=sqrt, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=1.000) f1: (test=0.989) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=True, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=sqrt, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=1.000) f1: (test=0.992) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=True, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=sqrt, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=1.000) f1: (test=0.993) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=True, algorithm__criterion=gini, algorithm__max_depth=None, algorithm__max_features=sqrt, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=100, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.984) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=2, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.992) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=2, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.992) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=2, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=1.000) f1: (test=0.991) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=2, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.993) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=100, algorithm__min_samples_leaf=1, algorithm__min_samples_split=2, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.991) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.989) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.991) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=1.000) f1: (test=0.990) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.991) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=False, algorithm__criterion=log_loss, algorithm__max_depth=10, algorithm__max_features=0.5, algorithm__max_leaf_nodes=10, algorithm__min_samples_leaf=1, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=False; auc: (test=0.999) f1: (test=0.986) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 1/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=None, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.986) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 2/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=None, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.986) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 3/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=None, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=1.000) f1: (test=0.987) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 4/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=None, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.991) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "[CV 5/5] END algorithm__bootstrap=True, algorithm__criterion=log_loss, algorithm__max_depth=100, algorithm__max_features=0.75, algorithm__max_leaf_nodes=None, algorithm__min_samples_leaf=10, algorithm__min_samples_split=100, algorithm__n_estimators=50, algorithm__random_state=12, algorithm__warm_start=True; auc: (test=0.999) f1: (test=0.980) total time=   0.3s\n",
      "Number of collinear feature: 3\n",
      "Pipeline(steps=[('all_preprocess',\n",
      "                 Pipeline(steps=[('featurefiller', FeatureFiller()),\n",
      "                                 ('preprocess',\n",
      "                                  ColumnTransformer(transformers=[('true_false',\n",
      "                                                                   TrueFalseTransformer(),\n",
      "                                                                   ['true_false']),\n",
      "                                                                  ('one_hot',\n",
      "                                                                   OneHotTransformer(),\n",
      "                                                                   ['one_hot']),\n",
      "                                                                  ('date_cols',\n",
      "                                                                   DateTransformer(),\n",
      "                                                                   ['dates']),\n",
      "                                                                  ('float_cols',\n",
      "                                                                   FloatTransformer(),\n",
      "                                                                   ['floats']),\n",
      "                                                                  ('max_of_list',\n",
      "                                                                   ListMaxTransformer(),...\n",
      "                                                                   'multi_label'),\n",
      "                                                                  ('drop_cols',\n",
      "                                                                   'drop',\n",
      "                                                                   ['random_col',\n",
      "                                                                    'other'])])),\n",
      "                                 ('dropsingle', DropSingleValueCols()),\n",
      "                                 ('removemulticollinear', RemoveCollinearity()),\n",
      "                                 ('setcolumorder', SetColumnOrder())])),\n",
      "                ('algorithm',\n",
      "                 RandomForestClassifier(bootstrap=False, criterion='log_loss',\n",
      "                                        max_depth=10, max_features=0.5,\n",
      "                                        max_leaf_nodes=100, n_estimators=50,\n",
      "                                        random_state=12, warm_start=True))])\n",
      "Tuned Model's preformance on training data:\n",
      "{'Accuracy': 0.999, 'Precision': 0.998, 'F1': 0.999, 'Recall': 1.0, 'ROC AUC': 0.999, 'conf_matrix': [['True Negative\\n\\nCount: 3999\\nActl Rate: 0.998', 'False Positive\\n\\nCount: 7\\nActl Rate: 0.002'], ['False Negative\\n\\nCount: 1\\nActl Rate: 0.0', 'True Positive\\n\\nCount: 3993\\nActl Rate: 1.0']]}\n",
      "Tuned Model's preformance on validation data:\n",
      "{'Accuracy': 0.991, 'Precision': 0.988, 'F1': 0.991, 'Recall': 0.994, 'ROC AUC': 0.991, 'conf_matrix': [['True Negative\\n\\nCount: 495\\nActl Rate: 0.988', 'False Positive\\n\\nCount: 6\\nActl Rate: 0.012'], ['False Negative\\n\\nCount: 3\\nActl Rate: 0.006', 'True Positive\\n\\nCount: 496\\nActl Rate: 0.994']]}\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "best_algo_data = algo_dict[best_algo]\n",
    "module = importlib.import_module(best_algo_data['module'])\n",
    "algo = getattr(module, best_algo_data['class'])\n",
    "\n",
    "Xvalidate = validate_df.drop(config_params['target_name'], axis=1)\n",
    "yvalidate = validate_df[[config_params['target_name']]]\n",
    "\n",
    "print('Tuning best algorithm')\n",
    "model = tune_best_algo(algo, best_algo_data, col_dict, Xtrain, ytrain)\n",
    "shap_explainer, anchor_explainer = explainability_mods(model, Xtrain)\n",
    "tuned_val_metrics = metrics_row(yvalidate, model, Xvalidate)\n",
    "tuned_train_metrics = metrics_row(ytrain, model, Xtrain)\n",
    "\n",
    "print(\"Tuned Model's preformance on training data:\")\n",
    "print(tuned_train_metrics)\n",
    "\n",
    "print(\"Tuned Model's preformance on validation data:\")\n",
    "print(tuned_val_metrics)\n",
    "\n",
    "pd.DataFrame(tuned_train_metrics).to_csv(os.path.join(config_params['project_dir'], config_params['metrics_dir'], \"tuned_1train_metrics.csv\"))\n",
    "pd.DataFrame(tuned_val_metrics).to_csv(os.path.join(config_params['project_dir'], config_params['metrics_dir'], \"tuned_2validation_metrics.csv\"))\n",
    "\n",
    "print('Saving model')\n",
    "# This is done so that it is all put into a tar.gz file that can be used at inference\n",
    "# processor_steps = model.named_steps['all_preprocess']\n",
    "# model_step = model.named_steps['algorithm']\n",
    "model_dir = \"model_artifacts\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# preprocessor_filepath = os.path.join(model_dir, 'preprocessor.joblib')\n",
    "model_filepath = os.path.join(model_dir, 'model.joblib')\n",
    "shap_filepath = os.path.join(model_dir, 'shap_explainer.joblib')\n",
    "# joblib.dump(processor_steps, preprocessor_filepath)\n",
    "# joblib.dump(model_step, model_filepath)\n",
    "joblib.dump(model, model_filepath)\n",
    "joblib.dump(shap_explainer, shap_filepath)\n",
    "\n",
    "# s3_client.upload_file(Filename=preprocessor_filepath, Bucket=bucket, Key=os.path.join(model_path, \"preprocessor.joblib\"))\n",
    "# s3_client.upload_file(Filename=model_filepath, Bucket=bucket, Key=os.path.join(model_path, \"model.joblib\"))\n",
    "\n",
    "# model_path = os.path.join(config_params['project_dir'], \"05_model_artifacts\")\n",
    "# s3_client.upload_file(Filename=model_filepath, Bucket=bucket, Key=os.path.join(model_path, \"model.joblib\"))\n",
    "# s3_client.upload_file(Filename=shap_filepath, Bucket=bucket, Key=os.path.join(model_path, \"shap_explainer.joblib\"))\n",
    "\n",
    "anchor_local_path = os.path.join(model_dir, 'anchor_explainer')\n",
    "alibi.saving.save_explainer(anchor_explainer, anchor_local_path)\n",
    "\n",
    "# anchor_s3_path = os.path.join(model_dir, \"anchor_explainer\")\n",
    "# for root, _, files in os.walk():\n",
    "#     for filename in files:\n",
    "#         local_filepath = os.path.join(root, filename)\n",
    "#         s3_client.upload_file(Filename=local_filepath, Bucket=bucket, Key=os.path.join(anchor_s3_path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.994, 'Precision': 0.998, 'F1': 0.994, 'Recall': 0.99, 'ROC AUC': 0.994, 'conf_matrix': [['True Negative\\n\\nCount: 499\\nActl Rate: 0.998', 'False Positive\\n\\nCount: 1\\nActl Rate: 0.002'], ['False Negative\\n\\nCount: 5\\nActl Rate: 0.01', 'True Positive\\n\\nCount: 495\\nActl Rate: 0.99']]}\n"
     ]
    }
   ],
   "source": [
    "# with open(os.path.join(config_params['project_dir'], config_params['resource_dir'], 'test.json'), \"r\") as xf:\n",
    "#     contents = json.loads(xf.read())\n",
    "# test_df = pd.DataFrame(pd.read_json(contents))\n",
    "\n",
    "Xtest = test_df.drop(config_params['target_name'], axis=1)\n",
    "ytest = test_df[[config_params['target_name']]]\n",
    "\n",
    "tuned_model = joblib.load(model_filepath)\n",
    "\n",
    "tuned_test_metrics = metrics_row(ytest, tuned_model, Xtest)\n",
    "print(tuned_test_metrics)\n",
    "pd.DataFrame(tuned_test_metrics).to_csv(os.path.join(config_params['project_dir'], config_params['metrics_dir'], \"tuned_3test_metrics.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
