{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 18 - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Options\n",
    "\n",
    "### K-fold cross-validation\n",
    "\n",
    "The data is broken into *k* pieces (where *k* is a user specified number, usually five or ten). The model is trained on all *k* pieces except one, then tested on the final piece. This is repeated *k* times, at which point each of the *k* pieces has been used as the test set.\n",
    "\n",
    "The below images is an example of 5-fold cross-validation:\n",
    "\n",
    "<img src=\"../img/k_fold_cv.png\" width=500>\n",
    "\n",
    "*Note: See The Proposed Solution for a visualization of split and fold.*\n",
    "\n",
    "- **Split**: this is the row in the above image. It includes all data that the algorithm is being trained and tested on.\n",
    "- **Fold**: this is a segment of data. A fold holds the same specific observations from one split to the next, whether it's allocated for training or testing is what changes.\n",
    "\n",
    "\n",
    "An [Introduction to Statistical Learning](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) says this about the choice of the size of *k*: 'there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.'\n",
    "\n",
    "Once all the k-fold models have been run and tested against their specific test set, the performance of all models is averaged to produce an overall score.\n",
    "\n",
    "When using this method, it is important to separate the training and test sets before any preprocessing. If preprocessing is done first, there will be *data leakage*. Data leakage happens when information the model wouldn't have at the time of prediction has been included in the training data. For example, in <font color='red'>Entry 8</font> I covered centering and scaling. Including the values from the test set would alter the mean which is used to center the values. This gives the model information it wouldn't have had if preprocessing was done on only the training set.\n",
    "\n",
    "When using k-fold cross-validation for classification, it's best to stratify the classes so that the portion of each class is the same in each split as it is in the overall dataset. I'll cover stratification in more detail in the next entry.\n",
    "\n",
    "### Leave-one-out cross-validation\n",
    "\n",
    "Leave-one-out is very similar to k-fold. It works on the same principle, but only holds out a single observation as the test set. As such, it creates the same number of models as the number of observations.\n",
    "\n",
    "This method can get really computationally expensive really quickly. If the dataset is only as big as some of the toy datasets I've been using as examples, it should be fine. However, at work the training data regularly includes hundreds of thousands of observations, if not over a million (depending on the time range we train on).\n",
    "\n",
    "Leave-one-out can easily be adapted to leave-k-out, where k is a user defined number of observations to include in the test set. However, for the large datasets I'm practicing for, I'm leaning toward k-fold cross-validation.\n",
    "\n",
    "### Shuffle-split cross-validation\n",
    "\n",
    "\n",
    "\n",
    "### Repeated cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Proposed Solution\n",
    "\n",
    "\n",
    "Steps to complete:\n",
    "- load data\n",
    "- split data\n",
    "- segment into k-folds\n",
    "- preprocess\n",
    "- train\n",
    "- return mean score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_15?keywords=scikit+learn&qid=1583195970&s=books&sr=1-15)\n",
    "- [Train/Test Split and Cross Validation in Python](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "- [A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "- [3.1. Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Why every statistician should know about cross-validation](https://robjhyndman.com/hyndsight/crossvalidation/)\n",
    "- [Cross Validation Gone Wrong](https://betatim.github.io/posts/cross-validation-gone-wrong/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
