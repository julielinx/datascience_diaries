{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 22 - Continuous Targets - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Now that I've got a handle on the measurement options and equations, it's time to implement those measures on actual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Options\n",
    "\n",
    "The two primary options are to list metrics in the `scoring` parameter of a function like `cross_validate` and to use a function from the `metrics` module. There are quite a few things that are held in common between the two different methods. The equations behind the functions are the same but the error and deviance functions return negative values when used as the `scoring` parameter and may return positive values when using the functions from the `metrics` module. I only suspect this because the `metrics` functions don't have `neg_` prefixed to them.\n",
    "\n",
    "In general, the naming convention follows a few rules. According to [Scikit-Learn's documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring):\n",
    "\n",
    "- functions ending with `_score` return a value to maximize, the higher the better\n",
    "- functions ending with `_error` or `_loss` return a value to minimize, the lower the better\n",
    "\n",
    "### `scoring` parameter\n",
    "\n",
    "The first option is to just list scoring methods in the scoring parameter of `cross_validate`. I finally found the list of options on the [3.3. Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html) page under the [3.3.1. The scoring parameter: defining model evaluation rules](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) section, which also groups the options into either classification, clustering, or regression.\n",
    "\n",
    "The metrics that can be used with this method are limited to those that don't require extra parameters. This standardization makes it possible to just name the metric to use without having to bother with dictionaries, lists, optional parameters or any other add ons. The available regression metrics are:\n",
    "\n",
    "- explained_variance\n",
    "- r2\n",
    "- max_error\n",
    "- neg_median_absolute_error\n",
    "- neg_mean_absolute_error\n",
    "- neg_mean_squared_error\n",
    "- neg_mean_squared_log_error\n",
    "- neg_root_mean_squared_error\n",
    "- neg_mean_poisson_deviance\n",
    "- neg_mean_gamma_deviance\n",
    "\n",
    "*Side note*, the ones prefixed with `neg_` return a negative value despite most of them being an absolute or squared value, which would normally be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "I covered explained variance, $R^2$, mean absolute error, mean squared error, and root mean squared error in <font color='red'>Entry 21</font>. Scikit-Learn has a few additional options, which I'll cover here.\n",
    "\n",
    "#### Max error\n",
    "\n",
    "[This metric](https://scikit-learn.org/stable/modules/model_evaluation.html#max-error) is on the Scikit-Learn page. It's the maximum value of the absolute errors.\n",
    "\n",
    "$\\text{max error} = max(| y_{i} - \\hat{y_{i}} |)$\n",
    "\n",
    "#### Median absolute error\n",
    "\n",
    "This is the same as MAE (mean absolute error), but using the median instead of the mean. The benefit of using this instead of MAE is that it is robust to outliers.\n",
    "\n",
    "$median\\text{ }squared\\text{ }error = median(\\sum | y_{i} - \\hat{y_{i}} |)$\n",
    "\n",
    "#### Mean squared logarithmic error\n",
    "\n",
    "This is just the MSE with a logarithmic compoent. [Scikit-Learn recommends](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-log-error) using it with targets with exponential growth. Keep in mind however that because it uses exponentiation it penalizes under-predicted estimates more than over-predicted estimates (larger numbers are reduced more using logarithmic functions). \n",
    "\n",
    "$MSLE = \\frac{1}{n} \\sum{(log_{e}(1+y_{i}) - log_{e}(1+\\hat{y_{i}}))}^2$\n",
    "\n",
    "#### Mean poisson deviance and mean gamma deviance\n",
    "\n",
    "Remeber back at the beginning of this section when I said only metrics that don't require extra parameters can be used with the `cross_validate` function? These two metrics are listed explicitly because of that.\n",
    "\n",
    "Mean poisson/gamma deviance, allong with MSE, belong to a function called tweedie deviance. Mean tweedie deviance error takes a `power` parameter. Power 0 = MSE, power 1 = poisson, power 2 = gamma. The higher the power the less sensitive the metric is to extreme deviations. Scikit-Learn has some good examples in the [3.3.4.8. Mean Poisson, Gamma, and Tweedie deviances](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-tweedie-deviance) section.\n",
    "\n",
    "$mean\\text{ }poisson\\text{ }deviance = 2(y_{i} log(\\frac{y_{i}}{\\hat{y_{i}}}) + \\hat{y_{i}} - y_{i})$\n",
    "\n",
    "$mean\\text{ }gamma\\text{ }deviance = 2(log(\\frac{\\hat{y_{i}}}{y_{i}}) + \\frac{y_{i}}{\\hat{y_{i}}} - 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `metrics` module\n",
    "\n",
    "The functions in the `metrics` module allow for more flexibility than the predefined options for the `scoring` parameter due to being able to take additional parameters. The `mean_squared_error`, `mean_absolute_error`, `explained_variance_score`, and `r2_score functions` can handle multiouput cases. Multiouput cases aren't something I see very often, so I'm going to leave coverage of this topic at that. See the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) for more information.\n",
    "\n",
    "The only additional function available is the full tweedie deviance, which accepts different power inputs. Other than that, the only difference is that the names of the functions are slightly altered. \n",
    "\n",
    "- explained_variance_score\n",
    "- max_error\n",
    "- mean_absolute_error\n",
    "- mean_squared_error\n",
    "- mean_squared_log_error\n",
    "- median_absolute_error\n",
    "- r2_score\n",
    "- mean_poisson_deviance\n",
    "- mean_gamma_deviance\n",
    "- mean_tweedie_deviance\n",
    "\n",
    "The `make_scorer` function from the `metrics` module makes these functions easily accessible. As mentioned above, the functions follow a naming convention which makes it easy to use them with `make_scorer` and `cross-validate`:\n",
    "\n",
    "- functions ending with _score return a value to maximize, the higher the better\n",
    "- functions ending with _error or _loss return a value to minimize, the lower the better. When converting into a scorer object using make_scorer, set the greater_is_better parameter to False\n",
    "\n",
    "The `make_scorer` function also allows for the creation of custom scoring functions. Details on how to do this with examples can be found in section [3.3.1.2. Defining your scoring strategy from metric functions](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring) of the Scikit-Learn documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Proposed Solution\n",
    "\n",
    "I plugged all of the regression metrics into the `scoring` parameter of the `cross_validate` function. The code is basically just a cleaned up version of <font color='red'>Entry 20's notebook</font> with all metrics applicable to a continuous target.\n",
    "\n",
    "The most interesting thing about this was to see the difference between the value returned by the default `score` method (0.81) vs the range of values returned from `cross_validate`. For example, max error ranged from -3.74 to -11.58, $R^2$ ranged from 0.56 to 0.94, and negative root mean squared error ranged from -2.17 to -5.32. $R^2$ and explained variance didn't match, so the mean(error) for this dataset obviously isn't 0.\n",
    "\n",
    "The range of values makes very clear the impact that the splits of the training and test sets makes. A range from 0.56 to 0.94 is much more enlightening than a single value of 0.81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fail\n",
    "\n",
    "After cobbling together everything for the theory entry, this one seemed downright easy. All the information I needed was in the Scikit-Learn documentation.\n",
    "\n",
    "I had to break these two entries into separate posts due to the sheer amount of text needed to flesh out the various equations, provide context for baseline terms and definitions, and explain the nuances of the coding options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Classification metrics - theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "These links all lead to the same basic page: 3.3 Metrics and scoring (the first link). The other three links are to specific portions of the documentation that are relevant to regression metrics.\n",
    "\n",
    "- [3.3. Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [3.3.1. The scoring parameter: defining model evaluation rules](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n",
    "- [3.3.4. Regression metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "- [3.3.1.2. Defining your scoring strategy from metric functions](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
