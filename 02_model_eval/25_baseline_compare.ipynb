{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 25 - Baseline Models\n",
    "\n",
    "As discussed in <font color='red'>Entry 16</font>, certain characteristics in the data can make the model look like it's performing better than it is. One of these characteristics is class imbalance, wherein one class is much more prevalent in the data. By always predicting the majority class, the model can seem like it's performing very well.\n",
    "\n",
    "A simple sanity check is to compare the model performance against a naive dummy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "In order to determine how well or poorly a model is performing, some kind of baseline must be established. This can represent random guessing, an educated guess like the average (regression) or most common (logistic) value, a constant value, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Options\n",
    "\n",
    "The recommendation of [Chris Albon](https://chrisalbon.com/) in *[Machine Learning with Python Cookbook](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386)* is to use a baseline dummy model, which Scikit-learn's Dummy estiamtors do for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Proposed Solution\n",
    "\n",
    "I used the DummyRegressor and the DummyClassifier to create baseline models. Each of the options has multiple choices for the naive solution to use (constant, median, most frequent, etc). By creating a little function I was able to quickly and easily try all of the options that didn't require parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fail\n",
    "\n",
    "I incorporated `cross_validate` with multiple scoring parameters into the function to assess the dummy model but it was too messy and cluttered. A single score worked just fine for this initial test.\n",
    "\n",
    "I thought about building out a full classification model, where I encode the categoricals and all that. However, I'm going to leave this to address in the final post of the series. Then I can practice incorporating all the different steps when I start working my way through the different algorithms.\n",
    "\n",
    "This last point isn't so much a fail as a confession. I did the dummy models in the <font color='red'>Entry 24 notebook</font>. However, I wanted to be able to easily find it for future reference, so I broke it out into it's own post, but the code I figured out while writing the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Thresholds - PR and ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Machine Learning with Python Cookbook](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386)\n",
    "- [3.3.6. Dummy estimators](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)\n",
    "- [sklearn.dummy.DummyRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)\n",
    "- [sklearn.dummy.DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html?highlight=dummyclassifier#sklearn.dummy.DummyClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
