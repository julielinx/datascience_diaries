{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 24 - Scoring Classification Models - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Options\n",
    "\n",
    "### `scoring` parameter\n",
    "\n",
    "The first option is to list scoring methods in the scoring parameter of `cross_validate` like I did for the regression metrics in the <font color='red'>Entry 22 notebook</font>. As a reminder, the list of available parameters is in the [3.3.1. The scoring parameter: defining model evaluation rules](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) section of the Scikit-Learn documentation.\n",
    "\n",
    "The same restrictions that applied to the regression metrics also apply to the classification metrics, they're limited to those that don't require extra parameters. The available classification metrics are:\n",
    "\n",
    "- accuracy\n",
    "- balanced_accuracy\n",
    "- roc_auc\n",
    "- roc_auc_ovr\n",
    "- roc_auc_ovo\n",
    "- roc_auc_ovr_weighted\n",
    "- roc_auc_ovo_weighted\n",
    "- neg_log_loss\n",
    "- neg_brier_score\n",
    "- precision\n",
    "- average_precision\n",
    "- precision_macro\n",
    "- precision_micro\n",
    "- precision_samples\n",
    "- precision_weighted\n",
    "- recall\n",
    "- recall_macro\n",
    "- recall_micro\n",
    "- recall_samples\n",
    "- recall_weighted\n",
    "- f1\n",
    "- f1_macro\n",
    "- f1_micro\n",
    "- f1_samples\n",
    "- f1_weighted\n",
    "- jaccard\n",
    "- jaccard_macro\n",
    "- jaccard_micro\n",
    "- jaccard_samples\n",
    "- jaccard_weighted\n",
    "\n",
    "From the list it's easy to see there are five versions of some of these metrics. Fortunately, they follow a standard naming convention which is spelled out in the [3.3.2.1. From binary to multiclass and multilabel](https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel) section of the Scikit-Learn documentation. Based on the definitions (and the section title), the variations are used on multiclass or multilabel problems.\n",
    "\n",
    "- `macro` simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "- `weighted` accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n",
    "- `micro` gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\n",
    "- `samples` applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes for each sample in the evaluation data, and returning their (sample_weight-weighted) average.\n",
    "\n",
    "There are two metrics on here that weren't in my review of my machine learning books: log loss and brier score.\n",
    "\n",
    "#### Log loss\n",
    "\n",
    "This metric returns the negative log-likelihood of the classifier given the true label.\n",
    "\n",
    "$L_{log}(y,p) = -\\text{logPr}(y | p) = -(y\\text{log}(p) + (1-y)(\\text{log}(1-p))$\n",
    "\n",
    "It's easiest to see in the last equation that one side or the other of the equation cancels out.\n",
    "\n",
    "There is a good explanation of log loss on the [Wiki fast](http://wiki.fast.ai/index.php/Main_Page) entry for [Log Loss](http://wiki.fast.ai/index.php/Log_Loss).\n",
    "\n",
    "The examples on that page are:\n",
    "\n",
    "For a given class label of 1 and a predicted probability of .25:\n",
    "\n",
    "$-{(1\\log(.25) + (1 - 1)\\log(1 - .25))}$</br>\n",
    "$-{(\\log(.25) + 0\\log(.75))}$ </br>\n",
    "$-{\\log(.25)}$\n",
    "\n",
    "For a given class label of 0 and a predicted probability of .25:\n",
    "\n",
    "$-{(0\\log(.25) + (1 - 0)\\log(0 - .25))}$</br>\n",
    "$-{(1\\log(-.25))}$</br>\n",
    "$-{\\log(-.25)}$</br>\n",
    "\n",
    "The metric is designed to penalize both type I and type II errors, but more so to discriminate against predictions that are confident about their wrong prediction. Wiki fast provides the following visualization of this concept:\n",
    "\n",
    "![log loss penality chart](http://wiki.fast.ai/images/4/43/Log_loss_graph.png)\n",
    "\n",
    "In the chart, the actual value is 1. When the probability is very low (the left side of the chart), the log loss value is high. As the probability rises, the log loss quickly decreases to more moderate values and is slow to approach 0 (predicted perfectly).\n",
    "\n",
    "#### Brier score\n",
    "\n",
    "This is the difference between the the probabily assigned to the prediction and the actual outcome.\n",
    "\n",
    "*brier score* $= \\frac{1}{n} \\sum{(f_{t} - o_{t})}^2$\n",
    "\n",
    "Where:\n",
    "- *n* = the total number of predictions\n",
    "- $f_{t}$ = the predicted probability\n",
    "- $o_{t}$ = the actual outcome\n",
    "\n",
    "Basically, for each prediction a probability is assigned. The class the sample is assigned to depends on the threshold for the probability. For simplicity sake, I'll say that anything above 0.5 is assigned to 1 (the true class) and everything below 0.5 is assigned to 0 (the negative class - don't get technical on me about values that are exactly 0.5, this is a *simple* example).\n",
    "\n",
    "So if the probability was 0.65 the sample is assigned to 1. Let's say this is correct and there are 50 total samples. The $\\sum{(f_{t} - o_{t})}^2$ portion of the equation would look as follows:\n",
    "\n",
    "$(0.65 - 1)^2$\n",
    "\n",
    "The same calculation would be applied to each of the 50 samples, then all added together, and finally divided by *n*.\n",
    "\n",
    "The values range from 0 to 1. As the [brier_score_loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) documentation puts it \"the lower the Brier score is for a set of predictions, the better the predictions are calibrated.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `metrics` module\n",
    "\n",
    "The list of classification metrics available in the `metrics` module is more extensive that what's available by default for the `scoring` parameter. The list of functions is:\n",
    "\n",
    "- accuracy_score\n",
    "- balanced_accuracy_score\n",
    "- precision_score\n",
    "- average_precision_score\n",
    "- precision_recall_curve\n",
    "- precision_recall_fscore_support\n",
    "- recall_score\n",
    "- roc_auc_score\n",
    "- roc_curve\n",
    "- cohen_kappa_score\n",
    "- confusion_matrix\n",
    "- hinge_loss\n",
    "- matthews_corrcoef\n",
    "- classification_report\n",
    "- f1_score\n",
    "- fbeta_score\n",
    "- hamming_loss\n",
    "- jaccard_score\n",
    "- log_loss\n",
    "- multilabel_confusion_matrix\n",
    "- zero_one_loss\n",
    "\n",
    "There are a few metrics on the list that I haven't defined yet.\n",
    "\n",
    "#### Hinge loss\n",
    "\n",
    "Based on the Scikit-Learn documentation [3.3.2.10. Hinge loss](https://scikit-learn.org/stable/modules/model_evaluation.html#hinge-loss) and [sklearn.metrics.hinge_loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss) as well as this [medium.com article](https://medium.com/analytics-vidhya/understanding-loss-functions-hinge-loss-a0ff112b40a1), this [towardsdatascience.com article](https://towardsdatascience.com/support-vector-machines-intuitive-understanding-part-1-3fb049df4ba1), and [this blog](https://jamesmccaffrey.wordpress.com/2018/10/04/hinge-loss-explained-with-a-table-instead-of-a-graph/) hinge loss is generally used with SVMs. The examples are all for values of +1 and -1. The purpose of hinge loss is for 'maximum-margin' classification (to place the plane of separation where there is the most space) and considers only prediction errors.\n",
    "\n",
    "$L_{Hinge}(y, w) = max(1 - wy, 0) = | 1 - wy |_{+}$\n",
    "\n",
    "Where:\n",
    "- *y* = true value\n",
    "- *w* = predicted probability\n",
    "\n",
    "This metric seems specialized to work mostly with SVMs and to have the same general purpose as log loss.\n",
    "\n",
    "#### fbeta score\n",
    "\n",
    "This is the same basic metric as the $F_{1}$ score, but adds a parameter, `beta`, that allows for weighting percision or recall.\n",
    "\n",
    "A parameter of `beta` < 1 gives more weight to precision and `beta` > 1 gives more weight to recall. At the extremes `beta` = 0 only considers precision and `beta` = +inf only considers recall.\n",
    "\n",
    "$F_{\\beta} = (1+\\beta^2) \\frac{precision \\times recall}{\\beta^2 precision + recall}$\n",
    "\n",
    "#### Hamming loss\n",
    "\n",
    "Based on the [Wikipedia entry](https://en.wikipedia.org/wiki/Hamming_distance), hamming loss is generally used to determine the distance between two strings. Or as the entry rephrases: \"In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.\"\n",
    "\n",
    "$L_{hamming}(y, \\hat{y}) = \\frac{1}{n_{labels}} \\sum{1(\\hat{y_{i}} \\neq y_{i})}$\n",
    "\n",
    "Where\n",
    "- $n_{labels}$ is the number of classes (or labels)\n",
    "\n",
    "Based on the equation, it looks like this translates to loss functions by taking the average number of mistakes per class. $\\sum{1(\\hat{y_{i}} \\neq y_{i})}$ is basically a count of when the observation and prediction don't match. That is then divided by the number of classes, giving an average number of mistakes per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Proposed Solution\n",
    "\n",
    "### `scoring` parameter\n",
    "\n",
    "Since I'm focused on binary problems, I can ignore the micro/macro/etc variations. As discussed earlier I'm not interested in the accuracy scores due to the imbalanced classes I work with, so those are out too. I'll be addressing ROC/AUC and thresholds in the <font color='red'>next entry</font>, so they'll be dealt with there.\n",
    "\n",
    "This leaves me with the following options:\n",
    "\n",
    "- neg_log_loss\n",
    "- neg_brier_score\n",
    "- precision\n",
    "- average_precision\n",
    "- recall\n",
    "- f1\n",
    "\n",
    "Considering the overall number of classification metrics I'm interseted, I find it a little ironic that I'm left with fewer classification based options that I did for the regression metrics. This really just means I get to figure out how to use the `make_scorer` function.\n",
    "\n",
    "### `metrics` module\n",
    "\n",
    "Using the same criteria as the metrics kept for the `scoring` parameter option and removing what was already covered there reduces my list of functions. I can also remove some functions like `confusion_matrix` that don't return a single scoring value. These cuts leave me with:\n",
    "\n",
    "- balanced_accuracy_score (with `adjusted=True` balanced accuracy is the Youden's J statistic/informedness)\n",
    "- cohen_kappa_score\n",
    "- matthews_corrcoef\n",
    "- fbeta_score\n",
    "\n",
    "### Overview\n",
    "\n",
    "The metrics I decided I'm interested in while completing <font color='red'>Entry 23</font>, along with where they're available is recapped below:\n",
    "\n",
    "- No information rate: easily obtained using the dummy classifiers\n",
    "- Cohen’s Kappa: `metrics` module\n",
    "- Precision: `scoring` parameter\n",
    "- Markedness: not available\n",
    "- Recall: `scoring` parameter\n",
    "- Informedness/Youden’s J index/balanced_accuracy: `metrics` module (*note* I can't use the `balanced_accuracy` option in the `scoring` parameter because I need to set the `adjusted` parameter to `True`\n",
    "- Specificity: may be available via the `imbalanced-learn` package\n",
    "- F1-score: `scoring` parameter\n",
    "- Matthews correlation coefficient: `metrics` module\n",
    "- Critical success index: not available\n",
    "\n",
    "Specificity isn't natively available via either the `scoring` parameter or the `metrics` module. However, the internet pointed out that secificity is recall of the negative class, so it is possible to fanagle it in. There is also a package `imbalanced-learn` that has a `specificity_score` function that I may just be able to wrap in`make_scorer`.\n",
    "\n",
    "Neither markdedness nor critical success index turned up any results with Scikit-Learn. The metrics would have to be custom created and wrapped in the `make_scorer` function. This isn't my focus, so I'll be skipping these two metrics for now.\n",
    "\n",
    "There were a few additional metrics that I may be interested in that were listed as options in Scikit-Learn:\n",
    "\n",
    "- neg_log_loss\n",
    "- neg_brier_score\n",
    "- fbeta_score\n",
    "- log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Naive baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Log Loss](http://wiki.fast.ai/index.php/Log_Loss)\n",
    "- [Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n",
    "- [Brier score](https://en.wikipedia.org/wiki/Brier_score)\n",
    "- [sklearn.metrics.brier_score_loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss)\n",
    "- [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss)\n",
    "- [Understanding loss functions : Hinge loss](https://medium.com/analytics-vidhya/understanding-loss-functions-hinge-loss-a0ff112b40a1)\n",
    "- [Support vector machines ( intuitive understanding ) — Part#1](https://towardsdatascience.com/support-vector-machines-intuitive-understanding-part-1-3fb049df4ba1)\n",
    "- [Hinge Loss Explained with a Table Instead of a Graph](https://jamesmccaffrey.wordpress.com/2018/10/04/hinge-loss-explained-with-a-table-instead-of-a-graph/)\n",
    "- [How do you minimize “hinge-loss”?](https://math.stackexchange.com/questions/782586/how-do-you-minimize-hinge-loss)\n",
    "- [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
