{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry 31 - Training Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very end of [Entry 30](https://julielinx.github.io/blog/30_learning_curves_imp_perform/) I mentioned that learning curves can be used to diagnose problems other than just high bias and high variance. Another easy problem to check for is the appropriateness of the training data.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Algorithms require a minimum amount of data in order to train a model. The amount of data required can be a difficult thing to guess.\n",
    "\n",
    "### More is(n't always) better\n",
    "\n",
    "\"More data is better\" tends to be the catch phrase of machine learning. Aurelien gives two good examples in [Hands-On Machine Learning](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646):\n",
    "\n",
    "- [Scaling to very very large corpora for natural language disambiguation](homl.info/6) by Michele Banko and Eric Brill\n",
    "- [The Unreasonable Effectiveness of Data](homl.info/7) by Alon Halevy, Peter Norvig, and Fernando Pereira\n",
    "\n",
    "These two papers both discuss natural language processing (NLP) problems, but I've heard similar sentiments in other areas as well, such as computer vision.\n",
    "\n",
    "The learning curves in the [Entry 30 notebook](https://github.com/julielinx/datascience_diaries/blob/master/02_model_eval/30_nb_learning_curves.ipynb) proved that more data doesn't necessarily mean better models. The scaled `house_16H` dataset had trained the best model it could after only seeing about 25% of the data.\n",
    "\n",
    "The information in [Entry 30](https://julielinx.github.io/blog/30_learning_curves_imp_perform/) obtained from [Machine Learning Mastery](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) also pointed out that in cases of overfitting, more data can actually make a model worse.\n",
    "\n",
    "### Size (probably) matters\n",
    "\n",
    "Not only can overfitting be avoided by using only as much as data as is needed, but by using less data the algorithm will train faster. However, there is no free lunch in data science.\n",
    "\n",
    "The flip side to using a subset of data is that the way the training data is split can effect the model's predictive power. I described data splitting methods in [Entry 17](https://julielinx.github.io/blog/17_resampling/) and also discussed the potential dangers of selecting a poor subset in relation to classification problems.\n",
    "\n",
    "Problems in data splitting are also present for predicting on continuous values. Aurelien has a great visualization of this on page 25 of *Hands-On Machine Learning*:\n",
    "\n",
    "<img src='../img/training_sample.png'>\n",
    "\n",
    "Training on all the data in the chart produced the solid line. When the red points (slight outliers) were removed from the training data it produced the dashed line. These two models would produce predictions that are quite different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Options\n",
    "\n",
    "This is where the learning curves from the [last entry](https://julielinx.github.io/blog/30_learning_curves_imp_perform/) again come in handy.\n",
    "\n",
    "The only place I saw learning curves explicitly called out as a method to assess the training data size was on page 201 of [Machine Learning with Python Cookbook](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386) - *11.11 Visualizing the Effect of Training Set Size*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Proposed Solution\n",
    "\n",
    "The shape formed by the training and test learning curves provide information about the sufficiency of the training dataset. Once the algorithm has learned all it can from the data, the two curve flattens out and proceed in parallel. If the curve doesn't flatten out, that generally means the model doesn't have enough data.\n",
    "\n",
    "<img src='../img/learning_curve_ex.png'>\n",
    "\n",
    "If the decision is made to reduce the amount of training data, the thing to keep in mind is how the data is split. There may be subsets of the training data that will generate a different model than the one generated by the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fail\n",
    "\n",
    "This still leaves the question of whether the learning curve can also show:\n",
    "\n",
    "- whether there is sufficient test data\n",
    "- whether the data split is appropriate\n",
    "- how sensative the model is to data splits\n",
    "\n",
    "These are perfect questions to address once I get into the modeling series of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Model algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n",
    "- [Machine Learning with Python Cookbook](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386)\n",
    "- [How to use Learning Curves to Diagnose Machine Learning Model Performance](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
