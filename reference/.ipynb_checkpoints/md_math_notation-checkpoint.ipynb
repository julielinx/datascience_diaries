{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e27888b-933e-47b2-9587-382093d326c1",
   "metadata": {},
   "source": [
    "# Markdown Mathematical Examaples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570b8ec-0f21-4205-bf4c-992d65eb75db",
   "metadata": {},
   "source": [
    "$\\displaystyle x_{scale} = \\frac{x}{X_{max}-X_{min}}$\n",
    "\n",
    "$\\displaystyle x_{minscale} = \\frac{x-x_{min}}{X_{max}-X_{min}}$\n",
    "\n",
    "$\\displaystyle x_{norm} = \\frac{x-\\mu}{X_{max}-X_{min}}$\n",
    "\n",
    "$\\displaystyle x_{stand} = \\frac{x - \\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddd1c7-4161-40ab-b8c3-e4ec881b1562",
   "metadata": {},
   "source": [
    "## Regression scoring equations\n",
    "\n",
    "- $y_{i}$ is an observed value\n",
    "- $\\hat{y_{i}}$ is a predicted value\n",
    "- $\\bar{y_{i}}$ is the mean value\n",
    "- $\\mu$ is also the mean value\n",
    "- $n$ is the number of observations\n",
    "- $\\sum$ means to sum things together\n",
    "- $e = y_{i} - \\hat{y_{i}}$\n",
    "- $absolute\\text{ }error = | y_{i} - \\hat{y_{i}} |$\n",
    "- $squared\\text{ }error = (y_{i} - \\hat{y_{i}})^2$\n",
    "\n",
    "Mean Absolute Error (MAE):$MAE = \\frac{\\sum | y_{i} - \\hat{y_{i}} |}{n} = \\frac{1}{n} \\sum | y_{i} - \\hat{y_{i}} |$\n",
    "\n",
    "Sum of Squared Errors (SSE):    $SSE = \\sum (y_{i} - \\hat{y_{i}})^2$\n",
    "\n",
    "Root Mean Squared Error (RMSE):    $RMSE = \\sqrt{\\frac{1}{n} \\sum (y_{i} - \\hat{y_{i}})^2}$\n",
    "\n",
    "$R^2$ or coefficient of determination:    $R^2 = 1 - \\frac{\\sum{(y_{i} - \\hat{y_{i}})^2}}{\\sum{(y_{i} - \\bar{y_{i}})^2}}$ = $R^2 = \\frac{MSE}{Var_{yactual}}$\n",
    "\n",
    "Max error:    $\\text{max error} = max(| y_{i} - \\hat{y_{i}} |)$\n",
    "\n",
    "Median absolute error:    $median\\text{ }squared\\text{ }error = median(\\sum | y_{i} - \\hat{y_{i}} |)$\n",
    "\n",
    "Mean squared logarithmic error:    $MSLE = \\frac{1}{n} \\sum{(log_{e}(1+y_{i}) - log_{e}(1+\\hat{y_{i}}))}^2$\n",
    "\n",
    "### Mean Squared Error (MSE):\n",
    "\n",
    "$MSE = \\frac{\\sum (y_{i} - \\hat{y_{i}})^2}{n} = \\frac{1}{n} \\sum (y_{i} - \\hat{y_{i}})^2$\n",
    "\n",
    "$MSE(X, h_{\\theta}) = \\frac{1}{m} \\sum (\\theta^{T}x^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $X$: matrix of features\n",
    "- $h_{\\theta}$: prediction function, also called a *hypothesis*; $h_{\\theta} = \\theta^{T}x^{(i)}$\n",
    "- $\\theta$: array of weights\n",
    "- $x^{(i)}$: array of features for a specific observation\n",
    "- $y^{(i)}$: observed output for a specific observation\n",
    "\n",
    "### Explained variance\n",
    "\n",
    "$explained\\text{ }variance = 1 - \\frac{Var(y - \\hat{y})}{Var(y)}$\n",
    "\n",
    "Where $Var(y-\\hat{y}) = \\frac{\\sum{(error^2)} - mean(error)}{n}$\n",
    "\n",
    "### Mean poisson deviance and mean gamma deviance\n",
    "\n",
    "$mean\\text{ }poisson\\text{ }deviance = 2(y_{i} log(\\frac{y_{i}}{\\hat{y_{i}}}) + \\hat{y_{i}} - y_{i})$\n",
    "\n",
    "$mean\\text{ }gamma\\text{ }deviance = 2(log(\\frac{\\hat{y_{i}}}{y_{i}}) + \\frac{y_{i}}{\\hat{y_{i}}} - 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01269f-d573-42e7-9f59-62f86cf52090",
   "metadata": {},
   "source": [
    "## Classification scoring equations\n",
    "\n",
    "Prevalence: $prevalence = \\frac{TP+FN}{TP+TN+FP+FN}$\n",
    "\n",
    "Accuracy: $ACC = \\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "Balanced accuracy: $BA = \\frac{TPR+TNR}{2}$\n",
    "\n",
    "Jaccard index: $J(y_{i}, \\hat{y_{i}}) = \\frac{| y_{i} \\bigcap \\hat{y_{i}} |}{| y_{i} \\bigcup \\hat{y_{i}} |}$\n",
    "\n",
    "Zero one loss count: $zero \\text{ } one \\text{ } loss \\text{ } count = FP + FN$\n",
    "\n",
    "Zero one loss ratio: $zero \\text{ } one \\text{ } loss \\text{ } ratio = \\frac{FP + FN}{TP+TN+FP+FN}$\n",
    "\n",
    "Precision / Positive predictive value (PPV): $precision = \\frac{TP}{TP+FP} = 1 - FPR$\n",
    "\n",
    "Markedness (MK): $MK = \\frac{TP}{TP+FP} - \\frac{FN}{FN+TN} = PPV + NPV - 1$\n",
    "\n",
    "Negative predictive value (NPV): $NPV = \\frac{TN}{FN + TN} = 1 - FOR$\n",
    "\n",
    "False discovery rate (FDR): $FDR = \\frac{FP}{TP + FP} = 1 - PPV$\n",
    "\n",
    "False omission rate (FOR): $FOR = \\frac{FN}{FN + TN} = 1 - NPV$\n",
    "\n",
    "Recall / Sensitivity / True positive rate (TPR): $recall = \\frac{TP}{TP + FN} = 1-FNR$\n",
    "\n",
    "Informedness / Bookmaker Informedness (BM): $BM = \\frac{TP}{TP+FN} - \\frac{FN}{TN+FP} = TPR + TRN - 1$\n",
    "\n",
    "Youden's J index: $J = sensitivity + specificity - 1$\n",
    "\n",
    "Specificity (SPC) / True negative rate (TNR): $specificity = \\frac{TN}{TN + FP} = 1 - FPR$\n",
    "\n",
    "False positive rate (FPR), fall-out: $FPR = 1 - \\frac{TN}{TN + FP} = 1 - TNR$\n",
    "\n",
    "False negative rate (FNR) / Miss rate: $FNR = \\frac{FN}{TP + FN} = 1 - TPR$\n",
    "\n",
    "F1-score: $F_{1} = 2 \\times (\\frac{precision \\times recall}{precision + recall}) = \\frac{2TP}{2TP+FP+FN}$\n",
    "\n",
    "Matthews correlation coefficient: $MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$\n",
    "\n",
    "Threat score (TS) / Critical success index (CSI): $TS = \\frac{TP}{TP+FN+FP}$\n",
    "\n",
    "Log loss: $L_{log}(y,p) = -\\text{logPr}(y | p) = -(y\\text{log}(p) + (1-y)(\\text{log}(1-p))$\n",
    "\n",
    "fbeta score: $F_{\\beta} = (1+\\beta^2) \\frac{precision \\times recall}{\\beta^2 precision + recall}$\n",
    "\n",
    "### No information rate\n",
    "\n",
    "*no information rate* $= \\frac{1}{C}$\n",
    "\n",
    "To account for the frequency of the classes:\n",
    "\n",
    "*no information rate* $= max(\\frac{count(c)}{n})$\n",
    "\n",
    "### Cohen's Kappa / Kappa statistic\n",
    "\n",
    "$kappa = \\frac{O - E}{1 - E}$\n",
    "\n",
    "Where\n",
    "\n",
    "- $O$ = observed accuracy\n",
    "- $E$ = expected accuracy based on the confusion matrix's marginal totals\n",
    "\n",
    "### Brier score\n",
    "\n",
    "*brier score* $= \\frac{1}{n} \\sum{(f_{t} - o_{t})}^2$\n",
    "\n",
    "Where:\n",
    "- *n* = the total number of predictions\n",
    "- $f_{t}$ = the predicted probability\n",
    "- $o_{t}$ = the actual outcome\n",
    "\n",
    "### Hinge loss\n",
    "\n",
    "$L_{Hinge}(y, w) = max(1 - wy, 0) = | 1 - wy |_{+}$\n",
    "\n",
    "Where:\n",
    "- *y* = true value\n",
    "- *w* = predicted probability\n",
    "\n",
    "### Hamming loss\n",
    "\n",
    "$L_{hamming}(y, \\hat{y}) = \\frac{1}{n_{labels}} \\sum{1(\\hat{y_{i}} \\neq y_{i})}$\n",
    "\n",
    "Where\n",
    "- $n_{labels}$ is the number of classes (or labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf7179-bf96-4838-8132-6d47bf54fd78",
   "metadata": {},
   "source": [
    "## Profit and cost equations\n",
    "\n",
    "Profit: $profit = xTP - yFP - zFN$\n",
    "\n",
    "### Probability cost function (PCF)\n",
    "\n",
    "$PCF = \\frac{P \\times C(fn)}{P \\times C(fp) + (1 - P) \\times C(fn)}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- *P* is the (prior) probability of the event (all positives)\n",
    "  + I.E., *P* is the proportion of positives in the data\n",
    "  + As such, 1 - *P* is the probability of a non-event, or the proportion of all negatives in the data\n",
    "- *C(fn)* is the cost of a false negative (positive observation predicted as a negative)\n",
    "- *C(fp)* is the cost of a false positive (negative observation predicted as a positive)\n",
    "\n",
    "### Normalized expected cost (NEC)\n",
    "\n",
    "$NEC = PCF \\times (1-TP) + (1-PCF) \\times FP$\n",
    "\n",
    "$\\frac{TP_{1} - TP_{2}}{FP_{1} - FP_{2}} = \\frac{p(-)C(+|-)}{p(+)C(-|+)}$ = $\\frac{p(a)C(b|a)}{p(b)C(a|b)}$\n",
    "\n",
    "Where:\n",
    "- $p(a)$: the probability of a given example being in class $a$\n",
    "- $C(a|b)$: the cost incurred if an example in class $b$ is misclassified as being in class $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303861e-a20c-4ed6-8a2a-2e1e310e498f",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "- $\\hat{y} = w_{0} x_{0} + w_{1} x_{1} + \\dotsb + w_{p} x_{p} + b$\n",
    "- $\\hat{y} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + \\dotsb + \\theta_{n} x_{n}$\n",
    "- $Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\dotsb + \\beta_{p}X_{p} + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91d293-8b4f-48dd-bc3f-365e3a6fb412",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "\n",
    "$\\hat{\\theta} = (X^{T} X)^{-1} X^{T} y$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\hat{\\theta}$: theta array, the hypothesized weights\n",
    "- $X$: input feature matrix\n",
    "- $X^{T}$: the transpose of X\n",
    "- $y$: array of target values\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha \\frac{1}{m}\\displaystyle\\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}$\n",
    "\n",
    "Where:\n",
    "- $\\theta_{j}$: the specific feature being updated\n",
    "- $:=$ is assignment (in Python it's like writing `==`)\n",
    "- $\\alpha$: learning rate\n",
    "- $m$: number of training examples\n",
    "- $x^{(i)}$: the feature array of the *i*th observation\n",
    "- $h_{\\theta}(x^{(i)})$: returns the predicted value for the *i*th observation\n",
    "- $y^{(i)}$: the observed value for the *i*th observation\n",
    "\n",
    "A vectorized version as written in [Hands-On Machine Learning with Scikit-Learn](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646) is:\n",
    "\n",
    "$\\theta^{\\text{next step}} = \\theta - \\eta \\frac{2}{m}X^{T}(X \\theta -y)$\n",
    "\n",
    "Where\n",
    "- $\\theta$: the theta array\n",
    "- $\\eta$: the learning rate (previously notated as $\\alpha$); the symbol is eta\n",
    "- $X$: input feature matrix\n",
    "- $X^{T}$: the transpose of $X$\n",
    "- $y$: the array of target values\n",
    "\n",
    "Gradient Descent = $\\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)$\n",
    "\n",
    "- The theta array ($\\theta$)\n",
    "- The learning rate ($\\alpha$)\n",
    "- The partial derivative ($\\frac{\\partial}{\\partial \\theta_{j}}$)\n",
    "- The cost function ($J(\\theta)$)\n",
    "\n",
    "### Expanded Gradient Descent equation\n",
    "\n",
    "This variation spells out all the different parts of the equation. When all the terms are explicitly spelled out it becomes easier to see underlying similarities like the one between MSE and the cost function ($\\frac{1}{m} \\sum(\\hat{y}_{i} - y_{i})^2 = \\frac{1}{m} \\sum(h_{\\theta}(x_{i}) - y_{i})^2 \\approx \\frac{1}{m} \\sum(h_{\\theta}(x^{(i)}) - y^{(i)})$\n",
    "\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha \\frac{1}{m}\\displaystyle\\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\theta_{j}$: the specific feature being updated\n",
    "- $:=$ is assignment (in Python it's like writing `==` instead of `=`)\n",
    "- $\\alpha$: alpha, the learning rate or step size\n",
    "- $m$: number of training examples\n",
    "- $x^{(i)}$: the feature array of the *i*th observation\n",
    "- $h_{\\theta}(x^{(i)})$: returns the predicted value for the *i*th observation\n",
    "  - $h_{\\theta}(x)$: a function that outputs a predicted value\n",
    "  - $h_{0}(x) = \\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\dotsb + \\theta_{n}x_{n} = \\theta^{T} x$\n",
    "  - For the line example in the Purpose section, the function would be $h_{\\theta}(x) = \\hat{y} = 0.88X + 0.03$\n",
    "  - In Linear Regression $h_{\\theta}(x)$ is assumed to be a linear function\n",
    "- $y^{(i)}$: the observed value for the *i*th observation\n",
    "\n",
    "### Simplified Gradient Descent equation\n",
    "\n",
    "The simplified version just condenses some of the terms. This makes it easier to see how changes to the cost function effect the overall equation.\n",
    "\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\theta_{j}$: a specific value in the theta array\n",
    "- $:=$ is assignment (in Python it's like writing `==`)\n",
    "- $\\alpha$: alpha, the learning rate or step size\n",
    "- $\\frac{\\partial}{\\partial \\theta_{j}}$: partial derivative, i.e. direction of change for $\\theta$\n",
    "- $J(\\theta)$: cost function (also written as: $J(\\theta_{0}, \\theta_{1}, \\dotsb, \\theta_{n})$)\n",
    "  - $\\frac{1}{2m}\\displaystyle\\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2$\n",
    "    - Where\n",
    "      - *m* is the number of training examples\n",
    "      - $x^{(i)}$ is the feature array of the *i*th observation\n",
    "      - $h_{\\theta}(x^{(i)})$ returns the predicted value for the *i*th observation\n",
    "      - $y^{(i)}$ is the observed value for the *i*th observation\n",
    "    - So, if we're finding the *mean* squared error, why are we multiplying by $\\frac{1}{2m}$ instead of $\\frac{1}{m}$? According to Andrew Ng in the [Machine Learning course](https://www.coursera.org/learn/machine-learning), multiplying by $\\frac{1}{2}$ makes the math easier\n",
    "    - For more than one feature the partial derivative for the cost function changes slightly\n",
    "        - $\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}$\n",
    "          - Where the j in $\\theta_{j}$ is the weight for a specific feature being updated\n",
    "          - Note that the \"squared\" portion of the term has disappeared from the $J(\\theta)$ portion of the equation and instead is multiplied by $x_{j}^{(i)}$\n",
    "          - This is why the cost function portion of the full equation looks different from the straight cost function\n",
    "\n",
    "### Vectorized Gradient Descent equation\n",
    "\n",
    "The vectorized version can be applied to the full dataset at once instead of row by row. This variation is from *Hands-On Machine Learning with Scikit-Learn* instead of the *Machine Learning* course, so the notation is slightly different.\n",
    "\n",
    "$\\theta^{\\text{next step}} = \\theta - \\eta \\frac{2}{m}X^{T}(X \\theta -y)$\n",
    "\n",
    "Where\n",
    "\n",
    "- $\\theta$: the theta array\n",
    "- $\\eta$: eta, the learning rate (previously notated as $\\alpha$)\n",
    "- $X$: input feature matrix\n",
    "- $X^{T}$: the transpose of $X$\n",
    "- $y$: the array of target values\n",
    "\n",
    "Making the notation consistent with the other two variations, the equation would look like this:\n",
    "\n",
    "$\\theta_{j} := \\theta_{j} - \\alpha \\frac{2}{m}X^{T}(X \\theta - y)$\n",
    "\n",
    "Breaking out the pieces of the equation helps us see where the four main components are:\n",
    "\n",
    "Gradient descent: $\\theta^{\\text{next step}} = \\theta - \\eta \\nabla_{\\theta} MSE(\\theta)$\n",
    "- Theta array\n",
    "  - Still written as $\\theta$\n",
    "- Learning rate\n",
    "  - Previously we had notated the learning rate as $\\alpha$, whereas in this equation it's written as $\\eta$\n",
    "- Partial derivative of the cost function\n",
    "  - $\\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta) = \\frac{2}{m} \\displaystyle\\sum_{i=1}^m (\\theta^{T}x^{(i)} - {y}^{(i)})x_{j}^{(i)}$\n",
    "  - Previously we had notated the cost function as $J(\\theta)$, whereas in this equation it's written as $MSE(\\theta)$\n",
    "  - The vectorized version is notated as: $\\nabla_{\\theta} MSE(\\theta) = \\frac{2}{m} X^{T}(X \\theta - y)$\n",
    "- Cost function\n",
    "  - $MSE(\\theta) = \\frac{1}{m} \\displaystyle\\sum_{i=1}^m (\\theta^{T}x^{(i)} - {y}^{(i)})^{2}$\n",
    "  - Previously we had notated the cost function as $J(\\theta)$, whereas in this equation it's written as $MSE(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8405fd-f928-4b45-a82d-83900c1c25cf",
   "metadata": {},
   "source": [
    "### OLS linear regression - LASSO\n",
    "\n",
    "- Cost function: $J(\\theta) = MSE(\\theta) + \\alpha \\displaystyle\\sum_{i=1}^n |\\theta_{i}|$\n",
    "  - Where\n",
    "    - MSE: mean squared error\n",
    "    - $\\alpha$: regularization term\n",
    "    - $\\theta$: the theta array\n",
    "- Lasso regression subgradient vector: $g(\\theta, J) = \\nabla_{0} \\mathrm{MSE}(\\theta) + \\alpha \n",
    "  \\begin{pmatrix}\n",
    "    \\mathrm{sign}(\\theta_{1}) \\\\\n",
    "    \\mathrm{sign}(\\theta_{2}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathrm{sign}(\\theta_{n})\n",
    "  \\end{pmatrix}$\n",
    "  - Where\n",
    "    - $g(\\theta, J)$: subgradient vector\n",
    "    - $ \\nabla_{0}$: differential operator (the symbol is called \"nabla\")\n",
    "    - $\\mathrm{MSE}$: mean squared error\n",
    "    - $\\alpha$: the regularization term\n",
    "    - $\\mathrm{sign}(\\theta_{i}) = \n",
    "  \\begin{cases}\n",
    "    -1   & \\mathrm{if } \\; \\theta_{i} < 0 \\\\\n",
    "    \\;\\,\\,0    & \\mathrm{if } \\; \\theta_{i} = 0 \\\\\n",
    "    +1   & \\mathrm{if } \\; \\theta_{i} > 0\n",
    "  \\end{cases}$\n",
    "  \n",
    "### OLS linear regression - ridge\n",
    "\n",
    "- Cost function: $J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\displaystyle\\sum_{i=1}^n \\theta_{i}^{2}$\n",
    "  - Note: The bias term $\\theta_{0}$ is not regularized as indicated by the start of $i$ at 1 instead of 0\n",
    "  - Where\n",
    "    - MSE: mean squared error\n",
    "    - $\\alpha$: regularization term\n",
    "    - $\\theta$: the theta array\n",
    "- Vectorized ridge regression: $\\hat{\\theta} = (X^{T} X + \\alpha A)^{-1} X^{T}y$\n",
    "  - Where\n",
    "    - $\\theta$: the theta array\n",
    "    - $X$: the feature matrix\n",
    "    - $X$: the transpose of the feature matrix\n",
    "    - $\\alpha$: the regularization term\n",
    "    - $A$: the identify matrix, except with the top-left cell being 0 (i.e., the bias term)\n",
    "    - $y$: the target array\n",
    "    \n",
    "### Elastic Net\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + r \\alpha \\displaystyle\\sum_{i=1}^n |\\theta_{i}| + \\frac{1 - r}{2} \\alpha \\displaystyle\\sum_{i=1}^n \\theta_{i}^{2}$\n",
    "\n",
    "Where\n",
    "\n",
    "- $J(\\theta)$: cost function\n",
    "- MSE: mean squared error\n",
    "- $r$: mix ratio\n",
    "  - When r = 0, Elastic Net is the same as Ridge Regression\n",
    "    - $r \\alpha \\displaystyle\\sum_{i=1}^n |\\theta_{i}|$ cancels to 0\n",
    "  - When r = 1, Elastic Net is the same as Lasso Regression\n",
    "    - $\\frac{1 - r}{2} \\alpha \\displaystyle\\sum_{i=1}^n \\theta_{i}^{2}$ cancels to 0\n",
    "- $\\alpha$: regularization term\n",
    "- $\\theta$: the theta array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3580974-547c-4b0e-aed9-8b86b226ebdc",
   "metadata": {},
   "source": [
    "## L1 and L2 Regularization\n",
    "\n",
    "- $\\text{L1 norm} = ||w||_{1} = |w_{1}| + |w_{2}| + \\dotsb + |w_{n}|$\n",
    "- $\\text{L2 norm} = ||w||_{2} = \\sqrt{|w_{1}|^2 + |w_{2}|^2 + \\dotsb + |w_{n}|^2}$\n",
    "- $\\text{Lp norm} = ||w||_{p} = \\sqrt[p]{|w_{1}|^p + |w_{2}|^p + \\dotsb + |w_{n}|^p}$\n",
    "\n",
    "#### Cost functions\n",
    "\n",
    "- Base cost function:\n",
    "  - $J(\\theta) = MSE(\\theta) = \\frac{1}{m} \\displaystyle\\sum_{i=1}^m (\\theta^{T}x^{(i)} - y^{(i)})^{2}$\n",
    "- Cost function with L1 regularization:\n",
    "  - $J(\\theta) = MSE(\\theta) + \\alpha \\displaystyle\\sum_{i=1}^n |\\theta_{i}|$\n",
    "- Cost function with L2 regularization:\n",
    "  - $J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\displaystyle\\sum_{i=1}^n \\theta_{i}^{2}$\n",
    "  \n",
    "From there, it's easy to turn into a classification prediction:\n",
    "\n",
    "$\\hat{y} = \n",
    "\\begin{cases}\n",
    "    0 \\text{ if } \\hat{p} < 0.5\\\\\n",
    "    1 \\text{ if } \\hat{p} \\geq 0.5\n",
    "  \\end{cases}$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8dccb-6efa-4bca-a672-e254c42faa84",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "$\\hat{y} = h_{0}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\dotsb + \\theta_{n}x_{n} = \\theta^{T} x$\n",
    "\n",
    "Where\n",
    "\n",
    "- $h_{\\theta}$: prediction function, also called a *hypothesis*; $h_{\\theta} = \\theta^{T}x^{(i)}$\n",
    "- $\\theta$: vector (I.E., list) of weights, with the first value being the y-intercept type value (represented by *b* in *Introduction to Machine Learning*)\n",
    "- $x$: the matrix of feature values (I.E., the DataFrame) with the first column ($x_{0}$, not listed in the equation) being all 1s so that $\\theta_{0}$ is always evaluated as the same value\n",
    "\n",
    "To make the linear regression equation into a logistic regression equation, a sigmoid function is added:\n",
    "\n",
    "$h_{0}(x) = g(\\theta^{T}x) = g(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-\\theta^{T}x}}$\n",
    "\n",
    "Where:\n",
    "- $z = X \\theta = \\theta^{T}x$\n",
    "  - (m x n) x (n x 1 ) = (m x 1)\n",
    "- e = Euler's number (i.e., the base of natural logarithm)\n",
    "  - The exact value is in the `math` library: `math.e`\n",
    "  - To use it in this equation, `math.exp(-z)`\n",
    "- g = sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a6310-2466-48d4-8d3c-8daced5a2af5",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "#### `DecisionTreeClassifier`\n",
    "\n",
    "- **Default**: gini impurity\n",
    "  - From page 234 of *Machine Learning with Python Cookbook*\n",
    "    - $G(t) = 1 - \\displaystyle\\sum_{i=1}^{c} P_{i}^2$\n",
    "    - Where\n",
    "      - $G(t)$: gini impurity at node $t$\n",
    "      - $t$: a specific node\n",
    "      - $c$: class\n",
    "      - $P_{i}$: proportion of observations of class $c$ at node $t$\n",
    "  - From page 177 of *Hands-On Machine Learning*\n",
    "    - $G_{i} = 1 - \\displaystyle\\sum_{k=1}^{n} P_{i,k}^2$\n",
    "    - Where\n",
    "      - $G_{i}$: gini impurity at node $i$\n",
    "      - $i$: a specific node\n",
    "      - $k$: class\n",
    "      - $P_{i, k}^2$: the raio of class $k$ instances among the training instances of node $i$\n",
    "- **Alternate**: entropy\n",
    "    - $H_{i} = - \\displaystyle\\sum_{\\substack{k=1\\\\ P_{i, k} \\neq 0}}^{n} P_{i, k} log_{2}(P_{i, k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65734ce-ab27-4430-a6cc-1bc16b0f055b",
   "metadata": {},
   "source": [
    "## Other\n",
    "\n",
    "- density = $\\frac{M}{v}$\n",
    "- gravity = $\\frac{GM}{r^2}$\n",
    "- escape velocity = $\\sqrt{\\frac{2GM}{r}}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- M = mass\n",
    "- v = volume\n",
    "- G = gravational constant\n",
    "- r = radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fd9db-1dd0-49d8-8487-1819530d97d9",
   "metadata": {},
   "source": [
    "## References / Examples\n",
    "\n",
    "- [Entry 8: Centering and Scaling](https://github.com/julielinx/datascience_diaries/blob/master/01_ml_process/08_center_scale_and_latex.ipynb)\n",
    "- [Entry 21: Scoring Regression Models - Theory](https://github.com/julielinx/datascience_diaries/blob/master/02_model_eval/21_reg_score_theory_and_latex.ipynb)\n",
    "- [Entry 22: Scoring Regression models - Implementation](https://github.com/julielinx/datascience_diaries/blob/master/02_model_eval/22_reg_score_implement.ipynb)\n",
    "- [Entry 23: Scoring Classification Models - Theory](https://github.com/julielinx/datascience_diaries/blob/master/02_model_eval/23_class_score_theory.ipynb)\n",
    "- [Entry 24: Scoring Classification Models - Implementation](https://github.com/julielinx/datascience_diaries/blob/master/02_model_eval/24_class_score_implement.ipynb)\n",
    "- [Entry 29: Thresholds - Profit and cost](https://github.com/julielinx/datascience_diaries/blob/master/02_model_eval/29_thresholds_profit_cost.ipynb)\n",
    "- [Entry 35: Regression](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/35_regression.ipynb)\n",
    "- [Entry 36: Ordinary Least Squares (OLS)](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/36_regression_OLS.ipynb)\n",
    "- [Entry 37a: Normal Equation](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/37a_regression_normal_equation.ipynb)\n",
    "- [Entry 37b: Gradient Descent](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/37b_regression_gradient_descent.ipynb)\n",
    "- [Entry 38: Regularization](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/38_l1_l2_regularization.ipynb)\n",
    "- [Entry 39: Lasso Regression](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/39_regression_lasso.ipynb)\n",
    "- [Entry 40: Ridge Regression](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/40_regression_ridge.ipynb)\n",
    "- [Entry 41: Elastic Net](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/41_regression_elasticnet.ipynb)\n",
    "- [Entry 42: Logistic Regression](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/01_regression/42_regression_logistic.ipynb)\n",
    "- [Entry 48: Decision Tree Impurity Measures](https://github.com/julielinx/datascience_diaries/blob/master/03_supervised_learning/02_tree_based/48_trees_impurity.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
