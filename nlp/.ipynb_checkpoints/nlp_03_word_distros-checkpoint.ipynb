{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry ? - Word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "## Filter data, create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ratios(df, text_col):\n",
    "    df['char_count'] = df[text_col].str.len()\n",
    "    df['word_count'] = df[text_col].str.count('\\\\w+')\n",
    "    df['ltr_count'] = df[text_col].str.count('[A-Za-z]')\n",
    "    df['ltr_ratio'] = (df['ltr_count'] / df['char_count']).fillna(0)\n",
    "    df['spec_char_count'] = df[text_col].apply(lambda x: sum(map(x.count, string.punctuation)))\n",
    "    df['spec_char_ratio'] = (df['spec_char_count'] / df['char_count']).fillna(0)\n",
    "    df['num_count'] = df[text_col].str.count('[0-9]+')\n",
    "    df['num_ratio'] = (df['num_count'] / df['char_count']).fillna(0)\n",
    "    df['vowel_count'] = df[text_col].str.count('[aeiouyAEIOUY]')\n",
    "    df['vowel_ratio'] = (df['vowel_count'] / df['char_count']).fillna(0)\n",
    "    df['caps_count'] = df[text_col].str.count('[A-Z]')\n",
    "    df['caps_ratio'] = (df['caps_count'] / df['char_count']).fillna(0)\n",
    "    df['newline_tab_count'] = df[text_col].str.count(r'[\\t\\r\\n]')\n",
    "    df['newline_tab_ratio'] = (df['newline_tab_count'] / df['char_count']).fillna(0)\n",
    "    df['qwerty_count'] = df[text_col].str.count('[asdfghjkl]')\n",
    "    df['qwerty_ratio'] = (df['qwerty_count'] / df['char_count']).fillna(0)\n",
    "    return df\n",
    "    \n",
    "def punct_tokens(df, text_col):\n",
    "    newline_list = '\\t\\r\\n'\n",
    "    remove_newline = str.maketrans(' ', ' ', newline_list)\n",
    "    emoji_string = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    punct_list = string.punctuation + '-‘_”' + emoji_string\n",
    "    nopunct = str.maketrans('', '', punct_list)\n",
    "    df['no_punct_tokens'] = df[text_col].fillna(\"\").str.lower().str.translate(remove_newline).str.translate(nopunct).str.split()\n",
    "    df['distinct_word_count'] = df['no_punct_tokens'].apply(lambda x: len(set(x)))\n",
    "    df['max_word_len'] = df['no_punct_tokens'].apply(lambda x: max([len(word) for word in x], default=0))\n",
    "    df['min_word_len'] = df['no_punct_tokens'].apply(lambda x: min([len(word) for word in x], default=0))\n",
    "    df['word_len_range'] = df['max_word_len'] - df['min_word_len']\n",
    "    df['word_diversity'] = (df['distinct_word_count'] / df['word_count']).fillna(0)\n",
    "    df['avg_word_len'] = (df['char_count'] / df['word_count']).fillna(0)\n",
    "    df['repeat_ltrs'] = df['no_punct_tokens'].apply(lambda x: [word for word in x if re.search(r'([a-zA-Z])\\1{2,}', word.lower())])\n",
    "    df['repeat_ltr_count'] = df[text_col].str.count(r'([a-zA-Z])\\1{2,}')\n",
    "    df['repeat_ltr_ratio'] = (df['repeat_ltr_count'] / df['word_count']).fillna(0)\n",
    "    return df\n",
    "\n",
    "def tribi_grams(df):\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    df['unigrams'] = df['no_punct_tokens'].apply(lambda x: [item for item in x if item not in stop])\n",
    "    df['bigrams'] = df['unigrams'].apply(lambda x:(list(nltk.bigrams(x))))\n",
    "    df['trigrams'] = df['unigrams'].apply(lambda x:(list(nltk.trigrams(x))))\n",
    "#     df['spam_unigram'] = df.apply(lambda x: [[x['spam'],word] for word in x['no_punct_tokens'], axis=1])\n",
    "#     df['spam_bigram'] = df.apply(lambda x: [[x['spam'],word] for word in x['bigrams'], axis=1])\n",
    "#     df['spam_trigram'] = df.apply(lambda x: [[x['spam'],word] for word in x['trigrams'], axis=1])\n",
    "    return df\n",
    "\n",
    "def clean_features(df, text_col):\n",
    "    df = count_ratios(df, text_col)\n",
    "    df = punct_tokens(df, text_col)\n",
    "    df.loc[df['avg_word_len'] == np.inf, 'avg_word_len'] = 0\n",
    "    df = tribi_grams(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = nltk.corpus.stopwords.words('english')\n",
    "# # To make a custom list, just do pd.Series(['words', 'to', 'remove'])\n",
    "\n",
    "# def create_word_freq(df, text_col):\n",
    "#     newline_list = '\\t\\r\\n'\n",
    "#     remove_newline = str.maketrans(' ', ' ', newline_list)\n",
    "#     emoji_string = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "#     punct_list = string.punctuation + '-‘_”' + emoji_string\n",
    "#     nopunct = str.maketrans('', '', punct_list)\n",
    "#     freq = df[text_col].str.lower().str.translate(\n",
    "#         remove_newline).str.translate(nopunct).str.split().apply(\n",
    "#         lambda x: [item for item in x if item not in stop])\n",
    "#     return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_unigram_cfd(df):\n",
    "#     gram_words = list(itertools.chain(*df.fraud_unigram.ravel()))\n",
    "#     gram_cfd = nltk.ConditionalFreqDist(gram_words)\n",
    "#     gram_cfd_df = pd.DataFrame(gram_cfd)\n",
    "#     gram_cfd_df = gram_cfd_df[~gramcfd_df.index_isin(stop)]\n",
    "#     gram_cfd_df['email_count'] = gram_cfd_df[0].fillna(0) + gram_cfd_df[1].fillna(0)\n",
    "#     gram_cfd_df['unigram_spam_ratio'] = gram_cfd_df[1].fillna(0) / gram_cfd_df['email_count'].fillna(0)\n",
    "#     return gram_cfd_df\n",
    "\n",
    "# def create_bigram_cfd(df):\n",
    "#     gram_words = list(itertools.chain(*df.fraud_bigram.ravel()))\n",
    "#     gram_cfd = nltk.ConditionalFreqDist(gram_words)\n",
    "#     gram_cfd_df = pd.DataFrame(gram_cfd)\n",
    "#     gram_cfd_df['email_count'] = gram_cfd_df[0].fillna(0) + gram_cfd_df[1].fillna(0)\n",
    "#     gram_cfd_df['unigram_spam_ratio'] = gram_cfd_df[1].fillna(0) / gram_cfd_df['email_count'].fillna(0)\n",
    "#     return gram_cfd_df\n",
    "\n",
    "# def create_trigram_cfd(df):\n",
    "#     gram_words = list(itertools.chain(*df.fraud_trigram.ravel()))\n",
    "#     gram_cfd = nltk.ConditionalFreqDist(gram_words)\n",
    "#     gram_cfd_df = pd.DataFrame(gram_cfd)\n",
    "#     gram_cfd_df['email_count'] = gram_cfd_df[0].fillna(0) + gram_cfd_df[1].fillna(0)\n",
    "#     gram_cfd_df['unigram_spam_ratio'] = gram_cfd_df[1].fillna(0) / gram_cfd_df['email_count'].fillna(0)\n",
    "#     return gram_cfd_df\n",
    "\n",
    "# def create_gram_cfd(df):\n",
    "#     uni_cfd = create_unigram_cfd(df)\n",
    "#     bi_cfd = create_bigram_cfd(df)\n",
    "#     tri_cfd = create_trigram_cfd(df)\n",
    "#     return uni_cfd, bi_cfd, tri_cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_cfd(df):\n",
    "    gram_words = list(itertools.chain(*df.no_punct_tokens.ravel()))\n",
    "    gram_cfd = nltk.ConditionalFreqDist(gram_words)\n",
    "    gram_cfd_df = pd.DataFrame(gram_cfd)\n",
    "    gram_cfd_df = gram_cfd_df[~gramcfd_df.index_isin(stop)]\n",
    "    gram_cfd_df['instance_count'] = gram_cfd_df[0].fillna(0) + gram_cfd_df[1].fillna(0)\n",
    "    gram_cfd_df['instance_ratio'] = gram_cfd_df[1].fillna(0) / gram_cfd_df['instance_count'].fillna(0)\n",
    "    return gram_cfd_df\n",
    "\n",
    "def create_bigram_cfd(df):\n",
    "    gram_words = list(itertools.chain(*df.fraud_bigram.ravel()))\n",
    "    gram_cfd = nltk.ConditionalFreqDist(gram_words)\n",
    "    gram_cfd_df = pd.DataFrame(gram_cfd)\n",
    "    gram_cfd_df['instance_count'] = gram_cfd_df[0].fillna(0) + gram_cfd_df[1].fillna(0)\n",
    "    gram_cfd_df['instance_ratio'] = gram_cfd_df[1].fillna(0) / gram_cfd_df['instance_count'].fillna(0)\n",
    "    return gram_cfd_df\n",
    "\n",
    "def create_trigram_cfd(df):\n",
    "    gram_words = list(itertools.chain(*df.fraud_trigram.ravel()))\n",
    "    gram_cfd = nltk.ConditionalFreqDist(gram_words)\n",
    "    gram_cfd_df = pd.DataFrame(gram_cfd)\n",
    "    gram_cfd_df['instance_count'] = gram_cfd_df[0].fillna(0) + gram_cfd_df[1].fillna(0)\n",
    "    gram_cfd_df['instance_ratio'] = gram_cfd_df[1].fillna(0) / gram_cfd_df['instance_count'].fillna(0)\n",
    "    return gram_cfd_df\n",
    "\n",
    "def create_gram_cfd(df):\n",
    "    uni_cfd = create_unigram_cfd(df)\n",
    "    bi_cfd = create_bigram_cfd(df)\n",
    "    tri_cfd = create_trigram_cfd(df)\n",
    "    return uni_cfd, bi_cfd, tri_cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all, df_counts, df_ratios = split_data(sms_df)\n",
    "\n",
    "word_freq = create_word_freq(sms_df, 'text')\n",
    "unigram_cfd, bigram_cfd, trigram_cfd = create_gram_cfd(sms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']),\n",
       "       list(['ok', 'lar', 'joking', 'wif', 'u', 'oni']),\n",
       "       list(['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', 'overs']),\n",
       "       ..., list(['pity', 'mood', 'soany', 'suggestions']),\n",
       "       list(['guy', 'bitching', 'acted', 'like', 'id', 'interested', 'buying', 'something', 'else', 'next', 'week', 'gave', 'us', 'free']),\n",
       "       list(['rofl', 'true', 'name'])], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_cfd(df):\n",
    "    gram_words = list(itertools.chain(*df.no_punct_tokens.ravel()))\n",
    "    gram_cfd = nltk.ConditionalFreqDist(gram_words)\n",
    "    gram_cfd_df = pd.DataFrame(gram_cfd)\n",
    "    gram_cfd_df = gram_cfd_df[~gramcfd_df.index_isin(stop)]\n",
    "    gram_cfd_df['instance_count'] = gram_cfd_df[0].fillna(0) + gram_cfd_df[1].fillna(0)\n",
    "    gram_cfd_df['instance_ratio'] = gram_cfd_df[1].fillna(0) / gram_cfd_df['instance_count'].fillna(0)\n",
    "    return gram_cfd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(itertools.chain(*sms_df.no_punct_tokens.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'until', 'jurong', 'point', 'crazy']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-f13e7dccb932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cond_samples)\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1866\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "nltk.ConditionalFreqDist(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "sent = \"the the the dog dog some other words that we do not care about\"\n",
    "cfdist = ConditionalFreqDist()\n",
    "for word in word_tokenize(sent):\n",
    "    condition = len(word)\n",
    "    cfdist[condition][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3, 'dog': 2, 'some': 1, 'other': 1, 'words': 1, 'that': 1, 'we': 1, 'do': 1, 'not': 1, 'care': 1, ...})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"the the the dog dog some other words that we do not care about\"\n",
    "nltk.FreqDist(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-77fe4851dfcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cond_samples)\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1866\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "nltk.ConditionalFreqDist(nltk.FreqDist(sent.split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
