{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567a2708-5096-47e8-8733-a99ab27e1ea3",
   "metadata": {},
   "source": [
    "# Write scripts that Pipeline uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1535232-a591-4c84-bf41-44a03aa5d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0580d778-d265-4498-b8d5-10cee27c1e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('param_config.yaml', 'r') as config_file:\n",
    "    config_params = yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094a00a5-981f-425f-950a-28b3b63cb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_source_dir = config_params['processor_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16fc224-69ba-41a0-908d-ae2090bfbee8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1a. Transformer Classes\n",
    "\n",
    "The classes used to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6af1755-3482-4e0d-9c1f-746dd0e912fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source_dir/transformers_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $preprocessor_source_dir/transformers_script.py\n",
    "\"\"\"\n",
    "Custom transformers to preprocess the data for training.\n",
    "\n",
    "    \n",
    "References\n",
    "-----------\n",
    "    See BaseEstimator and TransformerMixin for compatability with sklearn.\n",
    "    See MultiLabelBinarizer and OneHotEncoder for dependencies.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "\n",
    "class FeatureFiller(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Fills missing features with an empty value.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "        self._col_dict = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Captures the column names used at training and their data type.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        _col_dict : dict\n",
    "            Saves a dictionary to ``self`` that holds the names of the columns used at training as\n",
    "            the keys and their datatype as the value.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        This step ensures that if features that were used at training aren't in the inference data\n",
    "        that the model will still run. This is necessary because the data captured at inference\n",
    "        doesn't retain features that are empty.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._col_names = list(X.columns)\n",
    "        \n",
    "        # init_dict = X.dtypes.apply(lambda x: x.name).to_dict()\n",
    "        # self._col_dict = {}\n",
    "        # for col, dtype in init_dict.items():\n",
    "        #     self._col_dict.setdefault(dtype, []).append(col)\n",
    "        self._col_dict = X.dtypes.apply(lambda x: x.name).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Creates an empty data frame with the columns and data types\n",
    "        captured in fit from the training data.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed feature data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running FeatureFiller')\n",
    "        new_X = pd.DataFrame(columns=self._col_names)\n",
    "        new_X = new_X.astype(self._col_dict)\n",
    "\n",
    "        # for col_type in self._col_dict:\n",
    "        #     columns = self._col_dict[col_type]\n",
    "        #     new_X[columns] = new_X[columns].astype(col_type)\n",
    "        \n",
    "        reindex_X = X.reindex(columns=new_X.columns)\n",
    "        \n",
    "        new_df = pd.concat([new_X, reindex_X])\n",
    "        # print(\"Finished FeatureFiller\")\n",
    "        return new_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "\n",
    "class TrueFalseTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Ensures boolean values are represented uniformly as 0 and 1. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Captures column names used at training.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transforms data.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed feature data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running TrueFalseTransformer')\n",
    "        X = X.replace({'None':np.nan}).fillna('-1')\n",
    "        X = X.replace({'true':'1', 'false':'0'})\n",
    "        X = X.apply(pd.to_numeric, args=('coerce',))\n",
    "        # print(\"Finished TrueFalseTransformer\")\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Creates features from datetime values.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Captures column names used at training.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transforms data and sets the column names attribute.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running DateTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_datetime(X[col])\n",
    "            temp_df[f'{col}-month'] = X[col].dt.month.astype(float)\n",
    "            temp_df[f'{col}-day_of_week'] = X[col].dt.dayofweek.astype(float)\n",
    "            temp_df[f'{col}-hour'] = X[col].dt.hour.astype(float)\n",
    "            temp_df[f'{col}-day_of_month'] = X[col].dt.day.astype(float)\n",
    "            temp_df[f'{col}-is_month_start'] = X[col].dt.is_month_start.astype(int)\n",
    "            temp_df[f'{col}-is_month_end'] = X[col].dt.is_month_end.astype(int)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        temp_df = temp_df.fillna(-1)\n",
    "        # print(\"Finished DateTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "\n",
    "class FloatTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Ensures columns with strictly numeric values are all represented\n",
    "    as floats.\n",
    "    \n",
    "    Notes\n",
    "    -----   \n",
    "    Experimental results indicate than an integer feature (example: 1) will\n",
    "    be evaluated differently in a model than a float (example: 1.0).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Captures column names at training.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Sets the type as float and set null values to -1.0.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running FloatTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype != 'float':\n",
    "                X[col] = X[col].astype(float)\n",
    "        X = X.fillna(-1.0)\n",
    "        # print(\"Finished FloatTransformer\")\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "\n",
    "class ListMaxTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Evaluates a list value and returns the maximum value of the list as a single value.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Captures column names used at training.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Ensures uniform boolean handling, ensures all values are numeric,\n",
    "        finds maximum value, fills null values.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running ListMaxTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.replace({'true':'1', 'false':'0'}).fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).max()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        # print(\"Finished ListMaxTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "\n",
    "class ListNuniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Evaluates a list value and returns the count of unique items.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Captures column names used at training.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Counts unique items in a list for each observation.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running ListNuniqueTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col] = X[col].dropna().str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.groupby(temp_series.index).nunique()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        # print(\"Finished ListNuniqueTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "\n",
    "class DescStatTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Evaluates a list of numbers and returns the minimum value, maximum value,\n",
    "    mean, standard deviation, and count of unique items. Each of these is\n",
    "    assigned to its own column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Ensures numeric items in list; calculates the minimum, maximum, mean,\n",
    "        standard deviation, and count of unique items; fills null values.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running DescStatTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).agg(['min', 'max', 'mean', 'std', 'nunique'])\n",
    "            temp_series.columns = [f'{col}-{x}' for x in temp_series.columns]\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        # print(\"Finished DescStatTransformer\")\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "\n",
    "class OneHotTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    One hot encodes categorical columns that hold a single value per claim.\n",
    "    Each category is assigned its own column. Each claim will have a positive\n",
    "    value for a maximum of one column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Sets attributes for initial instantiation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        _filler : str\n",
    "            The value to use to fill missing values.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._filler = 'ml_empty'\n",
    "        self._col_names = None\n",
    "        self._encoder = None\n",
    "        self._transformer = None\n",
    "        self._transformed_feats = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fills null values, fits the encoder, and sets the encoded column names.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        _encoder : sklearn.preprocessing.OneHotEncoder\n",
    "            The initially encoder for one hot transformation. Uses sklearn class.\n",
    "        _transformer : sklearn.preprocessing.OneHotEncoder\n",
    "            The fitted one hot encoder.\n",
    "        _transformed_feats : list\n",
    "            The list of columns names after they have been one hot encoded. Used\n",
    "            for ``get_feature_names`` and ``get_feature_names_out``.\n",
    "        \n",
    "        See Also\n",
    "        -----------\n",
    "        sklearn.preprocessing.OneHotEncoder\n",
    "        \"\"\"\n",
    "        \n",
    "        self._col_names = X.dropna(axis=1, how='all').columns\n",
    "        X = X[self._col_names].fillna(self._filler)\n",
    "        self._encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self._transformer = self._encoder.fit(X)\n",
    "        self._transformed_feats = self._transformer.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transforms data.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            One hot encoded data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running OneHotTransformer')\n",
    "        X = X.replace({'None':np.nan}).fillna(self._filler)\n",
    "        X = self._transformer.transform(X[self._col_names])\n",
    "        # print(\"Finished OneHotTransformer\")\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return list(self._transformed_feats)\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return list(self._transformed_feats)\n",
    "\n",
    "class MultilabelTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    One hot encoding for categorical columns that can have multiple items\n",
    "    per claim. Each category is assigned its own column. Each claim can have\n",
    "    a positive value for more than one column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Sets attributes for initial instantiation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        _filler : str\n",
    "            The value to use to fill missing values.\n",
    "        \"\"\"\n",
    "        self._filler = 'ml_empty'\n",
    "        self._encoder = None\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Sets attributes for specific instantiation, fits multi label encoder,\n",
    "        and sets the column names to reflect original column name in the\n",
    "        category column name.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            The list of columns names after they have been encoded. Used\n",
    "            for ``get_feature_names`` and ``get_feature_names_out``.\n",
    "        _encoder : sklearn.preprocessing.MultiLabelBinarizer\n",
    "            The fitted encoder for multilabel transformation. Uses sklearn class.\n",
    "        \n",
    "        See also\n",
    "        -----------\n",
    "        sklearn.preprocessing.MultiLabelBinarizer\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        self._encoder = MultiLabelBinarizer()\n",
    "        self._encoder.fit(X)\n",
    "        self._col_names = [X.name + '_' + x for x in self._encoder.classes_]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Ensures unique list of items, transforms data.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running MultilabelTransformer')\n",
    "        X = X.replace({'None':np.nan})\n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        trans_array = self._encoder.transform(X)\n",
    "        df = pd.DataFrame(trans_array, columns=self._col_names, index=X.index)   \n",
    "        # print(\"Finished MultilabelTransformer\")     \n",
    "        return df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    \n",
    "class DropSingleValueCols(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns where all rows have the same value.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_index = []\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Iterates through columns, if the number of unique values in the column is\n",
    "        greater than 1, it retains the column, otherwise the column is dropped.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame | np.ndarray\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_index : list\n",
    "            Saves the column index values to ``self`` ensure consistent handling.\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        for i in range(len(X.columns)):\n",
    "            if X.iloc[:,i].nunique() > 1:\n",
    "                self._col_index.append(i)\n",
    "        self._col_names = list(X.iloc[:,self._col_index].columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Retains columns that had more than 1 unique value during fitting.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame | np.ndarray\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running DropSingleValueCols')\n",
    "        if type(X) == np.ndarray:\n",
    "            print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        X = X[self._col_names]\n",
    "        # X = X.iloc[:,self._col_index]\n",
    "        # print(\"Finished DropSingleValueCols\")\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "       \n",
    "class RemoveCollinearity(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Reduces columns that represent the same information in the dataset to\n",
    "    a single, representative column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._corr_dict = {}\n",
    "        self._drop_cols = set()\n",
    "        self._col_index = []\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Calculates correlations between columns. Compares correlated\n",
    "        columns and drops one if the correlation is greater than 0.97.\n",
    "        Sets the column name attribute for columns to keep.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _corr_dict : dictionary\n",
    "            Dictionary of correlated features.\n",
    "        _drop_cols : set\n",
    "            List of columns to drop due to correlations.    \n",
    "        _col_index : list\n",
    "            Column index values to ensure consistent handling.    \n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        drop_list = []\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        corr_df = X.corr()\n",
    "        for i, col in enumerate(corr_df.columns):\n",
    "            sliced_col = abs(corr_df.iloc[i+1:, i])\n",
    "            corr_feats = sliced_col[sliced_col > .97].index.tolist()\n",
    "            if len(corr_feats) > 0:\n",
    "                self._corr_dict[col] = corr_feats\n",
    "                drop_list += corr_feats\n",
    "        self._drop_cols = set(drop_list)\n",
    "        # print('Collinear feature drop list:', drop_list)\n",
    "        print('Number of collinear feature:', len(drop_list))\n",
    "        self._col_names = list(set(X.columns) - self._drop_cols)\n",
    "        for i, col in enumerate(X.columns):\n",
    "            if col in self._col_names:\n",
    "                self._col_index.append(i)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Retains columns uncorrelated columns.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame | np.ndarray\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running RemoveCollinearity')\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        X =  X[self._col_names]\n",
    "        # X =  X.iloc[:,self._col_index]\n",
    "        # print(\"Finished RemoveCollinearity\")\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    \n",
    "class SetColumnOrder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Ensures uniform column order.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    Column names for preprocessing are stored in a json object for\n",
    "    easy updating. When imported into Python as a dictionary, the\n",
    "    order of keys is not guaranteed, thus the order of the columns\n",
    "    must be standardized before being sent to the next step.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Sets attributes for specific instantiation.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame | np.ndarray\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        _col_names : list\n",
    "            Saves the list of columns used at training to ``self``.\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        self._col_names = list(set(X.columns))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Orders the columns in the dataframe.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : pd.DataFrame\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # print('Running SetColumnOrder')\n",
    "        if type(X) == np.ndarray:\n",
    "            # print('Processing numpy array')\n",
    "            X = pd.DataFrame(X)\n",
    "        # elif type(X) == pd.core.frame.DataFrame:\n",
    "        #     print('Processing pandas dataframe')\n",
    "        X =  X[self._col_names]\n",
    "        # X =  X.iloc[:,self._col_index]\n",
    "        # print(\"Finished SetColumnOrder\")\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"Returns column name attribute for specific instantiation.\"\"\"\n",
    "        \n",
    "        return self._col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa941090-f54b-4803-9732-479ed41d0a83",
   "metadata": {},
   "source": [
    "# 1b. Function Scripts/Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c474185-a811-4790-9221-ff26cc53c990",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source_dir/functions_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $preprocessor_source_dir/functions_module.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def pandas_reads(file_path, col_list = None):\n",
    "    \"\"\"\n",
    "    Reads all useful data file formats\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    file_path : str\n",
    "        The directory path and file name of the file to read.\n",
    "\n",
    "    col_list : list\n",
    "        Optional list of column names to load. This functionality\n",
    "        is only compatible for parquet, csv, and txt file formats.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc : dataframe\n",
    "        A pandas dataframe of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'parquet':\n",
    "        doc = pd.read_parquet(file_path, columns=col_list)\n",
    "    elif file_type == 'csv':\n",
    "        doc = pd.read_csv(file_path, usecols=col_list)\n",
    "    elif file_type == 'json':\n",
    "        with open(file_path, 'r') as f:\n",
    "            doc = json.loads(f.read())\n",
    "    elif file_type == 'txt':\n",
    "        doc = pd.read_csv(file_path, sep='\\t', usecols=col_list)\n",
    "    elif file_type == 'pickle':\n",
    "        doc = pd.read_pickle(file_path)\n",
    "    else:\n",
    "        print(\"\"\"Error: Accepted file types include parquet, csv, json, txt, and pickle.\n",
    "        \n",
    "        Use one of these file types or add to the pandas_reads function in functions_module.py\"\"\")\n",
    "    return doc\n",
    "\n",
    "def pandas_writes(df, file_path):\n",
    "    \"\"\"\n",
    "    Writes all useful data file formats\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df : dataframe\n",
    "        The dataframe to be written.\n",
    "\n",
    "    file_path : str\n",
    "        The directory path and file name of the file to write.\n",
    "    \"\"\"\n",
    "\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'parquet':\n",
    "        df.to_parquet(file_path)\n",
    "    elif file_type == 'csv':\n",
    "        df.to_csv(file_path)\n",
    "    elif file_type == 'json':\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(json.dumps(df.to_json(date_format='iso')))\n",
    "    elif file_type == 'txt':\n",
    "        df.to_csv(file_path, sep='\\t')\n",
    "    elif file_type == 'pickle':\n",
    "        df.to_pickle(file_path)\n",
    "    else:\n",
    "        print(\"\"\"Error: Accepted file type is dataframe only.\n",
    "        If you have a dictionary, turn it into a dataframe first.\n",
    "        Accepted write types include parquet, csv, json, txt, and pickle.\n",
    "        Use one of these file types or add to the pandas_writes\n",
    "         function in functions_module.py\"\"\")\n",
    "\n",
    "def get_file_list(input_path):\n",
    "    \"\"\"\n",
    "    Make a list of file paths for all files in a directory.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    input_path : str\n",
    "        The file path to the directory where the files are stored.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    file_list : list\n",
    "        A list of the file paths where each file path is a strings.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    This is a help function that allows for a dynamic number of files\n",
    "    to be found in a directory. This allows for changes in\n",
    "    underlying data and the tables or collections where they're stored\n",
    "    without having to change this script.\n",
    "    \"\"\"\n",
    "\n",
    "    file_types = ['parquet', 'csv', 'json', 'txt', 'pickle']\n",
    "    try:\n",
    "        file_list = [os.path.join(input_path, file) for file in os.listdir(input_path) if file.split('.')[-1] in file_types]\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"\"\"Error: Accepted file types include parquet, csv, json, txt, and pickle.\n",
    "        \n",
    "        Use one or more of these file types in the specified directory or add to the get_file_list function in functions_module.py\"\"\")\n",
    "    return file_list\n",
    "\n",
    "def read_in_files(input_path):\n",
    "    \"\"\"\n",
    "    Reads the all files in the directory specified.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    input_path : str\n",
    "        The file path to the directory where the files are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_dict : dictionary\n",
    "        A dictionary of dataframes where each file is a key and\n",
    "        the values in that file are a dataframe of those values.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The dictionary structure is used to accomodate the dynamic\n",
    "    number of files.\n",
    "    \"\"\"\n",
    "\n",
    "    df_dict = {}\n",
    "    file_list = get_file_list(input_path)\n",
    "    for file in file_list:\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        print(f'read_in_files: reading file {file}')\n",
    "        df_dict[file_name] = pandas_reads(file)\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92267f0a-be4f-4d60-8058-d496ab66bbe1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source_dir/model_functions_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $preprocessor_source_dir/model_functions_module.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "from transformers_script import FeatureFiller\n",
    "from transformers_script import DropSingleValueCols\n",
    "from transformers_script import RemoveCollinearity\n",
    "from transformers_script import SetColumnOrder\n",
    "from sklearn.metrics import(\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score)\n",
    "\n",
    "import skexplain\n",
    "import shap\n",
    "import alibi\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "\n",
    "def define_algo_pipe(algo, col_dict):\n",
    "    \"\"\"\n",
    "    Defines a sklearn pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    algo : algorithm\n",
    "        The algorithm to use for model training. Example: ``XGBClassifier``\n",
    "    col_dict : dict\n",
    "        The dictionary of mappings for feature to preprocessing method. This\n",
    "        tells the sklearn pipeline which custom Asurion transformer\n",
    "        (i.e. ``TrueFalseTransformer``) to apply to which column(s).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    algo_pipe : sklearn pipeline\n",
    "        The sklearn pipeline for the preprocessing and model training.\n",
    "        \n",
    "    Notes\n",
    "    -----    \n",
    "    This function is also in the ``tune_model.py`` file. As these functions\n",
    "    are maintained in different scripts, differences may occur.\n",
    "    \"\"\"\n",
    "    \n",
    "    transformer_list = []\n",
    "    tModule = importlib.import_module(\"transformers_script\")\n",
    "    key_list = [x for x in col_dict.keys() if not x == 'drop_cols']\n",
    "    for key in key_list:\n",
    "        trans_function = getattr(tModule, col_dict[key]['transformer'])\n",
    "        if key == 'list_to_labels':\n",
    "            for col in col_dict[key]['columns']:\n",
    "                step_name = key + '_' + col\n",
    "                transformer_list.append((step_name, trans_function(), col))\n",
    "        else:\n",
    "            transformer_list.append((key, trans_function(), col_dict[key]['columns']))\n",
    "    transformer_list.append((\"drop_cols\", 'drop', col_dict[\"drop_cols\"][\"columns\"]))\n",
    "    preprocessor = ColumnTransformer(transformer_list)\n",
    "    \n",
    "    all_preprocess = Pipeline([\n",
    "        ('featurefiller', FeatureFiller()),\n",
    "        ('preprocess', preprocessor),\n",
    "        ('dropsingle', DropSingleValueCols()),\n",
    "        ('removemulticollinear', RemoveCollinearity()),\n",
    "        ('setcolumorder', SetColumnOrder())])\n",
    "\n",
    "    algo_pipe = Pipeline([\n",
    "        ('all_preprocess', all_preprocess),\n",
    "        ('algorithm', algo)])\n",
    "    return algo_pipe\n",
    "\n",
    "def train_model(algo, algo_data, col_dict, Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Trains a model and returns the evaluation metrics.\n",
    "    \n",
    "    Parameters\n",
    "    -----------    \n",
    "    algo : algorithm\n",
    "        The algorithm to use for model training. Example: `XGBClassifier`\n",
    "    algo_data : dict\n",
    "        The dictionary holding algorithm information including the\n",
    "        package information, any changes to default settings, and\n",
    "        the parameters for hyperparameter tuning.\n",
    "    col_dict : dict\n",
    "        The dictionary of mappings for feature to preprocessing method. This\n",
    "        tells the sklearn pipeline which custom Asurion transformer\n",
    "        (i.e. ``TrueFalseTransformer``) to apply to which column(s).\n",
    "    Xtrain : pd.DataFrame\n",
    "        The training feature dataset.\n",
    "    ytrain : pd.DataFrame\n",
    "        The training ground truth data.\n",
    "    \n",
    "    Returns\n",
    "    --------    \n",
    "    metric_result_dict : dict\n",
    "        A dictionary of the evaluation metrics for the trained model.\n",
    "        \n",
    "    Notes\n",
    "    -----    \n",
    "    The trained model is not returned because this model isn't used going\n",
    "    forward. Training is done at this stage to collect information on\n",
    "    how well different algorithms perform on the training data. The best\n",
    "    performing algorithm is then trained and tuned in a different step\n",
    "    for further use.\n",
    "    \"\"\"\n",
    "    \n",
    "    metric_result_dict = {}\n",
    "    results = None\n",
    "    print(algo_data['class'])\n",
    "    try:\n",
    "        if 'defaults' in algo_data:\n",
    "            model = algo(**algo_data['defaults'])\n",
    "        else:\n",
    "            model = algo()\n",
    "        model_pipe = define_algo_pipe(model, col_dict)\n",
    "        results = cross_validate(model_pipe, Xtrain, ytrain, cv=5, scoring=['accuracy', 'precision', 'f1', 'recall', 'roc_auc'])\n",
    "        for result in results.keys():\n",
    "            results[result] = results[result].tolist()\n",
    "        metric_result_dict[algo_data['class']] = results\n",
    "    except Exception as error:\n",
    "        print(\"Error on\", algo_data['class'])\n",
    "        print(\"Error:\", error)\n",
    "    print('\\n')\n",
    "    print(algo_data['class'], \"Result Dictionary:\")\n",
    "    print(results)\n",
    "    return metric_result_dict\n",
    "\n",
    "def compile_algo_metrics(metric_dict):\n",
    "    \"\"\"\n",
    "    Compiles cross validation metrics from the baseline models\n",
    "    and calculates several aggregated metrics.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    metric_dict : dict\n",
    "        The dictionary of evaluation metrics. Each algorithm\n",
    "        (i.e. ``XGBoostClassifier``constitutes one row with the\n",
    "        metrics (i.e. ``f1``) making up the columns.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    metric_desc_stat_df : dataframe\n",
    "        A dataframe with the metrics and calculated aggreated/averaged\n",
    "        results for all trained algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    metric_result_df = pd.DataFrame(metric_dict).T\n",
    "    metric_desc_stat_df = pd.DataFrame()\n",
    "    for col in metric_result_df.columns:\n",
    "        if 'time' in col:\n",
    "            continue\n",
    "        else:\n",
    "            mean_col = col + '_mean'\n",
    "            stdv_col = col + '_stdev'\n",
    "            penalized = col + '_penalized'\n",
    "            metric_desc_stat_df[mean_col] = metric_result_df[col].apply(np.mean)\n",
    "            metric_desc_stat_df[stdv_col] = metric_result_df[col].apply(np.std)\n",
    "            metric_desc_stat_df[penalized] = metric_desc_stat_df[mean_col] * (1 - metric_desc_stat_df[stdv_col])\n",
    "    metric_desc_stat_df = metric_desc_stat_df.sort_values([\n",
    "        'test_f1_penalized',\n",
    "        'test_roc_auc_penalized',\n",
    "        'test_recall_penalized',\n",
    "        'test_precision_penalized',\n",
    "        'test_accuracy_penalized'], ascending=False)\n",
    "    return metric_desc_stat_df\n",
    "\n",
    "def tune_best_algo(algo, algo_data, col_dict, Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Trains and tunes a model.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    algo : algorithm\n",
    "        The algorithm to use for model training. Example: ``XGBClassifier``    \n",
    "    algo_data : dict\n",
    "        The dictionary holding algorithm information including the\n",
    "        package information, any changes to default settings, and\n",
    "        the parameters for hyperparameter tuning.\n",
    "    col_dict : dict\n",
    "        The dictionary of mappings for feature to preprocessing method. This\n",
    "        tells the sklearn pipeline which custom Asurion transformer\n",
    "        (i.e. ``TrueFalseTransformer``) to apply to which column(s).\n",
    "    Xtrain : dataframe\n",
    "        The training feature dataset.\n",
    "    ytrain : dataframe\n",
    "        The training ground truth data.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    model : trained model\n",
    "        The trained model that has been tuned using ``RandomizedSearchCV``.\n",
    "        \n",
    "    See Also\n",
    "    --------\n",
    "    sklearn.mode_selection.RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    \n",
    "    algo_pipe = define_algo_pipe(algo(), col_dict)\n",
    "    param_grid = dict(('algorithm__'+key, value) for (key, value) in algo_data['params'].items())\n",
    "    search_params = {\"estimator\": algo_pipe,\n",
    "                     \"cv\": 5,\n",
    "                     \"param_distributions\": param_grid,\n",
    "                     \"scoring\": {'f1':'f1',\n",
    "                                 'auc':'roc_auc'},\n",
    "                     \"verbose\": 5,\n",
    "                     \"refit\": \"f1\",\n",
    "                     \"random_state\": 12}\n",
    "    print(\"Tuning for hyperparameters\")\n",
    "    tuner = RandomizedSearchCV(**search_params)\n",
    "    print(\"Training model\")\n",
    "    tuner.fit(Xtrain, ytrain)\n",
    "    model = tuner.best_estimator_\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def metrics_row(y_test, model, testX):\n",
    "    \"\"\"\n",
    "    Evalute trained model using the metrics accuracy, precision, AUC,\n",
    "    f1, recall, and the confusion matrix.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    y_test : pd.DataFrame\n",
    "        The validation ground truth values.\n",
    "    model : trained model\n",
    "        The model trained on the training dataset.\n",
    "    testX : pd.DataFrame\n",
    "        The validation feature dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result_dict : dict\n",
    "        A dictionary of the evaluation metrics rounded to 3 decimal places\n",
    "        including accuracy, precision, f1, recall, ROC AUC, and an\n",
    "        annotated confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = model.predict(testX)\n",
    "    proba_predictions = model.predict_proba(testX)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    \n",
    "    conf_matrix_norm = np.round(conf_matrix / conf_matrix.sum(axis=1), 3)\n",
    "    \n",
    "    annotation_list = []\n",
    "    lbls = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "    ct = 0\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tmp = []\n",
    "        for x in range(conf_matrix[i].shape[0]):\n",
    "            val = f\"{lbls[ct]}\\n\\nCount: {conf_matrix[i][x]}\\nActl Rate: {conf_matrix_norm[i][x]}\"\n",
    "            tmp.append(val)\n",
    "            ct += 1\n",
    "        annotation_list.append(tmp)\n",
    "        \n",
    "    result_dict = {\"Accuracy\": round(accuracy, 3),\n",
    "                   \"Precision\": round(precision, 3),\n",
    "                   \"F1\": round(f1, 3),\n",
    "                   \"Recall\": round(recall, 3),\n",
    "                   \"ROC AUC\": round(roc_auc, 3),\n",
    "                   \"conf_matrix\": annotation_list}\n",
    "    return result_dict\n",
    "\n",
    "# def eval_report(y_test, model, testX):\n",
    "def eval_report(y_test, preprocessor, model, testX):\n",
    "    \"\"\"\n",
    "    Evaluates the trained and tuned model against the hold out/test\n",
    "    data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    y_test : dataframe\n",
    "        The test dataset ground truth values.\n",
    "    preprocessor : sklearn pipeline\n",
    "        The preprocessor steps from the sklearn pipeline with the\n",
    "        named step ``all_preprocess``.\n",
    "    model : sklearn pipeline\n",
    "        The algorithm step from the sklearn pipeline with the named\n",
    "        step ``algorithm``.\n",
    "    testX : dataframe\n",
    "        The test dataset features.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    report_dict : dict\n",
    "        A dictionary of the evaluation metrics. These are specifically\n",
    "        formated to be compatible with the Model Registry's Model\n",
    "        Quality page.\n",
    "    \"\"\"\n",
    "\n",
    "    Xprocessed = preprocessor.transform(testX)\n",
    "    predictions = model.predict(Xprocessed)\n",
    "    # predictions = model.predict(testX)\n",
    "    # proba_predictions = model.predict_proba(testX)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    pr_curve = precision_recall_curve(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    # avg_precision = average_precision_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "        \n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\"value\":accuracy, \"standard_deviation\":\"NaN\"},\n",
    "            \"precision\": {\"value\":precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"f1\": {\"value\":f1, \"standard_deviation\":\"NaN\"},\n",
    "            \"recall\": {\"value\":recall, \"standard_deviation\":\"NaN\"},\n",
    "            # \"ROC AUC\": {\"value\":auc_score, \"standard_deviation\":\"NaN\"},\n",
    "            # \"avg_percision\": {\"value\":avg_precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"roc_auc\": {\"value\":roc_auc, \"standard_deviation\":\"NaN\"},\n",
    "            # \"log_loss\": {\"value\":log_loss_score, \"standard_deviation\":\"NaN\"},\n",
    "            # \"informedness\": {\"value\":informedness, \"standard_deviation\":\"NaN\"},\n",
    "            # \"cohen_kappa\": {\"value\":cohen_kappa, \"standard_deviation\":\"NaN\"},\n",
    "            # \"mathews_coef\": {\"value\":matthews_coef, \"standard_deviation\":\"NaN\"},\n",
    "            # \"fbeta\": {\"value\":fbeta, \"standard_deviation\":\"NaN\"},,\n",
    "            # \"roc\": {\n",
    "            #     \"fpr\": roc[0].tolist(),\n",
    "            #     \"tpr\": roc[1].tolist(),\n",
    "            #     \"thresholds\": roc[2].tolist()}\n",
    "            # \"pr_curve\": {\"precision\": pr_curve[0].tolist(),\n",
    "            #              \"recall\": pr_curve[1].tolist(),\n",
    "            #              \"thresholds\": pr_curve[2].tolist()},\n",
    "            \"confusion_matrix\": {\"0\": {\"0\": int(conf_matrix[0][0]), \"1\": int(conf_matrix[0][1])},\n",
    "                                 \"1\": {\"0\": int(conf_matrix[1][0]), \"1\": int(conf_matrix[1][1])}\n",
    "                                },\n",
    "            # \"receiver_operating_charastic_curve\": {\n",
    "            #     \"false_positive_rates\": list(fpr),\n",
    "            #     \"true_positive_rates\": list(tpr)\n",
    "            # }\n",
    "        }\n",
    "    }\n",
    "    return report_dict\n",
    "\n",
    "def explainability_mods(model, Xtrain):\n",
    "    \"\"\"\n",
    "    Trains a shap and an anchor explainer on preprocessed training data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pipeline\n",
    "        Pipeline that has been trained on the training data and includes\n",
    "        two final named steps ``all_preprocess`` and ``algorithm``.\n",
    "    Xtrain : pd.DataFrame\n",
    "        The training dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    shap_explainer : trained model\n",
    "        A fitted shap explainer model.\n",
    "    anchor_explainer : trained model\n",
    "        A fitted anchor explainer model.\n",
    "        \n",
    "    See Also\n",
    "    ---------\n",
    "    shap documentation\n",
    "    alibi documentation\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess_pipe = model.named_steps['all_preprocess']\n",
    "    train_processed = preprocess_pipe.transform(Xtrain)\n",
    "    just_model = model.named_steps['algorithm']\n",
    "    \n",
    "    shap_mask = shap.maskers.Partition(train_processed,\n",
    "                                       max_samples=1000)\n",
    "    shap_explainer = shap.Explainer(just_model.predict,\n",
    "                                    shap_mask,\n",
    "                                    algorithm = \"permutation\")\n",
    "    \n",
    "    predict_fn = lambda x: just_model.predict(x)\n",
    "    anchor_explainer = alibi.explainers.AnchorTabular(predict_fn, train_processed.columns, seed=1)\n",
    "    anchor_explainer.fit(train_processed.to_numpy())\n",
    "    \n",
    "    return shap_explainer, anchor_explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c0695-81d6-4052-9d3d-1b95b1d12603",
   "metadata": {},
   "source": [
    "# 2a. Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e80a6b-d1bd-4a63-b9af-21f7e36d5778",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_data.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functions_module import pandas_writes\n",
    "import argparse\n",
    "\n",
    "def calc_wts(col, predictability):\n",
    "    low_wt = 1 - predictability\n",
    "    col_len = len(col)\n",
    "    if predictability > 0:\n",
    "        wts = [predictability] + [(low_wt/(col_len-1)) for x in range(col_len-1)]\n",
    "    elif predictability == 0:\n",
    "        wts = [(low_wt/(col_len)) for x in range(col_len)]\n",
    "    else:\n",
    "        print('''High weight must be between 0 or greater and less than 1.\n",
    "              A value of 0 gives equal weight to all values\n",
    "              A value greater than 0 and less than one gives that weight to the first value and equal weights to all other values''')\n",
    "    xwts = list(reversed(wts))\n",
    "    return wts, xwts\n",
    "\n",
    "def generate_dates(df, target_name, year=2022):\n",
    "    true_vals = []\n",
    "    false_vals = []\n",
    "    month_wts, month_xwts = calc_wts(range(1, 12), 0)\n",
    "    day_wts, day_xwts = calc_wts(range(1, 31), 0.8)    \n",
    "    \n",
    "    t_year = [year for x in range(len(df[df[target_name]==1]))]\n",
    "    t_month = random.choices(range(1, 12), month_wts, k=len(df[df[target_name]==1]))\n",
    "    t_day = random.choices(range(1, 31), day_wts, k=len(df[df[target_name]==1]))\n",
    "    true_collist = zip(t_year,\n",
    "                       t_month,\n",
    "                       t_day)\n",
    "    for yr, mt, dy in true_collist:\n",
    "        try:\n",
    "            date = datetime.date(yr, mt, dy)\n",
    "            true_vals.append(date)\n",
    "        except ValueError:\n",
    "            true_vals.append(np.nan)\n",
    "            \n",
    "    f_year = [year for x in range(len(df[df[target_name]==0]))]\n",
    "    f_month = random.choices(range(1, 12), month_xwts, k=len(df[df[target_name]==0]))\n",
    "    f_day = random.choices(range(1, 31), day_xwts, k=len(df[df[target_name]==0]))\n",
    "    false_collist = zip(f_year,\n",
    "                        f_month,\n",
    "                        f_day)\n",
    "    for yr, mt, dy in false_collist:\n",
    "        try:\n",
    "            date = datetime.date(yr, mt, dy)\n",
    "            false_vals.append(date)\n",
    "        except ValueError:\n",
    "            false_vals.append(np.nan)\n",
    "    \n",
    "    return true_vals, false_vals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--target-name', type=str, dest='target_name')\n",
    "    parser.add_argument('--target-rate', type=str, dest='target_rate')\n",
    "    parser.add_argument('--sample-size', type=str, dest='sample_size')\n",
    "    parser.add_argument('--predictability', type=str, dest='predictability')\n",
    "    parser.add_argument('--train-size', type=str, dest='train_size')\n",
    "    parser.add_argument('--input-path', type=str, dest='input_path')\n",
    "    parser.add_argument('--output-path', type=str, dest='output_path')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    target_name = args.target_name\n",
    "    target_rate = args.target_rate\n",
    "    sample_size = args.sample_size\n",
    "    predictability = args.predictability\n",
    "    train_size = args.train_size\n",
    "    input_path = args.input_path\n",
    "    output_path = args.output_path\n",
    "\n",
    "    target_wts = [1-float(target_rate), float(target_rate)]\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, \"test\"), exist_ok=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # create ground truth\n",
    "    target_col = []\n",
    "    target_vals = [0,1]\n",
    "\n",
    "\n",
    "    col_list = zip([target_col],\n",
    "                [target_vals],\n",
    "                [target_wts])\n",
    "\n",
    "    for col, vals, wts in col_list:\n",
    "    target_col = random.choices(vals, wts, k=sample_size)\n",
    "\n",
    "    gt_df = pd.DataFrame({\n",
    "    target_name:target_col})\n",
    "    print(f\"Ground truth provided sample size: {sample_size}\")\n",
    "    print(f'Ground truth dataframe shape: {gt_df.shape}')\n",
    "    print(f'Ground truth distribution:\\n{gt_df[target_name].value_counts()}')\n",
    "    # gt_df.to_parquet(os.path.join(config_params['project_dir'], config_params['data_dir'], 'ground_truth.parquet'), index=False)\n",
    "\n",
    "    # Create Features\n",
    "    sample_df = gt_df.copy()\n",
    "\n",
    "    # All features include null, NaN, or None values to simulate missing data.\n",
    "\n",
    "    # single value observations\n",
    "    # Examples include 0, 1, 5, 9, or 0.85\n",
    "    # These simulate numeric features, categories, and booleans stored as numbers\n",
    "    col_names = ['true_false', 'one_hot', 'floats', 'random_col', 'other']\n",
    "\n",
    "    tf_vals = ['true', 'false', np.nan, '1', '0']\n",
    "    onehot_vals = ['red', 'orange', 'yellow', np.nan, 'green', 'blue', 'purple']\n",
    "    float_vals = list(range(0,10)) + [np.nan] + [x/10 for x in range(0, 100, 5)]\n",
    "    drop_vals = [np.nan] + list(range(0,10))\n",
    "    xrand_vals = list(range(5))\n",
    "\n",
    "    tf_high = predictability/2\n",
    "    tf_low = (1 - predictability - 0.01)/2\n",
    "    tf_wts = [tf_high, tf_low, 0.01, tf_high, tf_low]\n",
    "    tf_xwts = [tf_low, tf_high, 0.01, tf_low, tf_high]\n",
    "    onehot_wts, onehot_xwts = calc_wts(onehot_vals, predictability)\n",
    "    float_wts, float_xwts = calc_wts(float_vals, predictability)\n",
    "    drop_wts, drop_xwts = calc_wts(drop_vals, 0)\n",
    "    xrand_wts, xrand_xwts = calc_wts(xrand_vals, predictability)\n",
    "\n",
    "    col_list = zip(col_names,\n",
    "                [tf_vals, onehot_vals, float_vals, drop_vals, xrand_vals],\n",
    "                [tf_wts, onehot_wts, float_wts, drop_wts, xrand_wts],\n",
    "                [tf_xwts, onehot_xwts, float_xwts, drop_xwts, xrand_xwts])\n",
    "\n",
    "    for col, vals, col_wts, col_xwts in col_list:\n",
    "    true_vals = random.choices(vals, col_wts, k=len(sample_df[sample_df[target_name]==1]))\n",
    "    false_vals = random.choices(vals, col_xwts, k=len(sample_df[sample_df[target_name]==0]))\n",
    "    sample_df.loc[sample_df[target_name]==1, col] = true_vals\n",
    "    sample_df.loc[sample_df[target_name]==0, col] = false_vals\n",
    "\n",
    "    # date observations\n",
    "    # These simulate date features in the YYYY-MM-DD (2022-12-15) date format.\n",
    "    true_dates, false_dates = generate_dates(sample_df, target_name)\n",
    "    sample_df.loc[sample_df[target_name]==1, 'dates'] = true_dates\n",
    "    sample_df.loc[sample_df[target_name]==0, 'dates'] = false_dates\n",
    "\n",
    "    # multivalue observations\n",
    "    # These simulate list type features such as [red, blue, purple], [1, 2, 3, 4]\n",
    "    nbr_vals = list(range(0,10))\n",
    "    str_vals = ['apple', 'orange', 'grape', 'pineapple', 'strawberry', 'blueberry', 'grapefruit', 'apple']\n",
    "\n",
    "    nunique_col = []\n",
    "\n",
    "    for _ in range(sample_size):\n",
    "    val_size = random.randint(0,6)\n",
    "    if val_size < 1:\n",
    "        nunique_col.append(np.nan)\n",
    "    else:\n",
    "        if random.randint(0,10) < 5:\n",
    "            val_type = str_vals\n",
    "        else:\n",
    "            val_type = [str(x) for x in nbr_vals]\n",
    "        val = random.choices(val_type,k=val_size)\n",
    "        strified = ','.join(val)\n",
    "        nunique_col.append(strified)\n",
    "\n",
    "    descstat_col = []\n",
    "    max_col = []\n",
    "\n",
    "    nbrlst_cols = [descstat_col, max_col]\n",
    "\n",
    "    for col in nbrlst_cols:\n",
    "    for _ in range(sample_size):\n",
    "        val_size = random.randint(0,6)\n",
    "        if val_size < 1:\n",
    "            col.append(np.nan)\n",
    "        else:\n",
    "            val_type = [str(x) for x in nbr_vals]\n",
    "            val = random.choices(val_type,k=val_size)\n",
    "            strified = ','.join(val)\n",
    "            col.append(strified)\n",
    "\n",
    "    multi_col = []\n",
    "\n",
    "    for _ in range(sample_size):\n",
    "    val_size = random.randint(0,6)\n",
    "    if val_size < 1:\n",
    "        multi_col.append(np.nan)\n",
    "    else:\n",
    "        val = random.choices(str_vals, k=val_size)\n",
    "        strified = ','.join(val)\n",
    "        multi_col.append(strified)\n",
    "\n",
    "    # Add to dataframe\n",
    "    sample_df['max_of_list'] = max_col\n",
    "    sample_df['nunique_of_list'] = nunique_col\n",
    "    sample_df['desc_stats'] = descstat_col\n",
    "    sample_df['multi_label'] = multi_col\n",
    "\n",
    "    print(f\"Dataset provided sample size: {sample_size}\")\n",
    "    print(f'Full dataframe shape: {sample_df.shape}')\n",
    "\n",
    "    train, other = train_test_split(sample_df, train_size=train_size, random_state=12, stratify=sample_df[target_name])\n",
    "    test, validate = train_test_split(other, train_size=0.5, random_state=12, stratify=other[target_name])\n",
    "\n",
    "    pandas_writes(train, os.path.join(output_path, 'train', 'train.parquet'), index=False)\n",
    "    pandas_writes(validate, os.path.join(output_path, 'validate', 'validate.parquet'), index=False)\n",
    "    pandas_writes(test_data, os.path.join(output_path, 'test', 'test.json'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d20b8d-156e-4f05-b2b1-78e293277e9f",
   "metadata": {},
   "source": [
    "# 3a. Train Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0d9803-9414-4076-bd66-3fcd7511c828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py\n",
    "\"\"\"\n",
    "Trains an algorithm using specified defaults.\n",
    "\n",
    "**Input**:\n",
    "    col_dict : json\n",
    "        A json object that comprised of metadata for column transformations.\n",
    "        A human readable column type is used as the key. The value for each\n",
    "        key includes the following two pieces of metadata:\n",
    "        * transformer: the custom transformer to apply to applicable columns.\n",
    "        * columns: the column names to be processed with the transformer. Each\n",
    "        column should only be included in one column type.\n",
    "    algo_data : json\n",
    "        The metadata for the algorithm to run. Information includes:\n",
    "        * 'module': the module to import for the algorithm.\n",
    "        * 'class': the specific name of the algorithm.\n",
    "        * 'params': the parameters to use for fine tuning. These values aren't used\n",
    "        in this step, but are required in the tuning step.\n",
    "        * 'defaults': optional parameter used when alternate values are desired for the \n",
    "        default training parameters.                                    \n",
    "    train : parquet\n",
    "        The training dataset only, excludes data set aside for validation and testing.\n",
    "        \n",
    "**Arguments**:\n",
    "    --target : str\n",
    "        The name of the event to be used. `--target` name is case insensitive.\n",
    "    --train-size : number\n",
    "        The size for the training/test split.\n",
    "        Can be expressed either as a ratio (floating point between 0 and 1)\n",
    "        or as a percentage (whole number between 0 and 100).\n",
    "    --algo-name : str\n",
    "        The key from the algo_dict. This is dynamically populated from\n",
    "        the Sagemaker Pipeline from the algo_dict.json file.\n",
    "    \n",
    "**Output**:\n",
    "    algo_result_dict : json\n",
    "        The evaluation metrics for the trained model. This is used to\n",
    "        determine which algorithm is the best candidate for\n",
    "        hyperparameter tuning.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "    The algorithm, algo package, and any defaults are specified in\n",
    "    the Sagemaker Pipeline. Algorithm package is loaded dynamically\n",
    "    so the same script can be used for multiple algorithms while\n",
    "    loading a minimal number of packages. All relevant json files\n",
    "    are stored in the ``preprocessing_source_dir`` folder and loaded\n",
    "    to the EC2 upon startup.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    import json\n",
    "    import importlib\n",
    "    import os\n",
    "    import warnings\n",
    "    import argparse    \n",
    "    from model_functions_module import define_algo_pipe, train_model\n",
    "    from functions_module import pandas_reads, pandas_writes\n",
    "\n",
    "    from sklearn import set_config\n",
    "    set_config(transform_output=\"pandas\")\n",
    "    \n",
    "    print(\"numpy version:\", np.__version__)\n",
    "    print(\"pandas version:\", pd.__version__)\n",
    "    print(\"sklearn version:\", sklearn.__version__)\n",
    "    import platform\n",
    "    print(\"python version:\", platform.python_version())\n",
    "\n",
    "    warnings.simplefilter(\"once\")\n",
    "    # warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--target', type=str, dest='target_col')\n",
    "    parser.add_argument('--train-size', type=str, dest='train_size', default='0.8')\n",
    "    parser.add_argument('--algo-name', type=str, dest='algo_name')\n",
    "    parser.add_argument('--input-path', type=str, dest='input_path')\n",
    "    parser.add_argument('--output-path', type=str, dest='output_path')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    target_col = args.target_col\n",
    "    train_size = args.train_size\n",
    "    algo_name = args.algo_name\n",
    "    input_path = args.input_path\n",
    "    output_path = args.output_path\n",
    "    \n",
    "    print('Args: {}'.format(args))\n",
    "    \n",
    "    # ecr_basepath = \"/opt/app\"\n",
    "    # col_dict = pandas_reads(os.path.join(ecr_basepath, \"col_dict.json\"))\n",
    "    # algo_data = pandas_reads(os.path.join(ecr_basepath, \"algo_dict.json\"))[algo_name]\"\n",
    "    col_dict = pandas_reads(\"col_dict.json\")\n",
    "    algo_data = pandas_reads(\"algo_dict.json\")[algo_name]\n",
    "        \n",
    "        \n",
    "    module = importlib.import_module(algo_data['module'])\n",
    "    algo = getattr(module, algo_data['class'])\n",
    "\n",
    "    train_size = float(train_size)\n",
    "    if train_size > 1:\n",
    "        train_size = train_size/100\n",
    "      \n",
    "    train_filepath = os.path.join(input_path, 'train.parquet')\n",
    "\n",
    "    print('Loading data')\n",
    "    train_df = pandas_reads(train_filepath)\n",
    "    print(f'Train data size: {train_df.shape}')\n",
    "    \n",
    "    Xtrain = train_df.drop(target_col, axis=1)\n",
    "    ytrain = train_df[[target_col]]\n",
    "\n",
    "    print('Training algorithm with defaults')\n",
    "    algo_result_dict = pd.DataFrame(train_model(algo, algo_data, col_dict, Xtrain, ytrain))\n",
    "    \n",
    "    print('Saving metrics')\n",
    "    file_name = algo_name + '_result_dict.json'\n",
    "    pandas_writes(algo_result_dict, os.path.join(output_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f69da3-03bd-4bc0-ae67-3e0157d8d43f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3b. Evaluate Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357f0888-f7f7-49a4-80a3-a5637c10d9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing eval_basemodels.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eval_basemodels.py\n",
    "\"\"\"\n",
    "Compiles the evaluation results from all trained baseline models and\n",
    "calculates some aggregated metrics.\n",
    "\n",
    "**Input**:\n",
    "    input_files : json\n",
    "        Intakes json files holding the evaluation metrics from the trained\n",
    "        baseline models. These are automatically updated in the Sagemaker\n",
    "        Pipeline based on the alogrithm steps.\n",
    "\n",
    "**Output**:\n",
    "    agg_metrics_df : parquet\n",
    "        A parquet file holding dataframe of the compiled metrics.\n",
    "        \n",
    "Notes\n",
    "------    \n",
    "    The results for all algorithms are retained and saved to s3 in this\n",
    "    step to retain training provenance for reference and audit purposes.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import os\n",
    "    import argparse\n",
    "    from functions_module import pandas_reads, pandas_writes, get_file_list, read_in_files\n",
    "    from model_functions_module import compile_algo_metrics\n",
    "\n",
    "    print(\"numpy version:\", np.__version__)\n",
    "    print(\"pandas version:\", pd.__version__)\n",
    "    import platform\n",
    "    print(\"python version:\", platform.python_version())\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, dest='input_path')\n",
    "    parser.add_argument('--output-path', type=str, dest='output_path')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    input_path = args.input_path\n",
    "    output_path = args.output_path\n",
    "    \n",
    "    print('creating metrics dataframe')\n",
    "    init_dict = read_in_files(input_path)\n",
    "    metrics_dict = {}\n",
    "    for x in init_dict.values():\n",
    "        x = json.loads(x)\n",
    "        for k, v in x.items():\n",
    "            metrics_dict[k] = v\n",
    "    agg_metrics_df = compile_algo_metrics(metrics_dict)\n",
    "    \n",
    "    pandas_writes(agg_metrics_df, os.path.join(output_path, 'basemodel_metrics.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e7ffe5-e8bc-4733-8ef1-e5eb20840663",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Tune Best Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ee6990b-86d3-491c-8bcf-ae32ddf7c235",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tune_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tune_model.py\n",
    "\"\"\"\n",
    "Trains and tunes a final model for deployment. Includes functions that\n",
    "are used at prediction and are specified by the Sagemaker Pipeline\n",
    "enviornment/ecosystem.\n",
    "\n",
    "Deployed model should be able to handle both single observations\n",
    "as well as batches of observations.\n",
    "\n",
    "**Input**:\n",
    "    basemodel_metrics : parquet\n",
    "        The evaluation metrics for the baseline models that was compiled\n",
    "        in the prior Sagemaker Pipeline step and autopopulates into the\n",
    "        pipeline step that corresponds with this script.\n",
    "    col_dict : json\n",
    "        A json object that comprised of metadata for column transformations.\n",
    "        A human readable column type is used as the key. The value for each\n",
    "        key includes the following two pieces of metadata:\n",
    "        * transformer: the custom transformer to apply to applicable columns.\n",
    "        * columns: the column names to be processed with the transformer. Each\n",
    "        column should only be included in one column type.\n",
    "    algo_data : json\n",
    "        The metadata for the algorithm to run. Information includes:\n",
    "        * 'module': the module to import for the algorithm.\n",
    "        * 'class': the specific name of the algorithm.\n",
    "        * 'params': the parameters to use for fine tuning. These values aren't used\n",
    "        in this step, but are required in the tuning step.\n",
    "        * 'defaults': optional parameter used when alternate values are desired for the \n",
    "        default training parameters.                                    \n",
    "    train : parquet\n",
    "        The training dataset only, excludes data set aside for validation and testing.\n",
    "    validate : parquet\n",
    "        The validation dataset used to evaluate the model during hyperparameter tuning.\n",
    "    required_cols : json\n",
    "        The columns or tables required in order to make a prediction. This is currently a\n",
    "        hardcoded file; todo: dynamically populate this based on features used by the\n",
    "        trained and tuned model.        \n",
    "\n",
    "**Arguments**:\n",
    "    --target : str\n",
    "            The name of the event to be used. `--target` name is case insensitive.\n",
    "    --train-size : number\n",
    "        The size for the training/test split.\n",
    "        Can be expressed either as a ratio (floating point between 0 and 1)\n",
    "        or as a percentage (whole number between 0 and 100).\n",
    "    --model_dir : str\n",
    "        The directory where model artifacts are stored.\n",
    "    --output-data-dir : str\n",
    "        The directory where other artifacts, such as the required features,\n",
    "        are stored.\n",
    "    --train-data-dir : str\n",
    "        The directory where the training data is placed by the Sagemaker\n",
    "        Pipeline.\n",
    "    --validate-data-dir : str\n",
    "        The directory where the validation data is placed by the Sagemaker\n",
    "        Pipeline.\n",
    "    --metric-data-dir : str\n",
    "        The directory where the consolidated evaluation data from the\n",
    "        previous step is placed by the Sagemaker Pipeline.\n",
    "\n",
    "**Output**:\n",
    "    processor_steps : joblib\n",
    "        The portion of the sklearn pipeline that completes all processing steps.\n",
    "    model_step : joblib\n",
    "        The portion of the sklearn pipeline that makes a prediction.\n",
    "    shap_explainer : joblib\n",
    "        The fitted shap explainer.\n",
    "    anchor_explainer : dill\n",
    "        Returns a folder that includes two dill files, one that is the explainer and one\n",
    "        that holds metadata.\n",
    "    required_cols : json\n",
    "        The columns or tables required in order to make a prediction. This is currently a\n",
    "        hardcoded file; todo: dynamically populate this based on features used by the\n",
    "        trained and tuned model.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "    Functions required for the model to make predictions at inference are\n",
    "    ``input_fn``, ``output_fn``, ``predict_fn``, and ``model_fn``.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import argparse\n",
    "import importlib\n",
    "from io import StringIO\n",
    "from sagemaker_containers.beta.framework import encoders, worker\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# import skexplain\n",
    "# import shap\n",
    "# import alibi\n",
    "from functions_module import pandas_reads, pandas_writes\n",
    "from model_functions_module import define_algo_pipe, tune_best_algo, metrics_row\n",
    "# from model_functions_module import explainability_mods\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "import platform\n",
    "print(\"python version:\", platform.python_version())\n",
    "\n",
    "warnings.simplefilter(\"once\")\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "input_path = '/opt/ml/processing/input'\n",
    "output_path = '/opt/ml/processing/output'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--target', type=str, dest='target_col')\n",
    "    parser.add_argument('--train-size', type=str, dest='train_size', default='0.8')\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--train-data-dir', type=str, default=os.environ.get('SM_CHANNEL_TRAIN_DATA'))\n",
    "    parser.add_argument('--validate-data-dir', type=str, default=os.environ.get('SM_CHANNEL_VALIDATE_DATA'))\n",
    "    parser.add_argument('--metric-data-dir', type=str, default=os.environ.get('SM_CHANNEL_METRIC_DATA'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('Args: {}'.format(args))\n",
    "        \n",
    "    train_size = float(args.train_size)\n",
    "    if train_size > 1:\n",
    "        train_size = train_size/100\n",
    "      \n",
    "    metric_desc_stat_df = pandas_reads(os.path.join(args.metric_data_dir,'basemodel_metrics.parquet'))\n",
    "    best_algo = metric_desc_stat_df['test_f1_penalized'].idxmax()\n",
    "    best_results = metric_desc_stat_df.loc[best_algo, ['test_f1_mean',\n",
    "                                              'test_roc_auc_mean',\n",
    "                                              'test_recall_mean',\n",
    "                                              'test_precision_mean',\n",
    "                                              'test_accuracy_mean']]\n",
    "    print('Best model is:', best_algo)\n",
    "    print(\"Model's performance was:\")\n",
    "    print(best_results)\n",
    "        \n",
    "    \n",
    "    # ecr_basepath = \"/opt/app\"\n",
    "    # col_dict = pandas_reads(os.path.join(ecr_basepath, \"col_dict.json\"))\n",
    "    # algo_data = pandas_reads(os.path.join(ecr_basepath, \"algo_dict.json\"))[algo_name]\"\n",
    "    col_dict = pandas_reads(\"col_dict.json\")\n",
    "    algo_data = pandas_reads(\"algo_dict.json\")[algo_name]\n",
    "    \n",
    "    module = importlib.import_module(algo_data['module'])\n",
    "    algo = getattr(module, algo_data['class'])\n",
    "        \n",
    "    train_filepath = os.path.join(args.train_data_dir, 'train.parquet')\n",
    "    validate_filepath = os.path.join(args.validate_data_dir, 'validate.parquet')\n",
    "\n",
    "    print('Loading data')\n",
    "    train_df = pandas_reads(train_filepath)\n",
    "    print(f'Train data size: {train_df.shape}')\n",
    "    validate_df = pandas_reads(validate_filepath)\n",
    "    print(f'Validate data size: {validate_df.shape}')\n",
    "    \n",
    "    Xtrain = train_df.drop(args.target_col, axis=1)\n",
    "    ytrain = train_df[[args.target_col]]\n",
    "    Xvalidate = validate_df.drop(args.target_col, axis=1)\n",
    "    yvalidate = validate_df[[args.target_col]]\n",
    "    \n",
    "    print('Tuning best algorithm')\n",
    "    model = tune_best_algo(algo, algo_data, col_dict, Xtrain, ytrain)\n",
    "    # shap_explainer, anchor_explainer = explainability_mods(model, Xtrain)\n",
    "    tuned_val_metrics = metrics_row(yvalidate, model, Xvalidate)\n",
    "    \n",
    "    print(\"Tuned Model's preformance on training data:\")\n",
    "    print(metrics_row(ytrain, model, Xtrain))\n",
    "    \n",
    "    print(\"Tuned Model's preformance on validation data:\")\n",
    "    print(tuned_val_metrics)\n",
    "    \n",
    "    print('Saving model')\n",
    "    # This is done so that it is all put into a tar.gz file that can be used at inference\n",
    "    processor_steps = model.named_steps['all_preprocess']\n",
    "    model_step = model.named_steps['algorithm']\n",
    "    # joblib.dump(model, os.path.join(args.model_dir, 'model.joblib'))\n",
    "    joblib.dump(processor_steps, os.path.join(args.model_dir, 'preprocessor.joblib'))\n",
    "    joblib.dump(model_step, os.path.join(args.model_dir, 'model.joblib'))\n",
    "    # joblib.dump(shap_explainer, os.path.join(args.model_dir, 'shap_explainer.joblib'))\n",
    "    # alibi.saving.save_explainer(anchor_explainer, os.path.join(args.model_dir, 'anchor_explainer'))\n",
    "    \n",
    "    # Write required cols to output path\n",
    "    required_cols = pandas(reads(\"required_cols.json\"))\n",
    "    pandas_writes(required_cols, os.path.join(args.output_data_dir, 'required-features.json'))\n",
    "        \n",
    "# Functions for Inference\n",
    "\n",
    "# Note: these are required in order for the deployed model to\n",
    "# make predictions once deployed.\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"\n",
    "    Parses the input data payload.\n",
    "    \n",
    "    Accepts data formats include csv, json, or parquet file types.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    input_data : str or parquet\n",
    "        The feature data to be used to make a prediction. Accepted file\n",
    "        formats include csv, json, or parquet.\n",
    "    content_type : {csv, json, parquet}\n",
    "        The file format of the input_data.\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    \n",
    "    df : dataframe\n",
    "        Data is converted to a dataframe for further processing.\n",
    "        \n",
    "    Notes\n",
    "    -----------\n",
    "    \n",
    "    There appears to be some kind of handling completed by AWS when the\n",
    "    data is passed through the end point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # print('Pandas version:', pd.__version__)\n",
    "    # print('Sklearn version:', sklearn.__version__)\n",
    "    \n",
    "    print(f'Loading input data with content type {content_type}')\n",
    "    \n",
    "    if content_type == 'text/csv':\n",
    "        df = pd.read_csv(StringIO(input_data))\n",
    "    elif content_type == 'application/x-parquet':\n",
    "        df = pd.read_parquet(input_data)\n",
    "    elif content_type == 'application/json':\n",
    "        # contents = json.loads(input_data.read())\n",
    "        contents = json.loads(input_data)\n",
    "        df = pd.DataFrame(json.loads(contents), index=[0])\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script\".format(content_type))\n",
    "    # print(df)\n",
    "    return df\n",
    "        \n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"\n",
    "    Takes the prediction and formats it for output.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    prediction : dict\n",
    "        The model's prediction. In our case, this includes the trained\n",
    "        model's prediction as well as explainability information from\n",
    "        shap and anchor.\n",
    "    accept : {json, csv}\n",
    "        The file format for the output. The only tested and proven file\n",
    "        format is json. File format csv hasn't been tested.\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    \n",
    "    prediction : {json, csv}\n",
    "        The encoded prediction for output with the mimetype set to the\n",
    "        specified file format.\n",
    "    \n",
    "    Notes\n",
    "    -----------\n",
    "    The default accept/content-type between containers for serial\n",
    "    inference is JSON. We also want to set the ContentType or mimetype\n",
    "    as the same value as accept so the next container can read the\n",
    "    response payload correctly.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Prediction object: {prediction}\")\n",
    "    print(f\"Prediction object type: {type(prediction)}\")\n",
    "    \n",
    "    print(f'Running output function with accept type {accept}')\n",
    "    \n",
    "    if accept == 'application/json':\n",
    "        try:\n",
    "            # json_df = prediction.to_json(orient='records')\n",
    "            json_df = json.dumps(prediction)\n",
    "            print('Prediction to json:', json_df)\n",
    "            print('New type', type(json_df))\n",
    "        except Exception as e:\n",
    "            print('Error when creating json object', e)\n",
    "        return worker.Response(json_df, mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException('{} accept type is not supported by this script')\n",
    "        \n",
    "def predict_fn(df, estimator):\n",
    "    \"\"\"\n",
    "    Uses the output of the ``input_fn`` for the dataset and the output\n",
    "    of the ``model_fn`` for the estimator to make a prediction.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    df : dataframe\n",
    "        The features, which is obtained from the output of the\n",
    "        ``input_fn``.\n",
    "    estimator : tuple\n",
    "        The trained model and the trained explainers, which is obtained\n",
    "        from the output of the ``model_fn``.\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    \n",
    "    prediction : dict\n",
    "        Dictionary of the prediction probabilities, shap values, and\n",
    "        anchor values.\n",
    "    \n",
    "    Notes\n",
    "    -----------\n",
    "    \n",
    "    We use predict_proba to return prediction proababilities instead\n",
    "    of single yes/no values.\n",
    "    \n",
    "    Example\n",
    "    -----------\n",
    "    \n",
    "    An example of the return for this function is:\n",
    "    \n",
    "    {'prediction':\n",
    "        {0: 0.9995577335357666, 1: 0.00044227897888049483},\n",
    "    'shap': {\n",
    "        'float_cols__floats': -0.2596146665822727,\n",
    "        'one_hot__one_hot_purple': -0.11884183668430261,\n",
    "        'date_cols__dates-day_of_month': -0.04672476084749602},\n",
    "    'anchor': {\n",
    "        'values': [\n",
    "            'float_cols__floats > 0.00',\n",
    "            'one_hot__one_hot_purple > 0.00'],\n",
    "        'precision': 0.9994452149791956,\n",
    "        'coverage': 0.4319}}\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn import set_config\n",
    "    set_config(transform_output=\"pandas\")\n",
    "\n",
    "    # processor_steps, model_step, shap_explainer, anchor_explainer = estimator\n",
    "    processor_steps, model_step = estimator\n",
    "\n",
    "    Xprocessed = processor_steps.transform(df)\n",
    "    prediction = model_step.predict_proba(Xprocessed)\n",
    "    prediction_dict = pd.DataFrame(prediction,\n",
    "                                   columns=model_step.classes_).T.rename(\n",
    "        columns={0:'prediction'}).to_dict()\n",
    "    \n",
    "    max_evals = Xprocessed.shape[1] * 2 + 1\n",
    "#     shap_results = shap_explainer(Xprocessed, max_evals=max_evals)\n",
    "#     shap_df = pd.DataFrame(shap_results.values.T,\n",
    "#                            index=Xprocessed.columns,\n",
    "#                            columns=['shap'])\n",
    "#     shap_df['shap_abs'] = shap_df['shap'].abs()\n",
    "#     shap_largest = shap_df.nlargest(10, 'shap_abs', keep='all').sort_values('shap', ascending=False)\n",
    "#     shap_dict = shap_largest[['shap']].to_dict()\n",
    "    \n",
    "#     anchor_explainer.predictor(Xprocessed)\n",
    "#     anchor_explanation = anchor_explainer.explain(Xprocessed.to_numpy())\n",
    "#     anchor_dict = {'anchor': {'values':anchor_explanation.anchor,\n",
    "#                               'precision':anchor_explanation.precision,\n",
    "#                               'coverage':anchor_explanation.coverage}}\n",
    "    \n",
    "#     return {**prediction_dict, **shap_dict, **anchor_dict}\n",
    "    return {**prediction_dict}\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Loads in the trained model and other artifacts from the model_dir.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    model_dir : str\n",
    "        The directory path for the model directory where the model and\n",
    "        model artifacts are stored.\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    \n",
    "    estimator : tuple\n",
    "        A tuple that contains the preprocessing for the trained model,\n",
    "        the trained model, the shap arguments/explainer, and the\n",
    "        anchor explainer\n",
    "    \n",
    "    Notes\n",
    "    -----------\n",
    "    \n",
    "    The model artifacts are bundled into a tuple in order to pass all\n",
    "    of them to the ``predict_fn``. This allows us to include explainability\n",
    "    information with the prediction at the time of inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Loading model')    \n",
    "    processor_steps = joblib.load(os.path.join(model_dir, 'preprocessor.joblib'))\n",
    "    model_step = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    # model = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    # processor_steps = model.named_steps['all_preprocess']\n",
    "    # model_step = model.named_steps['algorithm']\n",
    "    \n",
    "#     print('Loading shap arguments')    \n",
    "#     shap_explainer = joblib.load(os.path.join(model_dir, 'shap_explainer.joblib'))\n",
    "    \n",
    "#     print('Loading anchor explainer')    \n",
    "#     predict_fn = lambda x: model_step.predict(x)\n",
    "#     anchor_explainer = alibi.saving.load_explainer(os.path.join(model_dir, 'anchor_explainer'), predict_fn)\n",
    "    \n",
    "#     estimator = processor_steps, model_step, shap_explainer, anchor_explainer\n",
    "    estimator = processor_steps, model_step\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e37a35-1708-418f-9f15-858a7a3151a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Evaluation\n",
    "\n",
    "Uses the test set label column and the batch transformed predictions to evaluate how well the model performs. Requires the headers be removed before being passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "033974ab-e773-4899-92f8-6357a206e5fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate.py\n",
    "\"\"\"\n",
    "Evalutes the trained and tuned model against the hold out/test dataset\n",
    "and stores that information for inclusion in the Sagemaker Studio\n",
    "Model Registry page.\n",
    "\n",
    "**Input**:\n",
    "    model : model.tar.gz\n",
    "        The file that holds the preprocessor and model.\n",
    "    test_y : json\n",
    "        The hold out/test data used to evaluate the model.\n",
    "\n",
    "**Arguments**:\n",
    "    --target : string\n",
    "        The name of the column that stores the ground truth values.\n",
    "\n",
    "**Output**:\n",
    "    report_dict : json\n",
    "        The json file containing the evaluation metrics for the trained\n",
    "        and tuned model as evaluated against the hold out/test dataset.\n",
    "        This object includes accuracy, precision, f1, recall, ROC AUC,\n",
    "        and the confusion matrix.\n",
    "    \n",
    "Notes\n",
    "-----\n",
    "    When correctly configured, this information can be included in\n",
    "    Sagemaker Studio's Model Registry page for easy reference.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    import os\n",
    "    import joblib\n",
    "    import argparse\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    import tarfile\n",
    "    import pathlib\n",
    "    from sklearn import set_config\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    # from functions_module import eval_report\n",
    "\n",
    "    set_config(transform_output=\"pandas\")\n",
    "\n",
    "    print(\"numpy version:\", np.__version__)\n",
    "    print(\"pandas version:\", pd.__version__)\n",
    "    print(\"sklearn version:\", sklearn.__version__)\n",
    "    import platform\n",
    "    print(\"python version:\", platform.python_version())\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--target', type=str, dest='target_col')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print('Args: {}'.format(args))\n",
    "\n",
    "    model_path = \"/opt/ml/processing/input/model\"\n",
    "    test_y_path = \"/opt/ml/processing/input/test/test.json\"\n",
    "    \n",
    "    with tarfile.open(os.path.join(model_path, \"model.tar.gz\")) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    processor_step = joblib.load(\"preprocessor.joblib\")\n",
    "    model_step = joblib.load(\"model.joblib\")\n",
    "    # model =  Pipeline([\n",
    "    #     ('all_preprocess', processor_step),\n",
    "    #     ('algorithm', model_step)])\n",
    "    # model = joblib.load(\"model.joblib\")\n",
    "    \n",
    "    with open(test_y_path, \"r\") as xf:\n",
    "        contents = json.loads(xf.read())\n",
    "    # print(\"Contents variable:\", contents)\n",
    "    test_df = pd.DataFrame(pd.read_json(contents))\n",
    "    print(\"Test dataframe type:\")\n",
    "    print(type(test_df))\n",
    "    # print('Test dataframe')\n",
    "    print(test_df)\n",
    "    Xtest = test_df.drop(args.target_col, axis=1)\n",
    "    ytest = test_df[[args.target_col]]\n",
    "    \n",
    "    report_dict = eval_report(ytest, processor_step, model_step, Xtest)\n",
    "    # report_dict = eval_report(ytest, model, Xtest)\n",
    "    print('Report Dict:', report_dict)\n",
    "        \n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = os.path.join(output_dir, 'evaluation.json')\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
