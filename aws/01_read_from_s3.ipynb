{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550379b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker and Production Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01a300",
   "metadata": {},
   "source": [
    "There are a lot of considerations in moving from a local model used to train and predict on batch data to a production model. This series of posts explores how to create an MLOps compliant production pipeline using AWS's SageMaker Studio.\n",
    "\n",
    "SageMaker Studio is a suite of tools that helps manage the infrastructure and collaboration for a machine learning project in the AWS ecosystem. Some of the biggest advantages of SageMaker Studio include:\n",
    "\n",
    "- Ability to spin up hardware resources as needed\n",
    "- Automatically spin down hardware resources once the task is complete\n",
    "- Ability to create a pipeline to automate the machine learning process from preprocessing data through deploying the model\n",
    "\n",
    "This first post in the series will go over how to pull data from S3 and obtain file metadata. All outputs should match what is in the Notebook unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d1b35",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "For brevity, I'll assume that SageMaker Studio and an IAM role with the appropriate permissions have been set up. In a corporate/enterprise environment, these will generally be set up by an administrator or someone on the architecture team.\n",
    "\n",
    "- For directions on setting up the SageMaker environment see [Onboard to Amazon SageMaker Domain Using Quick setup](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html)\n",
    "- For directions on setting up an AWS account and IAM role see [Set Up Amazon SageMaker Prerequisites](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)\n",
    "\n",
    "This notebook can be run Jupyter Notebook in SageMaker Studio or as a stand alone SageMaker Jupyter Notebook instance. It *may* work in a local environment where the AWS credentials are specified, but that use case hasn't been tried or tested. This series is designed to take advantage of the managed infrastructure and other benefits of using SageMaker Studio, so that will be the prefered environment for all posts in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd0f9d",
   "metadata": {},
   "source": [
    "## Write data to S3\n",
    "\n",
    "*Note*, if you already have data in an S3 bucket, you can skip this step. However, the rest of the code in this post, as well as the rest of the series, uses the data saved to the default S3 bucket in this step.\n",
    "\n",
    "The first part of any data science project is to get data. In working with AWS and SageMaker, the best practices choice for data storage is S3. S3 is the default for SageMaker inputs and outputs, including things like training data sets and model artifacts.\n",
    "\n",
    "First, let's put some data into S3. The below cell reads in four files from the [Insurance Company Benchmark Data Set](https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29) hosted on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "\n",
    "I chose this data set for two main reasons:\n",
    "\n",
    "- The features represent both textual/categorical and numeric data types\n",
    "- Multile files are used to store the data\n",
    "\n",
    "It is common to have to clean data prior to training. This process can easily start with data in multiple files that need to go through an ETL (extract, transform, load) process before a final single file is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99547a0",
   "metadata": {},
   "source": [
    "### Read in Data from UCI Repo\n",
    "\n",
    "First we need to pull in our sample data from the UCI Machine Learning Repository. We can do this with `pandas`. Like many data scientists `pandas` is my go to library for data import/export, storage, and wrangling.\n",
    "\n",
    "The `pandas` library now utilizes functionality from the `s3fs` library, which allows you to work with S3 files the same way you would with files on the local machine. *Note*, `s3fs` needs to be installed on the machine you're working on, but it does *not* need to be imported into the notebook. In my experience it's installed by default in SageMaker notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5f2578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  76  77  78  79  80  81  82  \\\n",
       "0  33   1   3   2   8   0   5   1   3   7  ...   0   0   0   1   0   0   0   \n",
       "1  37   1   2   2   8   1   4   1   4   6  ...   0   0   0   1   0   0   0   \n",
       "2  37   1   2   2   8   0   4   2   4   3  ...   0   0   0   1   0   0   0   \n",
       "3   9   1   3   3   3   2   3   2   4   5  ...   0   0   0   1   0   0   0   \n",
       "4  40   1   4   2  10   1   4   1   4   7  ...   0   0   0   1   0   0   0   \n",
       "\n",
       "   83  84  85  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "4   0   0   0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/ticdata2000.txt', header=None)\n",
    "test = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/ticeval2000.txt', header=None)\n",
    "ground_truth = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/tictgts2000.txt', header=None)\n",
    "columns = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/dictionary.txt', encoding='latin-1')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf238c1",
   "metadata": {},
   "source": [
    "### Set AWS variables\n",
    "\n",
    "There are several variables you'll need when sending information back and forth across the AWS infrastructure. These are generally permission/access type variables. It's also useful to capture frequently used information like the bucket and prefix to the specific folder you'll be reading and writing to. This also makes it easier to change the folder path should you want to use a different base location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87ee635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.session\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'ins_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548ec2c",
   "metadata": {},
   "source": [
    "### Write to S3\n",
    "\n",
    "Just like with reading in data, you can write data back to S3 using `pandas` per your usual workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e55fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(f's3://{bucket}/{prefix}/raw/train.csv', index=False)\n",
    "test.to_csv(f's3://{bucket}/{prefix}/raw/test.csv', index=False)\n",
    "ground_truth.to_csv(f's3://{bucket}/{prefix}/raw/gt.csv', index=False)\n",
    "columns.to_csv(f's3://{bucket}/{prefix}/raw/metadata/col_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba7017",
   "metadata": {},
   "source": [
    "To see your data in AWS, simply print the `bucket` and `prefix` name and visit that folder in the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{bucket}/{prefix}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0443cf",
   "metadata": {},
   "source": [
    "## Set Library Dependencies\n",
    "\n",
    "Frequently, the library version doesn't match the version needed to run your code. The below cell demonstrates how to load packages as well as upgrade the versions. One of the most frequent library mismatches that I've run into recently is `pandas`. The default was `pandas 1.0.X` at the time this post was created. My code generally requires the updates from `pandas 1.3.5` or later.\n",
    "\n",
    "The `pandas` version you see will probably be different than the one listed in the below output. The second cell below is the code to install or upgrade packages. The third cell is included to double check that the changes you want have been appropriately applied. *Note*, the code for this particular notebook should run on just about any `pandas` version >= 1.X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d016b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8470f52-b2ff-43f7-bc51-6d84688f8adf",
   "metadata": {},
   "source": [
    "The below two cells are optional. This code was included as an example of how to update library dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c4d83",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install category_encoders\n",
    "!{sys.executable} -m pip install pandas numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d5e34",
   "metadata": {},
   "source": [
    "## Read from S3\n",
    "\n",
    "Now we get to the main point of this post. Reading in files and metadata from S3. First we need `numpy`, `pandas`, and `boto3`. `numpy` and `pandas` are packages for manipulating data, `boto3` facilitates interaction with AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94df613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3220b",
   "metadata": {},
   "source": [
    "### Read a single file\n",
    "\n",
    "Reading a single file is easy if you know the S3 URI.  Basically, we can do this the same way we initially read the file from the UCI Repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb1f8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1  2  3   4  5  6  7  8  9  ...  76  77  78  79  80  81  82  83  84  85\n",
       "0  33  1  3  2   8  0  5  1  3  7  ...   0   0   0   1   0   0   0   0   0   0\n",
       "1  37  1  2  2   8  1  4  1  4  6  ...   0   0   0   1   0   0   0   0   0   0\n",
       "2  37  1  2  2   8  0  4  2  4  3  ...   0   0   0   1   0   0   0   0   0   0\n",
       "3   9  1  3  3   3  2  3  2  4  5  ...   0   0   0   1   0   0   0   0   0   0\n",
       "4  40  1  4  2  10  1  4  1  4  7  ...   0   0   0   1   0   0   0   0   0   0\n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.read_csv(f's3://{bucket}/{prefix}/raw/train.csv')\n",
    "example.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f1160",
   "metadata": {},
   "source": [
    "# Read multiple files\n",
    "\n",
    "Things get a little trickier when you need to read multiple files from a subdirectory.\n",
    "\n",
    "## List files in subdirectory\n",
    "\n",
    "You may or may not need to see what's in the folder. But I typically find it handy to be able to confirm what is or isn't there. There are several ways to do this:\n",
    "\n",
    "- `boto3.client`\n",
    "- `boto3.resource`\n",
    "- command line\n",
    "\n",
    "*Note*, all three of these methods return the name of the subdirectory as well as the files within it.\n",
    "\n",
    "I'll look at these one by one. This is where having `bucket` and `prefix` variables comes in really handy. *Note*, no outputs are included for the cells that would reveal account specific information, including full S3 URIs. To see this information for yourself, please clone the repo and run the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a2baf",
   "metadata": {},
   "source": [
    "#### boto3.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099b9a3",
   "metadata": {},
   "source": [
    "This is a lot of information and rather messy. We can narrow it down to just the information about the files by looking at the 'Contents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)['Contents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99bb14d",
   "metadata": {},
   "source": [
    "#### boto3.resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3277942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ins_dataset/raw/gt.csv\n",
      "ins_dataset/raw/metadata/col_info.csv\n",
      "ins_dataset/raw/test.csv\n",
      "ins_dataset/raw/train.csv\n"
     ]
    }
   ],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "for object_summary in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    print(object_summary.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e2307",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Comand line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "801a24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f's3://{bucket}/{prefix}/raw/'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925941c8",
   "metadata": {},
   "source": [
    "We then include the `file_path` variable in a bash command entered right into the Jupyter cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce807132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE metadata/\n",
      "2022-10-21 16:11:32       8002 gt.csv\n",
      "2022-10-21 16:11:32     683549 test.csv\n",
      "2022-10-21 16:11:31    1006399 train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e55da6",
   "metadata": {},
   "source": [
    "The size of the files (the middle values in the above output) can be useful for determining the amount of memory needed. I've also used it to chose smaller files for testing/prototyping code before spinning up larger instances to process larger files.\n",
    "\n",
    "The information can be dumped into a csv file using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13ad738",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $file_path | cat >> files.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414eb4e",
   "metadata": {},
   "source": [
    "### Capture file names and read in\n",
    "\n",
    "I find the `boto3.resource` output easiest to work with, so I'll use it to capture the file names and read in what I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff137b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ins_dataset/raw/gt.csv',\n",
       " 'ins_dataset/raw/metadata/col_info.csv',\n",
       " 'ins_dataset/raw/test.csv',\n",
       " 'ins_dataset/raw/train.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for object_summary in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    if (len(object_summary.key.rsplit('.')) == 2):\n",
    "        file_names.append(object_summary.key)\n",
    "        \n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab157fa",
   "metadata": {},
   "source": [
    "In the above snippet I split on `.` to ensure that the returned object is a file instead of a folder. Also note that this code goes into subdirectories and returns those files as well.\n",
    "\n",
    "These kinds of conditionals can be used in many different ways to return only specific file types (csv vs parquet vs txt, etc) or to grab only portions of the file path. One use case for this would be a dictionary where the key is the file name and the value is the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "files = {}\n",
    "\n",
    "for object_summary in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    if (len(object_summary.key.rsplit('.')) == 2) & (len(object_summary.key.split('/')) <= 3):\n",
    "        files[object_summary.key.split('/')[-1].split('.')[0]] = f\"s3://{bucket}/{object_summary.key}\"\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c7456",
   "metadata": {},
   "source": [
    "From the dictionary of 'name' and 'URI' it's easy to create a dictionary of dataframes.\n",
    "\n",
    "The dictionary of dataframes data structure is extremely useful in that it's easy to name a dataframe and to have an unspecified number of dataframes to be read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5967584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt\n",
      "test\n",
      "train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gt':       0\n",
       " 0     0\n",
       " 1     0\n",
       " 2     1\n",
       " 3     0\n",
       " 4     0\n",
       " ...  ..\n",
       " 3996  0\n",
       " 3997  1\n",
       " 3998  0\n",
       " 3999  0\n",
       " 4000  0\n",
       " \n",
       " [4001 rows x 1 columns],\n",
       " 'test':                                                       0\n",
       " 0     0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18...\n",
       " 1     33,1,4,2,8,0,6,0,3,5,0,4,1,1,8,2,2,6,0,0,1,2,6...\n",
       " 2     6,1,3,2,2,0,5,0,4,5,2,2,1,4,5,5,4,0,5,0,0,4,0,...\n",
       " 3     39,1,3,3,9,1,4,2,3,5,2,3,2,3,6,2,4,4,2,1,1,3,2...\n",
       " 4     9,1,2,3,3,2,3,2,4,5,4,1,2,4,4,2,4,4,2,1,1,5,1,...\n",
       " ...                                                 ...\n",
       " 3996  33,1,2,4,8,0,7,2,0,5,2,2,2,6,2,0,3,6,5,0,0,1,0...\n",
       " 3997  24,1,2,3,5,1,5,1,3,4,2,4,4,4,2,2,4,4,2,0,0,3,3...\n",
       " 3998  36,1,2,3,8,1,5,1,3,7,0,2,2,5,3,2,3,4,2,0,0,3,4...\n",
       " 3999  33,1,3,3,8,1,4,2,3,7,1,2,2,3,4,1,3,5,1,1,1,2,3...\n",
       " 4000  8,1,2,3,2,4,3,0,3,5,2,2,0,6,3,8,0,1,8,0,0,0,0,...\n",
       " \n",
       " [4001 rows x 1 columns],\n",
       " 'train':                                                       0\n",
       " 0     0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18...\n",
       " 1     33,1,3,2,8,0,5,1,3,7,0,2,1,2,6,1,2,7,1,0,1,2,5...\n",
       " 2     37,1,2,2,8,1,4,1,4,6,2,2,0,4,5,0,5,4,0,0,0,5,0...\n",
       " 3     37,1,2,2,8,0,4,2,4,3,2,4,4,4,2,0,5,4,0,0,0,7,0...\n",
       " 4     9,1,3,3,3,2,3,2,4,5,2,2,2,3,4,3,4,2,4,0,0,3,1,...\n",
       " ...                                                 ...\n",
       " 5818  36,1,1,2,8,0,6,1,2,1,2,6,5,3,2,2,5,2,2,0,0,4,1...\n",
       " 5819  35,1,4,4,8,1,4,1,4,6,0,3,2,2,5,0,0,9,2,1,1,3,3...\n",
       " 5820  33,1,3,4,8,0,6,0,3,5,1,4,3,3,4,0,1,8,1,0,0,2,3...\n",
       " 5821  34,1,3,2,8,0,7,0,2,7,2,0,0,4,5,0,2,7,0,2,0,2,4...\n",
       " 5822  33,1,3,3,8,0,6,1,2,7,1,2,1,4,4,1,2,6,1,0,1,3,2...\n",
       " \n",
       " [5823 rows x 1 columns]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {}\n",
    "\n",
    "for df_name in files.keys():\n",
    "    print(df_name)\n",
    "    df_dict[df_name] = pd.read_table(files[df_name], header=None)\n",
    "    \n",
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d2a87",
   "metadata": {},
   "source": [
    "## Delete Files\n",
    "\n",
    "To ensure no ongoing charges are charged to your account, you can delete the files from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50922559",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = prefix + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968074f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "s3_bucket.objects.filter(Prefix=prefix).delete()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
