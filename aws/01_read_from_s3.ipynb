{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1131c641",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker and Production Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf07e27",
   "metadata": {},
   "source": [
    "There are a lot of considerations in moving from a local model used to train and predict on batch data to a production model. This series of posts explores how to create an MLOps compliant production pipeline using AWS's SageMaker Studio.\n",
    "\n",
    "This first post in the series will go over how to pull data from S3 and obtain file metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cdeb54",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "For brevity, I'll assume that SageMaker Studio and an IAM role with the appropriate permissions have been set up. In a corporate/enterprise environment, these will generally be set up by an administrator or someone on the architecture team.\n",
    "\n",
    "- For directions on setting up the SageMaker environment see [Onboard to Amazon SageMaker Domain Using Quick setup](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html)\n",
    "- For directions on setting up an AWS account and IAM role see [Set Up Amazon SageMaker Prerequisites](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c6595",
   "metadata": {},
   "source": [
    "## Write data to S3\n",
    "\n",
    "*Note*, if you already have data in an S3 bucket, you can skip this step. However, the rest of the code in this post, as well as the rest of the series, uses the data saved to the default S3 bucket in this step.\n",
    "\n",
    "The first part of any data science project is to get data. In working with AWS and SageMaker, the best practices choice for data storage is S3. S3 is the default for SageMaker inputs and outputs, including things like training data sets and model artifacts.\n",
    "\n",
    "First, let's put some data into S3. The below cell reads in four files from the [Insurance Company Benchmark Data Set](https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29) hosted on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "\n",
    "I chose this data set for two main reasons:\n",
    "\n",
    "- The features represent both textual/categorical and numeric data types\n",
    "- Multile files are used to store the data\n",
    "\n",
    "It is common to have to clean data prior to training. This process can easily start with data in multiple files that need to go through an ETL (extract, transform, load) process before a final single file is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a38c30",
   "metadata": {},
   "source": [
    "### Read in Data from UCI Repo\n",
    "\n",
    "First we need to pull in our sample data from the UCI Machine Learning Repository. We can do this with `pandas`.\n",
    "\n",
    "The `pandas` library now utilizes functionality from the `s3fs` library, which allows you to work with S3 files the same way you would with files on the local machine. *Note*, `s3fs` needs to be installed on the machine you're working on, but it does *not* need to be imported into the notebook. In my experience it's installed by default in SageMaker notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9ceade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  76  77  78  79  80  81  82  \\\n",
       "0  33   1   3   2   8   0   5   1   3   7  ...   0   0   0   1   0   0   0   \n",
       "1  37   1   2   2   8   1   4   1   4   6  ...   0   0   0   1   0   0   0   \n",
       "2  37   1   2   2   8   0   4   2   4   3  ...   0   0   0   1   0   0   0   \n",
       "3   9   1   3   3   3   2   3   2   4   5  ...   0   0   0   1   0   0   0   \n",
       "4  40   1   4   2  10   1   4   1   4   7  ...   0   0   0   1   0   0   0   \n",
       "\n",
       "   83  84  85  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "4   0   0   0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/ticdata2000.txt', header=None)\n",
    "test = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/ticeval2000.txt', header=None)\n",
    "ground_truth = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/tictgts2000.txt', header=None)\n",
    "columns = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-mld/dictionary.txt', encoding='latin-1')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82eff7",
   "metadata": {},
   "source": [
    "### Set AWS variables\n",
    "\n",
    "There are several variables you'll need when sending information back and forth across the AWS infrastructure. These are generally permission/access type variables. It's also useful to capture frequently used information like the bucket and prefix to the specific folder you'll be reading and writing to. This also makes it easier to change the folder path should you want to use a different base location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77faa659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.session\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'ins_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cf54d",
   "metadata": {},
   "source": [
    "### Write to S3\n",
    "\n",
    "Just like with reading in data, you can write data back to S3 using `pandas` per your usual workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b5a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(f's3://{bucket}/{prefix}/raw/train.csv', index=False)\n",
    "test.to_csv(f's3://{bucket}/{prefix}/raw/test.csv', index=False)\n",
    "ground_truth.to_csv(f's3://{bucket}/{prefix}/raw/gt.csv', index=False)\n",
    "columns.to_csv(f's3://{bucket}/{prefix}/raw/metadata/col_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa8a15",
   "metadata": {},
   "source": [
    "To see your data in AWS, simply print the `bucket` and `prefix` name and visit that folder in the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad455519",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{bucket}/{prefix}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165cc1e3",
   "metadata": {},
   "source": [
    "## Set Library Dependencies\n",
    "\n",
    "Frequently, the library version doesn't match the version needed to run your code. The below cell demonstrates how to load packages as well as upgrade the versions. One of the most frequent that I've run into recently, is a default of `pandas 1.0.X` where my code requires the updates from `pandas 1.3.5` or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d888e246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ffe7d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install category_encoders\n",
    "!{sys.executable} -m pip install pandas numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b41980",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc2df5",
   "metadata": {},
   "source": [
    "## Read from S3\n",
    "\n",
    "Now we get to the main point of this post. Reading in files and metadata from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abbd84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95833483",
   "metadata": {},
   "source": [
    "### Read a single file\n",
    "\n",
    "Reading a single file is easy if you know the S3 URI.  Basically, we can do this the same way we initially read the file from the UCI Repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48e71951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1  2  3   4  5  6  7  8  9  ...  76  77  78  79  80  81  82  83  84  85\n",
       "0  33  1  3  2   8  0  5  1  3  7  ...   0   0   0   1   0   0   0   0   0   0\n",
       "1  37  1  2  2   8  1  4  1  4  6  ...   0   0   0   1   0   0   0   0   0   0\n",
       "2  37  1  2  2   8  0  4  2  4  3  ...   0   0   0   1   0   0   0   0   0   0\n",
       "3   9  1  3  3   3  2  3  2  4  5  ...   0   0   0   1   0   0   0   0   0   0\n",
       "4  40  1  4  2  10  1  4  1  4  7  ...   0   0   0   1   0   0   0   0   0   0\n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.read_csv(f's3://{bucket}/{prefix}/raw/train.csv')\n",
    "example.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbe606",
   "metadata": {},
   "source": [
    "# Read multiple files\n",
    "\n",
    "Things get a little trickier when you need to read multiple files from a subdirectory.\n",
    "\n",
    "## List files in subdirectory\n",
    "\n",
    "You may or may not need to see what's in the folder. But I typically find it handy to be able to confirm what is or isn't there. There are several ways to do this:\n",
    "\n",
    "- `boto3.client`\n",
    "- `boto3.resource`\n",
    "- command line\n",
    "\n",
    "*Note*, all three of these methods return the name of the subdirectory as well as the files within it.\n",
    "\n",
    "I'll look at these one by one. This is where having `bucket` and `prefix` variables comes in really handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfdd5ee",
   "metadata": {},
   "source": [
    "#### boto3.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a98634",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0314772",
   "metadata": {},
   "source": [
    "This is a lot of information and rather messy. We can narrow it down to just the information about the files by looking at the 'Contents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)['Contents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f358a0e",
   "metadata": {},
   "source": [
    "#### boto3.resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "743521c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ins_dataset/raw/gt.csv\n",
      "ins_dataset/raw/metadata/col_info.csv\n",
      "ins_dataset/raw/test.csv\n",
      "ins_dataset/raw/train.csv\n"
     ]
    }
   ],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "for object_summary in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    print(object_summary.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762453d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Comand line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f's3://{bucket}/{prefix}/raw/'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf731a9",
   "metadata": {},
   "source": [
    "In the cell below, replace `file_path` with the output of the cell above. For security and privacy reasons, the values I use aren't included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c3a7bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE metadata/\n",
      "2022-10-20 14:23:48       8002 gt.csv\n",
      "2022-10-20 14:23:47     683549 test.csv\n",
      "2022-10-20 14:23:47    1006399 train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688356c5",
   "metadata": {},
   "source": [
    "The size of the files (the middle values in the above output) can be useful for determining the amount of memory needed. I've also used it to chose smaller files for testing/prototyping code before spinning up larger instances to process larger files.\n",
    "\n",
    "The information can be dumped into a csv file using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f36d6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://sagemaker-us-east-1-707031497630/ins_dataset/raw/ | cat >> files.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d978007f",
   "metadata": {},
   "source": [
    "### Capture file names and read in\n",
    "\n",
    "I find the `boto3.resource` output easiest to work with, so I'll use it to capture the file names and read in what I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d7f6547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ins_dataset/raw/gt.csv',\n",
       " 'ins_dataset/raw/metadata/col_info.csv',\n",
       " 'ins_dataset/raw/test.csv',\n",
       " 'ins_dataset/raw/train.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for object_summary in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    if (len(object_summary.key.rsplit('.')) == 2):\n",
    "        file_names.append(object_summary.key)\n",
    "        \n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74644a2f",
   "metadata": {},
   "source": [
    "In the above snippet I split on `.` to ensure that the returned object is a file instead of a folder. Also note that this code goes into subdirectories and returns those files as well.\n",
    "\n",
    "These kinds of conditionals can be used in many different ways to return only specific file types (csv vs parquet vs txt, etc) or to grab only portions of the file path. One use case for this would be a dictionary where the key is the file name and the value is the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65605d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "files = {}\n",
    "\n",
    "for object_summary in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    if (len(object_summary.key.rsplit('.')) == 2) & (len(object_summary.key.split('/')) <= 3):\n",
    "        files[object_summary.key.split('/')[-1].split('.')[0]] = f\"s3://{bucket}/{object_summary.key}\"\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a5e39",
   "metadata": {},
   "source": [
    "From the dictionary of 'name' and 'URI' it's easy to create a dictionary of dataframes.\n",
    "\n",
    "The dictionary of dataframes data structure is extremely useful in that it's easy to name a dataframe and to have an unspecified number of dataframes to be read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d8ff21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt\n",
      "test\n",
      "train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gt':       0\n",
       " 0     0\n",
       " 1     0\n",
       " 2     1\n",
       " 3     0\n",
       " 4     0\n",
       " ...  ..\n",
       " 3996  0\n",
       " 3997  1\n",
       " 3998  0\n",
       " 3999  0\n",
       " 4000  0\n",
       " \n",
       " [4001 rows x 1 columns],\n",
       " 'test':                                                       0\n",
       " 0     0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18...\n",
       " 1     33,1,4,2,8,0,6,0,3,5,0,4,1,1,8,2,2,6,0,0,1,2,6...\n",
       " 2     6,1,3,2,2,0,5,0,4,5,2,2,1,4,5,5,4,0,5,0,0,4,0,...\n",
       " 3     39,1,3,3,9,1,4,2,3,5,2,3,2,3,6,2,4,4,2,1,1,3,2...\n",
       " 4     9,1,2,3,3,2,3,2,4,5,4,1,2,4,4,2,4,4,2,1,1,5,1,...\n",
       " ...                                                 ...\n",
       " 3996  33,1,2,4,8,0,7,2,0,5,2,2,2,6,2,0,3,6,5,0,0,1,0...\n",
       " 3997  24,1,2,3,5,1,5,1,3,4,2,4,4,4,2,2,4,4,2,0,0,3,3...\n",
       " 3998  36,1,2,3,8,1,5,1,3,7,0,2,2,5,3,2,3,4,2,0,0,3,4...\n",
       " 3999  33,1,3,3,8,1,4,2,3,7,1,2,2,3,4,1,3,5,1,1,1,2,3...\n",
       " 4000  8,1,2,3,2,4,3,0,3,5,2,2,0,6,3,8,0,1,8,0,0,0,0,...\n",
       " \n",
       " [4001 rows x 1 columns],\n",
       " 'train':                                                       0\n",
       " 0     0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18...\n",
       " 1     33,1,3,2,8,0,5,1,3,7,0,2,1,2,6,1,2,7,1,0,1,2,5...\n",
       " 2     37,1,2,2,8,1,4,1,4,6,2,2,0,4,5,0,5,4,0,0,0,5,0...\n",
       " 3     37,1,2,2,8,0,4,2,4,3,2,4,4,4,2,0,5,4,0,0,0,7,0...\n",
       " 4     9,1,3,3,3,2,3,2,4,5,2,2,2,3,4,3,4,2,4,0,0,3,1,...\n",
       " ...                                                 ...\n",
       " 5818  36,1,1,2,8,0,6,1,2,1,2,6,5,3,2,2,5,2,2,0,0,4,1...\n",
       " 5819  35,1,4,4,8,1,4,1,4,6,0,3,2,2,5,0,0,9,2,1,1,3,3...\n",
       " 5820  33,1,3,4,8,0,6,0,3,5,1,4,3,3,4,0,1,8,1,0,0,2,3...\n",
       " 5821  34,1,3,2,8,0,7,0,2,7,2,0,0,4,5,0,2,7,0,2,0,2,4...\n",
       " 5822  33,1,3,3,8,0,6,1,2,7,1,2,1,4,4,1,2,6,1,0,1,3,2...\n",
       " \n",
       " [5823 rows x 1 columns]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {}\n",
    "\n",
    "for df_name in files.keys():\n",
    "    print(df_name)\n",
    "    df_dict[df_name] = pd.read_table(files[df_name], header=None)\n",
    "    \n",
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb00f03",
   "metadata": {},
   "source": [
    "## Delete Files\n",
    "\n",
    "To ensure no ongoing charges are charged to your account, you can delete the files from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "756c66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = prefix + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc22104",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = s3_resource.Bucket(bucket)\n",
    "s3_bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16862f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
